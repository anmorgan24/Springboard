{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cf97bc",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow in Python\n",
    "Not long ago, cutting-edge computer vision algorithms couldn’t differentiate between images of cats and dogs. Today, a skilled data scientist equipped with nothing more than a laptop can classify tens of thousands of objects with greater accuracy than the human eye. In this course, you will use TensorFlow 2.6 to develop, train, and make predictions with the models that have powered major advances in recommendation systems, image classification, and FinTech. You will learn both high-level APIs, which will enable you to design and train deep learning models in 15 lines of code, and low-level APIs, which will allow you to move beyond off-the-shelf routines. You will also learn to accurately predict housing prices, credit card borrower defaults, and images of sign language gestures.\n",
    "\n",
    "**Instructor:** Isaiah Hull, senior economist at Sweden's Central Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fa10e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import constant, add, ones, matmul, multiply, reduce_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "938aa02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "721b1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = Variable(x0)\n",
    "\twith GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = multiply(x,x)\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71615939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    return intercept + features*slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a074d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function to compute the MSE\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "    # Compute the predictions for the linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e20eb",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Introduction to TensorFlow\n",
    "Before you can build advanced models in TensorFlow 2, you will first need to understand the basics. In this chapter, you’ll learn how to define constants and variables, perform tensor addition and multiplication, and compute derivatives. Knowledge of linear algebra will be helpful, but not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ae246",
   "metadata": {},
   "source": [
    "### Constants and variables\n",
    "* TensorFlow's two basic objects of computation are: **constants** and **variables**\n",
    "\n",
    "#### What is TensorFlow?\n",
    "* An open-source library for graph-based numerical computation\n",
    "    * Developed by the Google Brain Team\n",
    "* Low- and high-level APIs\n",
    "    * Addition, multiplication, differentiation\n",
    "    * Design and train machine learning models\n",
    "* Important changes in TensorFlow 2.0\n",
    "    * Eager execution enabled by default\n",
    "        * Allows users to write simpler and more intuitive code\n",
    "        * Model building with Keras and Estimators (high-level APIs)\n",
    "        \n",
    "#### What is a tensor?\n",
    "* The TensorFlow documentation describes a **tensor** as \"generalization of vectors and matrices to potentially higher dimensions.\"\n",
    "* If you're not familiar with linear algebra, think of a tensor as **a collection of numbers, which is arranged into a particular shape**.\n",
    "    * 0-dimensional: point\n",
    "    * 1-dimensional: line\n",
    "    * etc\n",
    "    \n",
    "### Defining tensors in TensorFlow\n",
    "* Each object defined below will be a `tf.Tensor object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a5aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# 0D Tensor\n",
    "d0 = tf.ones((1,))\n",
    "\n",
    "# 1D Tensor\n",
    "d1 = tf.ones((2,))\n",
    "\n",
    "# 2D Tensor\n",
    "d2 = tf.ones((2, 2))\n",
    "\n",
    "# 3D Tensor\n",
    "d3 = tf.ones((2, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd300f9b",
   "metadata": {},
   "source": [
    "If we want to print the array contained in that object, we can apply the `.numpy()` method and pass the resulting object to the print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79683309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# Print the 3D tensor\n",
    "print(d3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c0268",
   "metadata": {},
   "source": [
    "### Defining constants in TensorFlow\n",
    "* A **constant** the simplest category of tensor\n",
    "* A constant does not change and cannot be trained\n",
    "    * Immutable\n",
    "    * Untrainable\n",
    "* A constant can have any dimension\n",
    "* In the code below, we've defined two constants:\n",
    "    * `a` is a 2x3 tensor of 3s\n",
    "    * `b` is a 2x2 tensor which is constructed from the 1-dimensional tensor: 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764e49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import constant\n",
    "\n",
    "# Define a 2x3 constant\n",
    "a = constant(3, shape=[2, 3])\n",
    "\n",
    "# Define a 2x2 constant\n",
    "b = constant([1, 2, 3, 4], shape=[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da619719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3]\n",
      " [3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "692fc0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999755d",
   "metadata": {},
   "source": [
    "* Above we worked exclusively with the constant operation\n",
    "* However, in some cases, there are more convenient options for defining certain types of special tensors\n",
    "<img src='data/convenience_functions.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4176b1",
   "metadata": {},
   "source": [
    "* Use the `.zeros` or `.ones` operations to generate a tensor of arbitrary (but defined) dimension, that is populated entirely with zeros or ones\n",
    "* Use the `zeros_like` or `ones_like` operations to populate tensors with zeros and ones, copying the dimensions of some input tensor passed to it.\n",
    "* Use the `.fill` operation to populate a tensor of arbitrary dimension with the same scalar value in each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dc6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_ex = tf.fill([3, 3],7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b00dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7]\n",
      " [7 7 7]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "print(fill_ex.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb671309",
   "metadata": {},
   "source": [
    "### Defining and initializing variables\n",
    "* Unlike a constant, a variable's value can change during computation\n",
    "* The value of a variable is **shared**, **persistent**, and **modifiable**.\n",
    "* A variable's **data type and shape are fixed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427ca6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# Define a variable\n",
    "a0 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.float32)\n",
    "a1 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.int16)\n",
    "\n",
    "# Define a constant\n",
    "b = tf.constant(2, tf.float32)\n",
    "\n",
    "# Compute their product\n",
    "c0 = tf.multiply(a0, b)\n",
    "c1 = a0 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc202cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10. 12.]\n",
      "[ 2.  4.  6.  8. 10. 12.]\n"
     ]
    }
   ],
   "source": [
    "print(c0.numpy())\n",
    "print(c1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc1532",
   "metadata": {},
   "source": [
    "* Note that certain TensorFlow operations, such as `tf.multiply` are overloaded, which allows us to use the simpler `a0*b` expression instead.\n",
    "\n",
    "#### Exercises: Defining data as constants\n",
    "Throughout this course, we will use `tensorflow` version 2.6.0 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing `constant` from `tensorflow`.\n",
    "\n",
    "After you have imported `constant`, you will use it to transform a `numpy` array, `credit_numpy`, into a `tensorflow` constant, `credit_constant`. This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.\n",
    "\n",
    "Note that `tensorflow` 2 allows you to use data as either a `numpy` array or a `tensorflow` `constant` object. Using a constant will ensure that any operations performed with that object are done in `tensorflow`.\n",
    "\n",
    "```\n",
    "# Import constant from TensorFlow\n",
    "from tensorflow import constant\n",
    "\n",
    "# Convert the credit_numpy array into a tensorflow constant\n",
    "credit_constant = constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('\\n The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('\\n The shape is:', credit_constant.shape)\n",
    "```\n",
    "\n",
    "#### Exercises: Defining variables\n",
    "Unlike a constant, a variable's value can be modified. This will be useful when we want to train a model by updating its parameters.\n",
    "\n",
    "Let's try defining and printing a variable. We'll then convert the variable to a `numpy` array, print again, and check for differences. Note that `Variable()`, which is used to create a variable tensor, has been imported from `tensorflow` and is available to use in the exercise.\n",
    "\n",
    "```\n",
    "# Define the 1-dimensional variable A1\n",
    "A1 = Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print('\\n A1: ', A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print('\\n B1: ', B1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef87fb",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "* TensorFlow has a model of computation that revolves around the use of graphs\n",
    "* A TensorFlow graph contains edges and nodes, where the edges are tensors and the nodes are operations\n",
    "\n",
    "<img src='data/tf_operation_flow.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "### Applying the addition operator\n",
    "* We first import the constant and add operations so that we may now define 0-, 1-, and 2-dimensional tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a0c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import constant and add from tensorflow\n",
    "# from tensorflow import constant, add\n",
    "\n",
    "# Define 0-dimensional tensors\n",
    "A0 = constant([1])\n",
    "B0 = constant([2])\n",
    "\n",
    "# Define 1-dimensional tensors\n",
    "A1 = constant([1, 2])\n",
    "B1 = constant([3, 4])\n",
    "\n",
    "# Define 2-dimensional tensors\n",
    "A2 = constant([[1, 2], [3, 4]])\n",
    "B2 = constant([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2a295",
   "metadata": {},
   "source": [
    "### Applying the addition operator\n",
    "* Finally, let's add them together using the operation for tensor addition\n",
    "* Note that we can perform scalar addition with `A0` and `B0`, vector addition with `A1` and `B1`, and matrix addition with `A2` and `B2`\n",
    "* The `add()` operation performs **element-wise addition** with two tensors\n",
    "* **Element-wise addition requires that both tensors have the same shape:**\n",
    "    * Scalar addition: 1 + 2 = 3\n",
    "    * Vector addition: [1, 2] + [3, 4] = [4, 6]\n",
    "    * Matrix addition:\n",
    "\n",
    "```\n",
    "A = [[1, 2],\n",
    "     [3, 4]]\n",
    "B = [[5, 6], \n",
    "     [7, 8]]\n",
    "A + B = [[6, 8],\n",
    "         [10,12]]\n",
    "```\n",
    "* Furthermore, the `add()` operator is **overloaded**\n",
    "    * We can also perform addition using the plus symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8829148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tensor addition with add()\n",
    "C0 = add(A0, B0)\n",
    "C1 = add(A1, B1)\n",
    "C2 = add(A2, B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe5b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[4 6]\n",
      "[[ 6  8]\n",
      " [10 12]]\n"
     ]
    }
   ],
   "source": [
    "print(C0.numpy())\n",
    "print(C1.numpy())\n",
    "print(C2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69be6d",
   "metadata": {},
   "source": [
    "### How to perform multiplication in TensorFlow\n",
    "* We will consider both element-wise and matrix multiplication\n",
    "* **Element-wise multiplication** performed using the `multiply()` operation\n",
    "    * Tensors involved **must have the same shape**\n",
    "* **Matrix multiplication** performed with `matmul()` operator \n",
    "    * The `matmul(A, B)` operation multiplies `A` by `B`\n",
    "    * **Note** that number of columns of `A` must equal the number of rows of `B`\n",
    "    \n",
    "#### Applying the multiplication operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a78861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import operators from tensorflow\n",
    "# from tensorflow import ones, matmul, multiply\n",
    "\n",
    "# Define tensors\n",
    "A0 = ones(1)\n",
    "A31 = ones([3, 1])\n",
    "A34 = ones([3, 4])\n",
    "A43 = ones([4, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6094a0",
   "metadata": {},
   "source": [
    "* What types of operations are valid on these tensors of ones?\n",
    "    * We can perform element-wise multiplication of any element by itself\n",
    "        * `multiply(A0, A0)`, `multiply(A31, A31)`, and `multiply(A34, A34)`\n",
    "    * We can perform matrix multiplication on `matmul(A43, A34)`\n",
    "        * but **not** matmul(A43, A43)\n",
    "        \n",
    "### Summing over tensor dimensions\n",
    "* The `reduce_sum()` operator sums over the dimensions of a tensor\n",
    "* This can be used to sum over all dimensions of a tensor or just one.\n",
    "* The `reduce_sum()` operator sums over th dimensions of a tensor\n",
    "    * `reduce_sum(A)` sums over all dimensions of A\n",
    "    * `reduce_sum(A, i)` sums over dimension i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff4e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import operations from tensorflow\n",
    "# from tensorflow import ones, reduce_sum\n",
    "\n",
    "# Define a 2x3x4 tensor of ones\n",
    "F = ones([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5b3b6",
   "metadata": {},
   "source": [
    "* If we sum over all elements of A, we get 24, since the tensor contains 24 elements, all of which are 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49144fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over all dimensions\n",
    "D = reduce_sum(F)\n",
    "\n",
    "# Sum over dimensions 0, 1, and 2\n",
    "D0 = reduce_sum(F, 0)\n",
    "D1 = reduce_sum(F, 1)\n",
    "D2 = reduce_sum(F, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7985443",
   "metadata": {},
   "source": [
    "* If we sum over dimension 0, we get a 3 x 4 matrix of 2s\n",
    "* If we sum over 1, we get a 2 by 4 matrix of 3s\n",
    "* If we sum over 2, we get a 2 x3 matrix of 4s\n",
    "* In each case, we reduce the size of the tensor by summing over one of its dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5da389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(24.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf414822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26c01199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e263b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72a3ef",
   "metadata": {},
   "source": [
    "#### Exercises Performing element-wise multiplication\n",
    "Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the $\\odot$ symbol, is shown below:\n",
    "\n",
    "<img src='data/ex1_matmul.png' width=\"200\" height=\"100\" align=\"center\"/>\n",
    "\n",
    "In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that `multiply()`, `constant()`, and `ones_like()` have been imported for you.\n",
    "\n",
    "```\n",
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = ones_like(A1)\n",
    "B23 = ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = A1 * B1\n",
    "C23 = A23 * B23\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('\\n C1: {}'.format(C1.numpy()))\n",
    "print('\\n C23: {}'.format(C23.numpy()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da525257",
   "metadata": {},
   "source": [
    "#### Exercises: Making predictions with matrix multiplication\n",
    "In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, `features`, and a target vector, `bill`, which are taken from a credit card dataset we will use later in the course.\n",
    "\n",
    "<img src='data/mat_mult.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "The matrix of input data, `features`, contains two columns: education level and age. The target vector, `bill`, is the size of the credit card borrower's bill.\n",
    "\n",
    "Since we have not trained the model, you will enter a guess for the values of the parameter vector, `params`. You will then use `matmul()` to perform matrix multiplication of `features` by `params` to generate predictions, `billpred`, which you will compare with `bill`. Note that we have imported `matmul()` and `constant()`.\n",
    "\n",
    "```\n",
    "# Define features, params, and bill as constants\n",
    "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = constant([[1000], [150]])\n",
    "bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = matmul(features, params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill-billpred\n",
    "print(error.numpy())\n",
    "```\n",
    "\n",
    "### Advanced Operations\n",
    "* In this lesson, we explore advanced operations:\n",
    "    * `gradient()`\n",
    "    * `reshape()`\n",
    "    * `random()`\n",
    "    \n",
    "<img src='data/adv_ops.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* **`gradient()`:** \n",
    "    * We will use this function in conjuction with gradient tape\n",
    "    * Computes the slope of a function at a point\n",
    "* **`reshape()`:**\n",
    "    * Changes the shape of a tensor (e.g. 10x10 to 100x1)\n",
    "* **`random()`:**\n",
    "    * Generates a tensor out of randomly-drawn values\n",
    "\n",
    "#### Finding the optimum \n",
    "* In many ML problems, we will need to find the optimum (minimum or maximum) of a function \n",
    "    * **Minimum:** Lowest value of a loss function\n",
    "    * **Maximum:** Highest value of objective function\n",
    "* We can do this using the `gradient()` operation, which tells us the slope of a function at a point\n",
    "    * We start this process by passing points to the gradient operation until we find one where the gradient is zero\n",
    "    * **Optimum:** Find a point where gradient = 0\n",
    "    * **Minimum:** Change in gradient > 0 (if it is increasing, we have a minimum)\n",
    "    * **Maximum:** Change in gradient < 0 (if it is decreasing, we have a maximum)\n",
    "  \n",
    "<img src='data/fixed_gradient.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The plot above shows the function `y = x`; notice that the gradient (the slope at a given point) is constant\n",
    "* This is not true is we instead consider the function `y = x**2` ($y=x^2$)\n",
    "    * When `x` is less than 0, `y` decreases when `x` increases\n",
    "    * When `x` is greater than 0, `y` increases when `x` increases\n",
    "    * Thus, the gradient is initially negative, but becomes positive for `x` larger than 0.\n",
    "    * This means that `x = 0` **minimizes** `y`\n",
    "\n",
    "<img src='data/varying_gradient.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb4e20",
   "metadata": {},
   "source": [
    "### Gradients in TensorFlow\n",
    "* We define `x` as `-1.0`\n",
    "* We then define `y` as `x**2` *within an instance of gradient tape*.\n",
    "* **Note** that we apply the `watch()` method to an instance of gradient tape and then pass the variable `x`.\n",
    "* This will allow us to compute the rate of change of `y` with respect to `x`\n",
    "* Next, we compute the gradient of `y` with respect to `x` using the tape instance of gradient tape\n",
    "* **Note that y is the first argument and x is the second**\n",
    "* As written, the operation computes the slope of `y` at a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14c2812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow under the alias tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "# Define y within instance of GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.multiply(x, x)\n",
    "    \n",
    "# Evaluate the gradient of y at x = -1\n",
    "g = tape.gradient(y, x)\n",
    "print(g.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1cc2d0",
   "metadata": {},
   "source": [
    "* Running the code and printing we find that the slope is -2 at `x = -1`, which means that `y` is initially decreasing in `x`, as seen in the graph above. \n",
    "* Much of the differentiation you do in deep learning models will be handled by high level APIs\n",
    "* However, **gradient tape remains an invaluable tool for building advanced and custom models.**\n",
    "\n",
    "### Reshaping images as tensors\n",
    "* A tool that is particularly usseful for image classification problems: **reshaping**\n",
    "* While some algorithms allow you to exploit the shape of the original image, other require you to `reshape` matrices into vectors before using them as inputs, as shown in the diagram\n",
    "\n",
    "#### Reshaping a grayscale image\n",
    "* Below we create a random grayscale image by drawing numbers from the set of integers between 0 and 255 (grayscale pixel scale) and use these to populate a 2x2 matrix\n",
    "* We can then reshape this into a 4x1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fc41f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow as alias tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Generate grayscale image\n",
    "gray = tf.random.uniform([2, 2], maxval=255, dtype='int32')\n",
    "\n",
    "# Reshape grayscale image\n",
    "gray = tf.reshape(gray, [2*2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfa60d",
   "metadata": {},
   "source": [
    "<img src='data/reshape_grayscale.png' width=\"200\" height=\"100\"/>\n",
    "\n",
    "#### How to reshape a color image\n",
    "* For color images, we generate 3 such matrices to form a 2x2x3 tensor\n",
    "* We could then reshape the image into a 4x3 tensor, as shown in the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89b96364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow as alias tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Generate color image\n",
    "color = tf.random.uniform([2, 2, 3], maxval= 255, dtype='int32')\n",
    "\n",
    "# Reshape color image\n",
    "color = tf.reshape(color, [2*2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2c088",
   "metadata": {},
   "source": [
    "#### Exercises: Reshaping tensors\n",
    "Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images.\n",
    "\n",
    "The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays `gray_tensor` and `color_tensor`. Reshape these arrays into 1-dimensional vectors using the `reshape` operation, which has been imported for you from `tensorflow`. Note that the shape of `gray_tensor` is 28x28 and the shape of `color_tensor` is 28x28x3.\n",
    "\n",
    "<img src='data/asl_a.png' width=\"200\" height=\"100\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44436d",
   "metadata": {},
   "source": [
    "```\n",
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = reshape(gray_tensor, (28*28, 1))\n",
    "\n",
    "# Reshape the color image tensor into a vector\n",
    "color_vector = reshape(color_tensor, (28*28*3, 1))\n",
    "```\n",
    "\n",
    "#### Exercises: Optimizing with gradients\n",
    "You are given a loss function, $y = x^2$, which you want to minimize. You can do this by computing the slope using the `GradientTape()` operation at different values of `x`. If the slope is positive, you can decrease the loss by lowering `x`. If it is negative, you can decrease it by increasing `x`. This is how gradient descent works.\n",
    "\n",
    "<img src='data/varying_gradient.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "In practice, you will use a high level `tensorflow` operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at `x` values of -1, 1, and 0. The following operations are available: `GradientTape()`, `multiply()`, and `Variable()`.\n",
    "\n",
    "```\n",
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = Variable(x0)\n",
    "\twith GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = multiply(x,x)\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))\n",
    "```\n",
    "\n",
    "#### Exercises: Working with image data\n",
    "You are given a black-and-white image of a `letter`, which has been encoded as a tensor, `letter`. You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, `model`, which can be used to classify `letter`.\n",
    "\n",
    "The 3x3 tensor, `letter`, and the 1x3 tensor, `model`, are available in the Python shell. You can determine whether `letter` is a K by multiplying `letter` by `model`, summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, `model` is a collection of weights, arranged in a tensor.\n",
    "\n",
    "Note that the functions `reshape()`, `matmul()`, and `reduce_sum()` have been imported from `tensorflow` and are available for use.\n",
    "\n",
    "```\n",
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model\n",
    "output = matmul(letter, model)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = reduce_sum(output)\n",
    "print(prediction.numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd715b3a",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Linear models\n",
    "In this chapter, you will learn how to build, solve, and make predictions with models in TensorFlow 2. You will focus on a simple class of models – the linear regression model – and will try to predict housing prices. By the end of the chapter, you will know how to load and manipulate data, construct loss functions, perform minimization, make predictions, and reduce resource use with batch training.\n",
    "\n",
    "### Input data\n",
    "In the previous chapter, we focused on how to perform core TensorFlow operations. In this chapter, we will work towards training a linear model with TensorFlow. So far we've only generated data using functions like `ones` and `random_uniform`, however when we train a machine learning model, we will (obviously) want to import data from an external source (whether numeric, image, text, or other data). Beyond simply importing the data, **numeric data will need to be assigned a type, and text and image data will need to be converted to a usable format**. While this is useful for complex data pipelines, it will be unnecessarily complicated for what we do in this chapter. \n",
    "\n",
    "#### Importing data for use in TensorFlow\n",
    "* **Data can be imported using `tensorflow`\n",
    "    * Useful for managing complex pipelines \n",
    "    * Not necessary for this chapter\n",
    "* **Simpler option used in this chapter**\n",
    "    * Import data using `pandas`\n",
    "    * Convert data to `numpy` array\n",
    "    * Use in `tensorflow` without modification\n",
    "    \n",
    "#### How to import and convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e1a0",
   "metadata": {},
   "source": [
    "```\n",
    "# Import numpy and pandas\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Load data from csv\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert to numpy array \n",
    "housing = np.array(housing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7114a2",
   "metadata": {},
   "source": [
    "* We will focus on data stored in csv format in this chapter\n",
    "* pandas also has methods for handling data in other formats\n",
    "    * e.g. `read_json()`, `read_html()`, `read_excel()`\n",
    "    \n",
    "<img src='data/read_csv_params.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The only required parameter to `read_csv` is the filepath or buffer\n",
    "    * **Note:** Instead of a filepath, you can also provide a URL to load data.\n",
    "* `sep` = delimiter; default is comma\n",
    "    * **Note that if you do use whitespace as a delimiter, you will also need to set the `delim_whitespace` parameter to `True`** (default is `False`).\n",
    "* Finally, if you are working with datasets that contain non-ASCII characters, you can specify the appropriate choice of encoding, so that your characters are correctly parsed.\n",
    "\n",
    "#### Using mixed type datasets\n",
    "* How to transform imported data for use in TensorFlow\n",
    "\n",
    "<img src='data/mixed_type_dfs.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### Setting the data type\n",
    "* Let's say we want to perform TensorFlow operations that require `price` to be a 32-bit floating point number and `waterfront` to be a boolean\n",
    "* We can do this in two ways:\n",
    "* 1.\n",
    "    * We select the relevant column in the DataFrame\n",
    "    * Provide relevant column as first argument to `np.array`\n",
    "    * Provide datatype as second argument\n",
    "\n",
    "```\n",
    "# Load KC dataset\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert price column to float32\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Convert waterfront column to Boolean \n",
    "waterfront = np.array(housing['waterfront'], np.bool)\n",
    "```    \n",
    "    \n",
    "* 2. Cast operation from TensorFlow\n",
    "\n",
    "```\n",
    "# Load KC dataset\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert price column to float32\n",
    "price = tf.cast(housing['price'], tf.float32)\n",
    "\n",
    "# Convert waterfront column to Boolean\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "```\n",
    "  \n",
    "* While either `tf.cast` or `np.array` will work, `waterfront` will be a `tf.tensor` type under the former option, and a numpy array under the latter. \n",
    "\n",
    "#### Load data using pandas\n",
    "Before you can train a machine learning model, you must first import data. There are several valid ways to do this, but for now, we will use a simple one-liner from `pandas`: `pd.read_csv()`. Recall from the video that the first argument specifies the path or URL. All other arguments are optional.\n",
    "\n",
    "In this exercise, you will import the King County housing dataset, which we will use to train a linear model later in the chapter.\n",
    "\n",
    "```\n",
    "# Import pandas under the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the path to a string variable named data_path\n",
    "data_path = 'kc_house_data.csv'\n",
    "\n",
    "housing = pd.read_csv(data_path)\n",
    "\n",
    "# Print the price column of housing\n",
    "print(housing.price)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38d3f8",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "* Loss functions play a fundamental role in ML and are a fundamental `tensorflow` operation\n",
    "    * Used to train a model\n",
    "    * Measure of model fit\n",
    "* **Higher value $\\Rightarrow$ worse fit**\n",
    "    * Minimize the loss function (usually)\n",
    "        * But in some cases we may also want to maximize a loss function instead (much less common)\n",
    "        * If this is the case, we can always place a minus sign before the function we want to maximize and minimize it instead\n",
    "        * For this reason, we will always talk about **loss functions and minimization** (because even in the off-change we want to maximize a function, we can always just use the trick mentioned above)\n",
    "        \n",
    "#### Common loss functions in TensorFlow\n",
    "* TensorFlow operations for common loss functions include:\n",
    "    * Mean squared error (MSE)\n",
    "    * Mean absolute error (MAE)\n",
    "    * Huber error\n",
    "* **Loss functions are accessible from `tf.keras.losses()`**\n",
    "    * `tf.keras.losses.mse()`\n",
    "    * `tf.keras.losses.mae()`\n",
    "    * `tf.keras.losses.Huber()`\n",
    "* The loss tells us to what degree our predictions are accurate\n",
    "* Below we plot the MSE, MAE, and Huber loss for error values between -2 and 2\n",
    "\n",
    "<img src='data/common_loss_funcs.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### MSE\n",
    "* Strongly penalizes outliers\n",
    "* High (gradient) sensitivity near minimum \n",
    "\n",
    "#### MAE\n",
    "* Scales linearly with size of error\n",
    "* Low sensitivity near minimum \n",
    "\n",
    "#### Huber\n",
    "* Similar to MSE near minimum\n",
    "* Similar to MAE away from minimum\n",
    "\n",
    "\n",
    "* **For greater sensitivity near the minimum, you will want to use the MSE or Huber loss.**\n",
    "* **To minimize the impact of outliers, you will want to use the MAE or Huber loss.**\n",
    "\n",
    "#### Defining a loss function\n",
    "* Let's say we decide to use the MSE loss:\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "# Compute the MSE loss\n",
    "loss = tf.keras.losses.mse(targets, predictions)\n",
    "```\n",
    "* In many cases, the training process will require us to supply a function that accepts our model's variables and data and returns a loss\n",
    "* Here we'll first define a model, \"linear_regression,\" which takes the intercept, slope, and features as arguments and returns the model's predictions\n",
    "\n",
    "```\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope = slope, features = features):\n",
    "    return intercept + features*slope\n",
    "```\n",
    "\n",
    "* Next we'll define a loss function called `loss_function` that accepts the slope and intercept of a linear model-- the variables-- and the input data, the targets and the features.\n",
    "* It then makes a prediction and computes and returns the associated MSE loss\n",
    "\n",
    "```\n",
    "# Define a loss function to compute the MSE\n",
    "def loss_function(intercept, slope, targets = targets, features = features):\n",
    "    # Compute the predictions for the linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "```\n",
    "\n",
    "* **Note that we've defined both functions to use default argument values for features and targets.**\n",
    "* We will do this whenever we train on the full sample to simplify the code\n",
    "* Also notice that we've nested TensorFlow's MSE loss function within a function that first uses the model to make predictions and then uses those predictions as an input to the MSE loss function\n",
    "* We can then evaluate this function for a given set of parameter values and input data\n",
    "\n",
    "```\n",
    "# Compute the loss for test data inputs\n",
    "loss_function(intercept, slope, test_targets, test_features)\n",
    "```\n",
    "* Note that if we had omitted the data argumetns, test_targets, and test_features, the loss function would have instead used the default targets and features argumetns we set to evaluate model performance.\n",
    "\n",
    "#### Exercises: Loss functions in TensorFlow\n",
    "\n",
    "Compute the loss using data from the King County housing dataset. You are given a target, price, which is a tensor of house prices, and predictions, which is a tensor of predicted house prices. You will evaluate the loss function and print out the value of the loss.\n",
    "\n",
    "```\n",
    "# Import the keras module from tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Compute the mean absolute error (mae)\n",
    "loss = keras.losses.mse(price, predictions)\n",
    "\n",
    "# Print the mean absolute error (mae)\n",
    "print(loss.numpy())\n",
    "```\n",
    "\n",
    "#### Exercises: Modifying the loss function\n",
    "In the previous exercise, you defined a `tensorflow` loss function and then evaluated it once for a set of actual and predicted values. In this exercise, you will compute the loss within another function called `loss_function()`, which first generates predicted values from the data and variables. The purpose of this is to construct a function of the trainable model variables that returns the loss. You can then repeatedly evaluate this function for different variable values until you find the minimum. In practice, you will pass this function to an optimizer in `tensorflow`. Note that `features` and `targets` have been defined and are available. Additionally, `Variable`, `float32`, and `keras` are available.\n",
    "\n",
    "```\n",
    "# Initialize a variable named scalar\n",
    "scalar = Variable(1.0, float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features = features):\n",
    "  \treturn scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features = features, targets = targets):\n",
    "\t# Compute the predicted values\n",
    "\tpredictions = model(scalar, features)\n",
    "    \n",
    "\t# Return the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4ed33",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "* A **linear regression model assumes a linear relationship:**\n",
    "    * $price = intercept + size * slope + error$\n",
    "* **This is an example of a univariate regression.**\n",
    "    * There is only one feature, `size`.\n",
    "* **Multiple regression models have more than one feature**\n",
    "    * e.g. `size` and `location`\n",
    "    \n",
    "```\n",
    "# Define the targets and features \n",
    "price = np.array(housing['price'], np.float32)\n",
    "size = np.array(housing['sqft_living'], np.float32)\n",
    "\n",
    "# Define the intercept and slope\n",
    "intercept = tf.Variable(0.1, np.float32)\n",
    "slope = tf.Variable(0.1, np.float32)\n",
    "```\n",
    "* A univariate linear regression identifies the relationship between a single feature and the target tensor.\n",
    "\n",
    "#### Exercises: Multiple linear regression\n",
    "In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature.\n",
    "\n",
    "You will use `price_log` as your target and `size_log` and `bedrooms` as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: `keras.losses.mae()`. Finally, the predicted values are computed as follows: `params[0] + feature1*params[1] + feature2*params[2]`. Note that we've defined a vector of parameters, `params`, as a variable, rather than using three variables. Here, `params[0]` is the intercept and `params[1]` and `params[2]` are the slopes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a080000",
   "metadata": {},
   "source": [
    "### Batch training\n",
    "* We've now learned how to train a linear model to predict house prices.\n",
    "* Now we will use batch training to handle large datasets\n",
    "\n",
    "#### What is batch training?\n",
    "* Let's pretend the KC Housing dataset is much larger, and we want to perform the training on a GPU, which has only a small amount of memory\n",
    "* **Since you can't fit the entire dataset in memory, you will instead divide it into batches and then train on those batches sequentially.**\n",
    "\n",
    "<img src='data/batch_training.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* A single pass over all of the batches is called **epoch** and the process itself is called **batch training**.\n",
    "* **Batch training** is extremely useful when working with large image datasets\n",
    "* **Batch training will also allow you to update model weights and optimizer parameters after each batch, rather than at the end of an epoch.**\n",
    "\n",
    "#### The chunksize parameter\n",
    "* `pd.read_csv()` allows us to load data in batches\n",
    "    * To avoid loading the entire dataset at once, **`chunksize`** parameter specifies batch size.\n",
    "    \n",
    "* Below, instead of loading the data in a single one-liner, we'll write a for loop that iterates through the data in steps of 100 examples\n",
    "* Each 100 will be available as a batch, which we can use to extract columns, such as `price` and `size` in the housing dataset\n",
    "* We can then convert these to numpy arrays and use them to train\n",
    "* Being able to load data from csv filed in fixed-sized batches using pandas allows us to handle datasets of tens or even hundreds of gigabytes without excedding the memory constraints of our system.\n",
    "\n",
    "```\n",
    "#Import pandas and numpy\n",
    "import pandas as pd\n",
    "import numypy as np\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
    "    # Extract price column\n",
    "    price = np.array(batch['price'], np.float32)\n",
    "    \n",
    "    # Extract size column\n",
    "    size = np.array(batch['size'], np.float32)\n",
    "```\n",
    "\n",
    "### Training a linear model in batches\n",
    "\n",
    "```\n",
    "# Define trainable variables\n",
    "intercept = tf.Variable(0.1, tf.float32)\n",
    "slope = tf.Variable(0.1, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    return intercept + features * slope\n",
    "    \n",
    "# Compute predicted values and return loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "    predictions = linear_regression(intercept, slope, features)\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "    \n",
    "# Define optimization operation\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "```\n",
    "#### The next step is to train the model in batches\n",
    "* We do this, once again, by using a for loop and supplying a chunksize to the read csv function\n",
    "* Note that we take each batch,\n",
    "    * separate it into features and a target, \n",
    "    * convert those into numpy arrays,\n",
    "    * and then pass them to the minimize operation\n",
    "* Within the minimize operation, we pass the loss function as a lambda function and we supply a variable list that contains only the trainable parameters, (intercept, and slope).\n",
    "* This loop will continue until we have stepped through all of the examples in `read_csv`\n",
    "* **Importantly, we did not ever need to have more than 100 examples in memory during the entire process.**\n",
    "* Finally, we print our trained intercept and slope\n",
    "\n",
    "```\n",
    "# Load the data in batches from pandas\n",
    "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
    "    # Extract the target and feature columns\n",
    "    price_batch = np.array(batch['prce'], np.float32)\n",
    "    size_batch = np.array(batch['lot_size'], np.float32)\n",
    "    \n",
    "    # Minimize the loss function\n",
    "    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print parameter values\n",
    "print(intercept.numpy(), slope.numpy())\n",
    "```\n",
    "\n",
    "* **Note that we did not use default argument values for input data. This is because our input data was generated in batches during the training process.**\n",
    "\n",
    "<img src='data/fullsample_vs_batch.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* In later chapters, we'll automate batch training by using high-level APIs\n",
    "* Importantly, however, high-level APIs will not typically load the sample in batches by default, as we have done here (above).\n",
    "\n",
    "#### Exercises: Preparing to batch train\n",
    "Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict `price_batch`, a batch of house prices, using `size_batch`, a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using `pandas`, converting it to `numpy` arrays, and then using it to minimize the loss function in steps.\n",
    "\n",
    "Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process.\n",
    "\n",
    "```\n",
    "# Define the intercept and slope\n",
    "intercept = Variable(10.0, float32)\n",
    "slope = Variable(0.5, float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "\t# Define the predicted values\n",
    "\treturn intercept + slope * features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "\t# Define the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    " \t# Define the MSE loss\n",
    "\treturn keras.losses.mse(targets, predictions)\n",
    "```\n",
    "\n",
    "#### Exercises: Training a linear model in batches\n",
    "In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model's variables, `intercept` and `slope`, after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory.\n",
    "\n",
    "Note that the loss function, `loss_function(intercept, slope, targets, features)`, has been defined for you. Additionally, `keras` has been imported for you and `numpy` is available as `np`. The trainable variables should be entered into `var_list` in the order in which they appear as loss function arguments.\n",
    "\n",
    "```\n",
    "# Initialize Adam optimizer\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b99c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee84950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89941de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "494650e7",
   "metadata": {},
   "source": [
    "<img src='data/mat_mult.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
