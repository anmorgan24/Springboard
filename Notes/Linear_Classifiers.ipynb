{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db8572d",
   "metadata": {},
   "source": [
    "# Linear Classifiers in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173444a0",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "* Logistic Regression is a linear classifier\n",
    "* sklearn's Logistic Regression can also output confidence scores rather than \"hard\" or definite predictions with `.predict_proba()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6f84e",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "#### Using Linear SVC\n",
    "* In sklearn the basic SVM classifier is called `LinearSVC()` or Linear Support Vector Classifier\n",
    "* Note that sklearn's Logistic Regression and SVM implementations handle multiple classes (if a dataset has more than 2 classes) automatically.\n",
    "\n",
    "#### Using SVC\n",
    "* The SVC class fits a nonlinear SVM by default\n",
    "\n",
    "* **Underfitting:** model is too simple, training accuracy low\n",
    "* **Overfitting:** model is too complex, testing accuracy low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782810d1",
   "metadata": {},
   "source": [
    "#### Linear Decision Boundaries\n",
    "* A decision boundary tells us what class our classifier will predict for any value of x\n",
    "* A decision boundary is considered **linear** when it is a line (in any orientation)\n",
    "    * This definition extends to (classifying) more than 2 features\n",
    "    * For five features, the space of possible x-values would be five-dimensional. In this case, the boundary would be a higher-dimensional **hyperplane** cutting the space into two halves.\n",
    "* A **nonlinear** boundary is any other type of boundary.\n",
    "    * Sometimes this leads to non-contiguous regions regions of a certain prediction (\"islands\", etc).\n",
    "* In their basic forms, logistic regression and SVMs are linear classifiers, which means they learn linear decision boundaries.\n",
    "    * However in some more complex forms, both may learn nonlinear decision boundaries\n",
    "\n",
    "#### Vocabulary:\n",
    "* **Classification:** learning to predict categories\n",
    "* **Regression:** learning to predict a continuous value\n",
    "* **Decision boundary:** the surface separating different predicted classes\n",
    "* **Linear classifier:** a classifier that learns linear decision boundaries \n",
    "    * e.g. logistic regression, linear SVM\n",
    "* **Linearly separable:** A data set is called linearly separable if it can be perfectly explained by a linear classifier **(straight line)**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b64f23",
   "metadata": {},
   "source": [
    "#### Linear Classifiers: Prediction Equations\n",
    "\n",
    "#### Dot products\n",
    "* Create two arrays, x and y:\n",
    "\n",
    "```\n",
    "x = np.arange(3)\n",
    "y = np.arange(3, 6)\n",
    "```\n",
    "* `x = array([0, 1, 2])`\n",
    "* `y = array([3, 4, 5])`\n",
    "\n",
    "* To take the **dot product** between these two arrays, we need to multiply them element-wise.\n",
    "* The result is:\n",
    "    * 1. `x` * `y` == `array([0, 4, 10])`\n",
    "    * 2. The sum of the numbers in this array (0 + 4 + 10) or `np.sum(x*y)` = `14`\n",
    "* A convenient notation for this is `@`\n",
    "    * `x@y` = 14\n",
    "    * In math notation, this is written x dot y\n",
    "* You can think of a **dot product** as multiplication in higher dimensions, since x and y are arrays of values\n",
    "* Using dot products, we can express how linear classifiers make predictions \n",
    "\n",
    "#### Linear classifier predictions:\n",
    "* `raw model output = coefficients * features + intercept`\n",
    "    * Dot product of coefficients and features, plus an intercept.\n",
    "* Linear classifier prediction: compute raw model output, check the **sign**:\n",
    "    * If **positive**, predict one class\n",
    "    * If **negative**, predict the other class\n",
    "    \n",
    "* Crucially, this pattern is the same for logistic regression and linear SVMs\n",
    "* In sklearn terms, we can say logistic regression and linear SVM have different `fit` functions but same `predict` function.\n",
    "    * The differences in `fit` relate to loss functions\n",
    "    \n",
    "* We can get the learned coefficients and intercept with:\n",
    "    * `lr.coef_`\n",
    "    * `lr.intercept_`\n",
    "* To compute raw model output for example 10:\n",
    "    * `lr.coef_ @ X[10] + lr.intercept_`\n",
    "        * If the raw model output is negative, then we predict the negative class (\"0\", for example)\n",
    "* In general, this is what the predict function does for *any* X: it computes the raw model output, checks if it's positive or negative, and then returns result based on the names of the classes in your data set (for example, \"0\" and \"1\").\n",
    "* The sign (positive or negative), tells you what side of the decision boundary you're on, and thus, your prediction\n",
    "* Along the decision boundary itself, the raw model output is zero\n",
    "* Furthermore, the values of the coefficients and intercept determine the boundary \n",
    "* When you call `fit` with scikit-learn, the logistic regression coefficients are automatically learned from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23cdc3",
   "metadata": {},
   "source": [
    "#### Loss functions\n",
    "* Many machine learning algorithms involve minimizing a loss function (example loss function: least squares)\n",
    "* You can think of minimizing the loss as jiggling around the coefficients or parameters of the model until this error term, or loss function is as small as possible\n",
    "* In other words, the loss function is a penalty score that tells us how well (or, to be precise, how poorly) the model is doing on the training data\n",
    "* We can think of the `fit` function as **running code that minimizes the loss.**\n",
    "* \"Minimization\" is with respect to the coefficients or parameters of the model\n",
    "* **Note** that the `.score` function in sklearn isn't necessarily the loss function\n",
    "* **The loss is used to fit the model on the data and the score is used to see how well we're doing.**\n",
    "* Often, however, they are the same.\n",
    "\n",
    "#### Classification errors: the 0-1 loss\n",
    "* Squared loss is not appropriate for classification problems, because our y-values are categories, not numbers\n",
    "* For classification, a natural quantity to think about is the number of errors we've made \n",
    "* This is the **0-1 loss**: it's 0 for a correct prediction and 1 for an incorrect prediction\n",
    "* By summing this function over all the training samples, we get the total number of mistakes we've made on the training set, since we add 1 to the total for each mistake\n",
    "* While the 0-1 loss is important to understand conceptually, it turns out to be very hard to minimize it directly in practice (which is why logistic regression and SVM don't use it)\n",
    "\n",
    "#### Minimizing a loss\n",
    "* with `scipy.optimize.minimize`\n",
    "* `minimize(np.square, 0).x`\n",
    "    * first argument represents equation to be minimized: $y=x^2$\n",
    "    * the second argument is our initial guess \n",
    "    * `.x` at the end to grab the input value that makes the function as small as possible\n",
    "    * Think of the code as answering the question, \"What values of the model coefficients make my sqaured error as small as possible?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b7d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84427a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6490891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044c821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae585d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05f2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b41a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307abbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79840d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
