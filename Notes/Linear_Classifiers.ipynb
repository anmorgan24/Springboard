{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c95550",
   "metadata": {},
   "source": [
    "# Linear Classifiers in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49958953",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "* Logistic Regression is a linear classifier\n",
    "* sklearn's Logistic Regression can also output confidence scores rather than \"hard\" or definite predictions with `.predict_proba()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2b1a8",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "#### Using Linear SVC\n",
    "* In sklearn the basic SVM classifier is called `LinearSVC()` or Linear Support Vector Classifier\n",
    "* Note that sklearn's Logistic Regression and SVM implementations handle multiple classes (if a dataset has more than 2 classes) automatically.\n",
    "\n",
    "#### Using SVC\n",
    "* The SVC class fits a nonlinear SVM by default\n",
    "\n",
    "* **Underfitting:** model is too simple, training accuracy low\n",
    "* **Overfitting:** model is too complex, testing accuracy low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d70baf",
   "metadata": {},
   "source": [
    "#### Linear Decision Boundaries\n",
    "* A decision boundary tells us what class our classifier will predict for any value of x\n",
    "* A decision boundary is considered **linear** when it is a line (in any orientation)\n",
    "    * This definition extends to (classifying) more than 2 features\n",
    "    * For five features, the space of possible x-values would be five-dimensional. In this case, the boundary would be a higher-dimensional **hyperplane** cutting the space into two halves.\n",
    "* A **nonlinear** boundary is any other type of boundary.\n",
    "    * Sometimes this leads to non-contiguous regions regions of a certain prediction (\"islands\", etc).\n",
    "* In their basic forms, logistic regression and SVMs are linear classifiers, which means they learn linear decision boundaries.\n",
    "    * However in some more complex forms, both may learn nonlinear decision boundaries\n",
    "\n",
    "#### Vocabulary:\n",
    "* **Classification:** learning to predict categories\n",
    "* **Regression:** learning to predict a continuous value\n",
    "* **Decision boundary:** the surface separating different predicted classes\n",
    "* **Linear classifier:** a classifier that learns linear decision boundaries \n",
    "    * e.g. logistic regression, linear SVM\n",
    "* **Linearly separable:** A data set is called linearly separable if it can be perfectly explained by a linear classifier **(straight line)**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e38086",
   "metadata": {},
   "source": [
    "#### Linear Classifiers: Prediction Equations\n",
    "\n",
    "#### Dot products\n",
    "* Create two arrays, x and y:\n",
    "\n",
    "```\n",
    "x = np.arange(3)\n",
    "y = np.arange(3, 6)\n",
    "```\n",
    "* `x = array([0, 1, 2])`\n",
    "* `y = array([3, 4, 5])`\n",
    "\n",
    "* To take the **dot product** between these two arrays, we need to multiply them element-wise.\n",
    "* The result is:\n",
    "    * 1. `x` * `y` == `array([0, 4, 10])`\n",
    "    * 2. The sum of the numbers in this array (0 + 4 + 10) or `np.sum(x*y)` = `14`\n",
    "* A convenient notation for this is `@`\n",
    "    * `x@y` = 14\n",
    "    * In math notation, this is written x dot y\n",
    "* You can think of a **dot product** as multiplication in higher dimensions, since x and y are arrays of values\n",
    "* Using dot products, we can express how linear classifiers make predictions \n",
    "\n",
    "#### Linear classifier predictions:\n",
    "* `raw model output = coefficients * features + intercept`\n",
    "    * Dot product of coefficients and features, plus an intercept.\n",
    "* Linear classifier prediction: compute raw model output, check the **sign**:\n",
    "    * If **positive**, predict one class\n",
    "    * If **negative**, predict the other class\n",
    "    \n",
    "* Crucially, this pattern is the same for logistic regression and linear SVMs\n",
    "* In sklearn terms, we can say logistic regression and linear SVM have different `fit` functions but same `predict` function.\n",
    "    * The differences in `fit` relate to loss functions\n",
    "    \n",
    "* We can get the learned coefficients and intercept with:\n",
    "    * `lr.coef_`\n",
    "    * `lr.intercept_`\n",
    "* To compute raw model output for example 10:\n",
    "    * `lr.coef_ @ X[10] + lr.intercept_`\n",
    "        * If the raw model output is negative, then we predict the negative class (\"0\", for example)\n",
    "* In general, this is what the predict function does for *any* X: it computes the raw model output, checks if it's positive or negative, and then returns result based on the names of the classes in your data set (for example, \"0\" and \"1\").\n",
    "* The sign (positive or negative), tells you what side of the decision boundary you're on, and thus, your prediction\n",
    "* Along the decision boundary itself, the raw model output is zero\n",
    "* Furthermore, the values of the coefficients and intercept determine the boundary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ef501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa515f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20869b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349876a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b4307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb77654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43b588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443acef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d5989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d90369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a27604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47adb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
