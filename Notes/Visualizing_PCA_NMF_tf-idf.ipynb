{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7501553a",
   "metadata": {},
   "source": [
    "# Visualizing PCA, NMF, tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a80c36",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "* Dimension reduction finds patterns in data and uses these patterns to re-express it in a compressed form\n",
    "* More effecient storage and computation- a big deal in a world of big data sets\n",
    "* Removes less-informative \"noise\" features (which cause problems for prediction tasks\n",
    "* In many real-world scenarios, it's dimension reduction that makes prediction possible.\n",
    "* PCA is the most fundamental of dimension reduction techniques\n",
    "\n",
    "### PCA = Principal Component Analysis\n",
    "* Fundamental dimension reduction technique\n",
    "* Performs dimensino reduction in two steps\n",
    "    1) __\"Decorreation\"__ (doesn't reduce the dimension at all\n",
    "    2) __Dimension Reduction__\n",
    "    \n",
    "#### 1) Decorrelation:\n",
    "* In this first step, PCA rotates data samples to be aligned with axes\n",
    "* PCA shifts data samples so they have mean 0\n",
    "* Note that no information is lost\n",
    "\n",
    "* PCA follows the fit/transform pattern\n",
    "* `PCA` is a scikit learn component like `KMeans` or `StandardScaler`\n",
    "* `fit()` learns the transformation from given data (learns how to shift and how to rotate the samples but doesn't actually change them)\n",
    "* `transform()` applies the learned transformation that `fit()` learned\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA()\n",
    "model.fit(samples)\n",
    "transformed = model.transform(samples)\n",
    "```\n",
    "* This returns a new array of transformed samples\n",
    "* This new array has the same number of rows and columns as the original sample array. In particular there is one row for each transformed sample.\n",
    "* Columns of the new array correspond to PCA features \n",
    "* __PCA features are not correlated__\n",
    "    * PCA, due to the rotation (/alignment along the axis) of the data, de-correlates it.\n",
    "* \"Principal Components\" = \"directions of variance\"\n",
    "* PCA aligns principal components with the axes\n",
    "* After a PCA model has been fit, principal components are available as `.components_` attribute. This is a numpy array with one row for each observation.\n",
    "   \n",
    "    \n",
    "* Linear correlation can be measured with __Pearson Correlation Coefficient__\n",
    "    * Values between -1 and 1 \n",
    "    * Value of 0 means no linear correlation \n",
    "    * The closer the value to either 1 or -1, the higher the correlation\n",
    "\n",
    "```\n",
    "#Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "#Assign the 0th column of grains: width\n",
    "width = grains[:,0]\n",
    "#Assign the 1st column of grains: length\n",
    "length = grains[:,1]\n",
    "#Scatter plot width vs length\n",
    "plt.scatter(width, length)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "#Calculate the Pearson correlation\n",
    "correlation, pvalue = pearsonr(width,length)\n",
    "#Display the correlation\n",
    "print(correlation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501efdd0",
   "metadata": {},
   "source": [
    "```\n",
    "#Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "#Create PCA instance: model\n",
    "model = PCA()\n",
    "#Apply the fit_transform method of model to grains: pca_features\n",
    "pca_features = model.fit_transform(grains)\n",
    "#Assign 0th column of pca_features: xs\n",
    "xs = pca_features[:,0]\n",
    "#Assign 1st column of pca_features: ys\n",
    "ys = pca_features[:,1]\n",
    "#Scatter plot xs vs ys\n",
    "plt.scatter(xs, ys)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "#Calculate the Pearson correlation of xs and ys\n",
    "correlation, pvalue = pearsonr(xs, ys)\n",
    "#Display the correlation\n",
    "print(correlation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf0146",
   "metadata": {},
   "source": [
    "### Intrinsic dimension\n",
    "* __Intrinsic dimension:__ the intrinsic dimension of a dataset is the number of features _needed_ to approximate the dataset. \n",
    "* Essential idea behind dimension reduction \n",
    "* What is the most compact representation of the samples?\n",
    "    * Example: airplane location\n",
    "    * you have a dataset of longitude, latitude coordinates for an airplane (2-D)\n",
    "    * dataset is intrinsically 1-D, by using _location displacement_ instead of location coordinates.\n",
    "* __PCA identifies intrinsic dimension when samples have any number of features.__\n",
    "* __Intrinsic dimension = number of PCA features with significant variance.__\n",
    "* PCA features are ordered by variance descending.\n",
    "\n",
    "* Plotting the variances of PCA features:\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(samples)\n",
    "```\n",
    "* Now create a range enumerating the PCA features and make a bar plot of the variances\n",
    "```\n",
    "feature = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xticks(features)\n",
    "plt.xlabel=('PCA feature')\n",
    "plt.ylabel=('variance')\n",
    "plt.show()\n",
    "```\n",
    "* intrinsic dimension can be ambiguous\n",
    "* intrinsic dimension is an idealization\n",
    "* there is not always one correct answer\n",
    "\n",
    "* The first principal component of the data is the direction in which the data varies the most.\n",
    "\n",
    "```\n",
    "#Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "#Create a PCA instance: model\n",
    "model = PCA()\n",
    "#Fit model to points\n",
    "model.fit(grains)\n",
    "#Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "#Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "#Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "#Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "#Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "#Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "#Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "#Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "#Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "#Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87561e14",
   "metadata": {},
   "source": [
    "#### Dimension reduction with PCA\n",
    "* Represents same data, using less features\n",
    "* important part of machine-learning pipelines\n",
    "* can be performed using PCA\n",
    "* dimesion reduction with PCA assumes the low variance features are noise and high variance features are informative\n",
    "* to use PCA for dimension reduction, you need to specify how many features to keep, e.g. `PCA(n_components=2)`\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(samples)\n",
    "transformed = pca.transform(samples)\n",
    "print(transformed.shape)\n",
    "```\n",
    "* PCA discards low variance features and assumes the high variance features are informative\n",
    "\n",
    "#### Word frequency arrays\n",
    "* word frequencies (\"tf-idf\")\n",
    "* rows represent documents, columns represent words (from a fixed vocabulary)\n",
    "* most entries of the word frequency array are 0 (sparse)\n",
    "* Arrays like this, with a lot of zeros, are said to be \"sparse\" and are often represented using a special type of array called a csr matrix: `scipy.sparse.csr_matrix`\n",
    "* `csr_matrix` saves space by only remembering the non-zero entries\n",
    "* sci-kit learn's `PCA` doesn't support `csr_matrix`\n",
    "* Use sklearn's `TruncatedSVD` instead, which performs same transformation\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "model = TruncatedSVD(n_components=3)\n",
    "model.fit(documents) #documents is csr_matrix\n",
    "transformed = model.transform(documents)\n",
    "```\n",
    "\n",
    "```\n",
    "#Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "#Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "#Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaled_samples)\n",
    "#Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaled_samples)\n",
    "#Print the shape of pca_features\n",
    "print(pca_features.shape)\n",
    "```\n",
    "#### tf-idf word frequency array\n",
    "* `TfidfVectorizer` from sklearn; transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has `fit()` and `transform()` methods like other sklearn objects.\n",
    "\n",
    "```\n",
    "#Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#Create a TruncatedSVD instance: svd\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "#Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "#Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n",
    "```\n",
    "\n",
    "```\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "#Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "#Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(articles)\n",
    "#Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "#Display df sorted by cluster label\n",
    "print(df.sort_values('label'))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0334d02",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)\n",
    "* __NMF:__ Non-negaive matrix factorization\n",
    "* NMF, like PCA, is a dimension reduction technique\n",
    "* NMF models are interprettable (unlike PCA)\n",
    "* NMF models are easy to interpret and explain\n",
    "* However, requires all sample features be non-negative (>=0)\n",
    "* NMF achieves its interpretability by decomposing samples as sums of their parts\n",
    "    * NMF expresses _documents_ as combinations of topics (or \"themes\")\n",
    "    * NMF expresses _images_ as combinations of patterns.\n",
    "* NMF available in `sklearn`\n",
    "* Follows `fit()` / `transform()` (same as PCA)\n",
    "* However, unlike PCA, the desired number of components must always be specified: `NMF(n_components=2)`\n",
    "* NMF works with numpy arrays and sparse arrays in the `csr_matrix` format\n",
    "* __tf-idf:__\n",
    "    * __tf:__ frequency of word in the document. Example: if 10% of the words in Document1 are 'DataCamp', then the `tf` for DataCamp for that document is 0.1.\n",
    "    * __idf:__ is a weighting scheme that reduces the influence of frequent words like \"the\"\n",
    "    \n",
    "```\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2)\n",
    "model.fit(samples)\n",
    "nmf_features = model.transform(samples)\n",
    "print(model.components_)\n",
    "print(nmf_features)\n",
    "```\n",
    "* Just like PCA has components, NMF has components which it learns from the samples\n",
    "* As with PCA, the dimension of the components of NMF is the same as the dimension of samples\n",
    "* Entries of the NMF component are _always_ non-negative\n",
    "* NMF feature values are _always_ non-negative as well\n",
    "* the features and the components can be used to (approximately) reconstruct the original data set.\n",
    "__Reconstruction of a sample:__\n",
    "\n",
    "```\n",
    "print(samples[i,:])\n",
    "print(nmf_features[i,:])\n",
    "```\n",
    "* If we multiply each of the NMF components (directly above) by the corresponding NMF feature value (also directly above) and add up each column, we get something very close to the original sample.\n",
    "* A sample can be reconstructed by multiplying components by feature values and adding them up.\n",
    "* Can also be expressed as a product of matrices (this is the **M**atrix **F**actorization of **NMF**)\n",
    "* Again, **NMF fits to nonnegative data only** (like in word frequency arrays, images encoded as arrays, arrays encoding audio spectograms, arrays representing the purchase histories on e-commerce sites).\n",
    "\n",
    "```\n",
    "#Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "#Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "#Fit the model to articles\n",
    "model.fit(articles)\n",
    "#Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)\n",
    "#Print the NMF features\n",
    "print(nmf_features.round(2))\n",
    "```\n",
    "\n",
    "```\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "#Create a pandas DataFrame: df\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "#Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway'])\n",
    "#Print the row for 'Denzel Washington'\n",
    "print(df.loc['Denzel Washington'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0d6c9",
   "metadata": {},
   "source": [
    "* Apply NMF model to articles:\n",
    "\n",
    "```\n",
    "print(articles.shape) # =(20000,800)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=10)\n",
    "nmf.fit(articles)\n",
    "print(nm.components_.shape) # =(10,800): 10 components defined above, 800 words chosen from original dataset\n",
    "```\n",
    "* NMF rows, or components live in an 800-dimensional space (1 dimension for each of the words)\n",
    "* Choosing a component and looking at which words have the highest values, we see that they fit a theme. Top words:\n",
    "    * species\n",
    "    * plant\n",
    "    * plants\n",
    "    * genetic\n",
    "    * evolution\n",
    "    * life\n",
    "* **So, if NMF is applied to documents, then the components correspond to topics.** And the NMF features reconstruct the documents from the topics\n",
    "* **For documents:**\n",
    "    * NMF components represent topics\n",
    "    * NMF features combine topics into documents\n",
    "* **For images:**\n",
    "    * NMF components are parts of images (patterns that frequently occur in the images)\n",
    "    \n",
    "**Representing a collection of images as a non-negative array:**\n",
    "* Grayscale image = no colors, only shades of gray ranging from black to white\n",
    "* Since there are only shades of gray (and no colors), a grayscale image can be encoded with __measures of pixel brightness__ \n",
    "* **Measures of pixel brightness** is represented with a value between 0 and 1 (0 is black)\n",
    "* Then the image can be represented as a 2-D array\n",
    "* These 2-D arrays of numbers can then be flattened by __enumerating the entries.__ For instance, we could read off the entries row by row, from left to right, top to bottom.\n",
    "* Thus, a collection of images of the same size can be encoded as a 2-D array in which each row corresponds to an image and each column represents a pixel\n",
    "* Viewing the images as samples, and the pixels as features, we see that the data is arranged similarly to the word frequency array\n",
    "* Since all entries are non-negative, NMF can be used to learn the parts of images\n",
    "* to recover the image: \n",
    "\n",
    "```\n",
    "bitmap = sample.reshape((2,3))\n",
    "print(bitmap)\n",
    "```\n",
    "* This yields a 2-D array of pixel brightness measurements\n",
    "* To display the corresponding image:\n",
    "\n",
    "```\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(bitmap, cmap= 'gray', interpolation= 'nearest')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "#Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "#Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "#Select row 3: component\n",
    "component = components_df.iloc[3,:]\n",
    "#Print result of nlargest\n",
    "print(component.nlargest())\n",
    "```\n",
    "\n",
    "```\n",
    "#Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "#Select the 0th row: digit\n",
    "digit = samples[0,:]\n",
    "#Print digit\n",
    "print(digit)\n",
    "#Reshape digit to a 13x8 array: bitmap\n",
    "bitmap = digit.reshape(13,8)\n",
    "#Print bitmap\n",
    "print(bitmap)\n",
    "#Use plt.imshow to display bitmap\n",
    "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "```\n",
    "* 7 is the number of cells in an LED\n",
    "\n",
    "```\n",
    "#Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "#Create an NMF model: model\n",
    "model = NMF(n_components=7)\n",
    "#Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "#Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "#Assign the 0th row of features: digit_features\n",
    "digit_features = features[0,:]\n",
    "#Print digit_features\n",
    "print(digit_features)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ebd9b",
   "metadata": {},
   "source": [
    "# Function to display image version of any 1D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e486574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_as_image(sample):\n",
    "    bitmap = sample.reshape((13, 8))\n",
    "    plt.figure()\n",
    "    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a6134",
   "metadata": {},
   "source": [
    "### Other:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d4d07",
   "metadata": {},
   "source": [
    "* Unlike NMF, PCA doesn't learn the parts of things. \n",
    "* Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images.\n",
    "\n",
    "```\n",
    "#Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "#Create a PCA instance: model\n",
    "model = PCA(n_components=7)\n",
    "#Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "#Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b308baa",
   "metadata": {},
   "source": [
    "## Building recommender systems using NMF\n",
    "* Task: finding similar articles\n",
    "* Similar articles should have similar topics and similar NMF features\n",
    "* Strategy:\n",
    "    * Apply NMF to the word-frequency array of the articles \n",
    "    * \n",
    "    \n",
    "```\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components =6)\n",
    "nmf_features = nmf.fit_transform(articles)\n",
    "```\n",
    "* Now we've got NMF features for every article, given by the columns of the new array\n",
    "* Now we need to define how to compare the articles, using their NMF features\n",
    "* Different versions of the same document have same/similar topic proportions, but it isn't always the case that the NMF feature values are exactly the same\n",
    "    * For instance, one version of a document may use very direct language, whereas another might interweave content with \"meaningless chatter\"\n",
    "    * \"Meaningless chatter\" reduces the frequency of the topic words overall, which reduces the values of the NMF features representing the topics. \n",
    "    * However, **on a scatterplot of the NMF features, all these versions lie on a single line passing through the origin.**\n",
    "    * For this reason, when comparing two documents,  it's a good idea to compare these lines. \n",
    "* **Cosine similarity:** uses the angle between the lines; higher values mean more similarity; maximum value is 1, when angle is 0 degrees.\n",
    "* **Calculating the cosine similarity:**\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import normalize\n",
    "norm_features = normalize(nmf_features)\n",
    "#now select row corresponding to current article; if has index 23:\n",
    "current_article = norm_features[23,:] \n",
    "similarities = norm_features.dot(current_article)\n",
    "print(similarities)\n",
    "```\n",
    "* This results in the cosine similarities\n",
    "* With the help of a pandas dataframe, we can label the similarities with the article titles\n",
    "* titles given as a list: `titles`\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "norm_features = normalize(nmf_features)\n",
    "df = pd.DataFrame(norm_features, index = titles)\n",
    "current_article = df.loc['Dog bites man']\n",
    "similarities = df.dot(current_article)\n",
    "print(similarities.nlargest())\n",
    "```\n",
    "\n",
    "```\n",
    "#Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "#Normalize the NMF features: norm_features\n",
    "norm_features = normalize(nmf_features)\n",
    "#Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "#Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "article = df.loc['Cristiano Ronaldo']\n",
    "#Compute the dot products: similarities\n",
    "similarities = df.dot(article)\n",
    "#Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())\n",
    "```\n",
    "\n",
    "```\n",
    "#Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "#Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "#Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "#Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "#Apply fit_transform to artists: norm_features\n",
    "norm_features = pipeline.fit_transform(artists)\n",
    "```\n",
    "\n",
    "```\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "#Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=artist_names)\n",
    "#Select row of 'Bruce Springsteen': artist\n",
    "artist = df.loc['Bruce Springsteen']\n",
    "#Compute cosine similarities: similarities\n",
    "similarities = df.dot(artist)\n",
    "#Display those with highest cosine similarity\n",
    "print(similarities.nlargest())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
