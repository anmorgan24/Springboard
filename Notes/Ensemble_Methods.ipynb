{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9f4ee6",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436cb9f",
   "metadata": {},
   "source": [
    "* Pre-reqs:\n",
    "    * Linear classifiers in Python\n",
    "    * Machine Learning with Tree-Based Models in Python\n",
    "    * Supervised Learning with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323336a4",
   "metadata": {},
   "source": [
    "### CAPSTONE TO DO: choose an evaluation metric for ensemble model: accuracy, sensitivity, specificty, other?\n",
    "* **Accuracy:** shows the percentage of the dataset instances correctly predicted by the model developed by the machine learning algorithm\n",
    "* **Sensitivity:** shows the percentage of COVID-19 positive patients correctly by the models\n",
    "* **Specificity:**  shows the percentage of COVID-19 negative patients correctly by the models\n",
    "\n",
    "* Create cross-val table of: CCI, TP, FP, Precision, Recall, F Measure/F1 Score, ROC/AUC?\n",
    "* **F1 Score for imbalanced data classes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5de746",
   "metadata": {},
   "source": [
    "### Combining multiple models\n",
    "* When you're building a model, you want to choose the one that performs the best according to some evaluation metric\n",
    "* Ensemble methods: form a new model by combining existing ones; the combined responses of different models will likely lead to a better decision than relying on a single response.\n",
    "    * the combined model will have better performance than any of the individual models (or at least be as good as the best individual model)\n",
    "* Ensemble learning is one of the most effective techniques in machine learning\n",
    "* A useful Python library for machine learning called `mlxtend`\n",
    "* Main scikit-learn module: `sklearn.ensemble`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f3846",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.ensemble import MetaEstimator\n",
    "# Base estimators\n",
    "est1 = Model1()\n",
    "est2 = Model1()\n",
    "estN = ModelN()\n",
    "# Meta estimator\n",
    "est_combined = MetaEstimator(estimators=[est1, est2,  ..., estN], \n",
    "                # Additional parameters (specific to the ensemble method)\n",
    ")\n",
    "# Train and test\n",
    "est_combined.fit(X_train, y_train)\n",
    "pred = est_combined.predict(X_test)\n",
    "```\n",
    "* The best feature of the meta estimator is that it works similarly to the scikit-learn estimators you already know, with the standard methods of fit and predict\n",
    "* Decision trees are the building block of many ensemble methods\n",
    "\n",
    "RECAP DT CODE:\n",
    "\n",
    "```\n",
    "# Split into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the regressor\n",
    "reg_dt = DecisionTreeRegressor(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "\n",
    "# Fit to the training set\n",
    "reg_dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = reg_dt.predict(X_test)\n",
    "print('MAE: {:.3f}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560a593",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa51ba2",
   "metadata": {},
   "source": [
    "* Concept of \"wisdom of the crowds\": refers to the collective intelligence based on a group of individuals instead of a single expert \n",
    "* Also known as **collective intelligence**\n",
    "* The aggregated opinion of the crowd can be as good as (and is usually superior to) the answer of any individual, even that of an expert.\n",
    "* Useful technique commonly applied to problem solving, decision making, innovation, and prediction (we are particularly interested in prediction)\n",
    "\n",
    "* **Majority voting:** a technique that combines the output of many classifiers using a maority voting approach\n",
    "    * Properties:\n",
    "        * Classification problems\n",
    "        * Majority voting $\\Rightarrow$ Mode of individual predictions\n",
    "        * **It is recommended to use an odd number of classifiers:** Use at least **three** classifiers, and when problem constraints allow it, use five or more\n",
    "        \n",
    "        \n",
    "* **Wise Crowd Characteristics:**\n",
    "    * **The ensemble needs to be diverse:** do this by using different algorithms or different datasets.\n",
    "    * **Independent and uncorrelated:** Each prediction needs to be independent and uncorrelated from the rest.\n",
    "    * **Use individual knowledge:** Each model should be able to make its own predition without relying on the other predictions\n",
    "    * **Aggregate individual predictions:** into a collective one\n",
    "    \n",
    "**NOTE:** Majority Voting can only be applied to classification problems\n",
    "    * For regression problems, see: ensemble voting regressor\n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf_voting = VotingClassifier(\n",
    "       estmators = [\n",
    "           ('label1', clf_1),\n",
    "           ('label2', clf_2),\n",
    "           ('labelN', clf_N)])\n",
    "         \n",
    "```\n",
    "\n",
    "```\n",
    "# Create the individual models\n",
    "clf_knn = KNeightborsClassifier(5) # k=5 nearest neighbors (to avoid multi-modal predictions)\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_lr = LogisticRegression()\n",
    "# Create voting classifier\n",
    "clf_voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', clf_knn),\n",
    "        ('dt', clf_dt),\n",
    "        ('lr', clf_lr)])\n",
    "# Fit combined model to the training set and predict\n",
    "clf_voting.fit(X_train, y_train)\n",
    "y_pred = clf_voting.predict(X_test)\n",
    "```\n",
    "#### Evaluate the performance\n",
    "\n",
    "```\n",
    "# Get the accuracy score\n",
    "acc = accuracy_score(X_test, y_pred)\n",
    "print(\"Accuracy: {:0.3f}\".format(acc))\n",
    "```\n",
    "\n",
    "\n",
    "* The main input- with keyword \"estimators\" - is a list of (string, estimator) tuples.\n",
    "* Each string is a label and each estimtor is an sklearn classifier\n",
    "* **You do not need to fit the classifiers individually, as the voting classifier will take care of that.**\n",
    "    * But you do need to tune hyperparameters? Not sure...\n",
    "    \n",
    "```\n",
    "# Make the invidual predictions\n",
    "pred_lr = clf_lr.predict(X_test)\n",
    "pred_dt = clf_dt.predict(X_test)\n",
    "pred_knn = clf_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of each model\n",
    "score_lr = f1_score(y_test, pred_lr)\n",
    "score_dt = f1_score(y_test, pred_dt)\n",
    "score_knn = f1_score(y_test, pred_knn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d835ae",
   "metadata": {},
   "source": [
    "* **1) Choose the best model**\n",
    "    * generate predictions as per each model\n",
    "    * get evaluation metric scores for each model's predictions (F1 score, accuracy, precision, recall, etc)\n",
    "    * Decide which model performs the best\n",
    "    \n",
    "* **2) Instantiate models**\n",
    "\n",
    "```\n",
    "# Instantiate the individual models\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_lr = LogisticRegression(class_weight='balanced')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "\n",
    "# Create and fit the voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[('knn', clf_knn), ('lr', clf_lr), ('dt', clf_dt)]\n",
    ")\n",
    "clf_vote.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "* **3) Evaluate your ensemble**\n",
    "\n",
    "```\n",
    "# Calculate the predictions using the voting classifier\n",
    "pred_vote = clf_vote.predict(X_test)\n",
    "\n",
    "# Calculate the F1-Score of the voting classifier\n",
    "score_vote = f1_score(y_test, pred_vote)\n",
    "print('F1-Score: {:.3f}'.format(score_vote))\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_test, pred_vote)\n",
    "print(report)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f00ff",
   "metadata": {},
   "source": [
    "### Averaging aka \"Soft Voting\"\n",
    "* Averaging is another popular ensemble method\n",
    "* Averaging is also referred to as **soft voting**\n",
    "* Averaging/ Soft Voting can be applied to both classification and regression.\n",
    "* In this technique, the combined prediction is the mean of the individual predictions\n",
    "* **Soft voting: Mean**\n",
    "    * **Regression:** mean of predicted values\n",
    "    * **Classification:** mean of predicted *probabilities*\n",
    "* As the mean doesn't have ambiguous cases (like the mode), we can use any number of estimators (odd or even), as long as we have at least two.\n",
    "\n",
    "* To build an averaging classifier, we'll use the same class as before: **`VotingClassifier()`**\n",
    "    * **Main difference:** we specify an additional parameter: **`voting='soft`**\n",
    "        * Default value of voting is `'hard'`\n",
    "#### Averaging Classifier    \n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf_voting = VotingClassifier(\n",
    "                estimators=[\n",
    "                    ('label1', clf_1),\n",
    "                    ('label2', clf_2),\n",
    "                    ...\n",
    "                    ('labelN', clf_N)],\n",
    "                voting = 'soft', \n",
    "                weights=[w_1, w_2, ..., w_N]\n",
    ")\n",
    "```\n",
    "* parameter `weights` is optional; specifies a weight for each of the estimators\n",
    "    * If specified, the combined prediction is a weighted average of the individual ones\n",
    "    * Otherwise, weights are considered uniform.\n",
    "* To build an **`AveragingRegressor()`:**\n",
    "#### Averaging Regressor:\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "reg_voting = VotingRegressor(\n",
    "        estimators = [\n",
    "            ('label1', reg_1),\n",
    "            ('label2', reg_2),\n",
    "            ...\n",
    "            ('labelN', reg_N)],\n",
    "        voting = 'soft',\n",
    "        weights = [w_1, w_2, ..., w_N]\n",
    ")\n",
    "```\n",
    "* The first parameter is also a list of the string/estimator tuples, but instead of classifiers, we use regressors\n",
    "\n",
    "\n",
    "***\n",
    "* **Averaging classifier example:**\n",
    "```\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "clf_knn = KNeighborsClassifier(5)\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_lr = LogisticRegression()\n",
    "clf_voting = VotingClassifier(\n",
    "        estimators = [\n",
    "            ('knn', clf_knn),\n",
    "            ('dt', clf_dt),\n",
    "            ('lr', clf_lr)],\n",
    "        voting='soft',\n",
    "        weights = [1, 2, 1]\n",
    ")\n",
    "```\n",
    "* Assuming we know that Decision Tree has better individual performance, we give it a higher weight\n",
    "* Ideally, the weights should be tuned while training the model, for example, using grid search cross-validation (`GridSearchCV`).\n",
    "\n",
    "```\n",
    "# Build the individual models\n",
    "clf_lr = LogisticRegression(class_weight='balanced')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "clf_svm = SVC(probability=True, class_weight='balanced', random_state=500)\n",
    "\n",
    "# List of (string, estimator) tuples\n",
    "estimators = [('lr', clf_lr), ('dt', clf_dt), ('svm', clf_svm)]\n",
    "\n",
    "# Build and fit an averaging classifier\n",
    "clf_avg = VotingClassifier(estimators = estimators,voting='soft')\n",
    "clf_avg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "acc_avg = accuracy_score(y_test,  clf_avg.predict(X_test))\n",
    "print('Accuracy: {:.2f}'.format(acc_avg))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76f185",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "#### The strength of weak models\n",
    "* What is a weak model and how to identify one by its properties?\n",
    "* Voting and averaging work by combining the predictions of already trained models\n",
    "    * Small number of estimators\n",
    "    * **Fine-tuned estimators**\n",
    "    * Individually optimized and trained for the problem\n",
    "    \n",
    "#### Weak vs. fine-tuned model\n",
    "\n",
    "#### Weak estimator\n",
    "* A \"weak\" model does not mean that it is a \"bad\" model, just that it is not as strong as a highly-optimized, finely-tuned model.\n",
    "* Performance is slightly better than random guessing\n",
    "* The error rate is less than 50%, but close to it\n",
    "* A weak model should be light in terms of space and computational requirements, and fast during training and modeling.\n",
    "* One good example: a decision tree limited to a depth of two\n",
    "* **The three desired properties:\n",
    "    * Low performance (just above guessing)\n",
    "    * It is light\n",
    "    * Low training and evaluation time\n",
    "* Common examples of weak models:\n",
    "    * decision tree with small depth\n",
    "    * Logistic Regression (makes the assumption that the classes are linearly separable)\n",
    "        * could also limit number of iterations for training\n",
    "        * or, specify a high value of the parameter C to use a weak regularization\n",
    "    * Linear Regression \n",
    "        * Makes the assumption that the output is a linear function of the input features\n",
    "        * could limit the number of iterations or not use normalization\n",
    "    * Other restricted models\n",
    "\n",
    "#### Bagging = Bootstrap Aggregating\n",
    "* Heterogenous vs homogenous ensemble methods\n",
    "#### Heterogenous:\n",
    "* Different algorithms (fine-tuned)\n",
    "* Work well with a small number of estimators\n",
    "* For example, we could combine a decision tree, a logistic regression, and a support vector machine using voting to improve the results\n",
    "* Included: Voting, Averaging, Stacking\n",
    "#### Homogenous\n",
    "* methods such as bagging\n",
    "* work by applying the same algorithm on all the estimators, and this algorithm must be a \"weak\" model\n",
    "* Large number of weak estimators\n",
    "* Bagging and Boosting are some of the most popular of this kind\n",
    "\n",
    "#### Condorcet's Jury Theorem:\n",
    "* **Requirements:**\n",
    "* All models must be independent\n",
    "* Each model performs better than random guessing\n",
    "* All individual models have similar performance\n",
    "\n",
    "* If these three conditions are met, then adding more models increases the probability of the ensemble to be correct, and makes this probability tend to 1 (1 equivalent to 100%)\n",
    "* The second and third requirements can be fulfilled by using the same weak model for all the estimators\n",
    "\n",
    "* To guarantee the first requirement of the theorem, the **bagging algorithm** trains individual models using a random subsample for each (known as **bootstrapping**)\n",
    "\n",
    "* A wise crowd needs to be diverse, either through using different datasets or algorithms\n",
    "\n",
    "* **Bootstrapping** requires:\n",
    "    * Random subsamples $\\Rightarrow$ provides **diversity** of data\n",
    "    * Using replacement\n",
    "    \n",
    "* Bagging helps reduce variance, as the sampling is truly random\n",
    "* Bias can be reduced since we use voting or averaging to combine the models \n",
    "* Bagging provides stablity and robustness\n",
    "* However, **bagging is computationally expensive in terms of space and time**\n",
    "\n",
    "* To take a sample, you'll use pandas' `.sample()` method, which has a replace parameter. For example, the following line of code samples with replacement from the whole DataFrame df:\n",
    "    * `df.sample(frac=1.0, replace=True, random_state=42)`\n",
    "    \n",
    "```\n",
    "# Take a sample with replacement\n",
    "X_train_sample = X_train.sample(frac=1.0, replace=True, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "# Build a \"weak\" Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=500)\n",
    "\n",
    "# Fit the model to the training sample\n",
    "clf.fit(X_train_sample, y_train_sample)\n",
    "```\n",
    "\n",
    "```\n",
    "def build_decision_tree(X_train, y_train, random_state=None):\n",
    "    # Takes a sample with replacement,\n",
    "    # builds a \"weak\" decision tree,\n",
    "    # and fits it to the train set\n",
    "\n",
    "def predict_voting(classifiers, X_test):\n",
    "    # Makes the individual predictions \n",
    "    # and then combines them using \"Voting\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba1259",
   "metadata": {},
   "source": [
    "#### BaggingClassifier\n",
    "* **Heterogenous Ensemble Functions**\n",
    "* A key difference between the ensemble functions from heterogenous and homogenous methods:\n",
    "\n",
    "```\n",
    "het_est = HeterogenousEnsemble(\n",
    "    estimators= [('est1', est1), ('est2', est2), ...], \n",
    "    # additional parameters\n",
    ")\n",
    "```\n",
    "\n",
    "* **Homogenous Ensemble Functions:**\n",
    "* To build a homogenous ensemble model, instead of a list of estimators, we pass the parameter base_estimator, which is the instantiated \"weak\" model we have chosen for our ensemble:\n",
    "\n",
    "```\n",
    "hom_est = HomogenousEnsemble(\n",
    "            base_estimator= est_base,\n",
    "            n_estimators= chosen_number,\n",
    "            #additional paramters\n",
    ")\n",
    "```\n",
    "\n",
    "#### BaggingClassifier example:\n",
    "\n",
    "```\n",
    "clf_dt = DecisionTreeClassifier(max_depth=3)\n",
    "clf_bag = BaggingClassifer(\n",
    "            base_estimator= clf_dt,\n",
    "            n_estimators=5\n",
    ")\n",
    "clf_bag.fit(X_train, y_train)\n",
    "y_pred = clf_bag.predict(X_test)\n",
    "```\n",
    "\n",
    "#### BaggingRegressor example\n",
    "\n",
    "```\n",
    "reg_lr = LinearRegression(normalization=False)\n",
    "reg_bag = BaggingRegressor(\n",
    "            base_estimator=reg_lr,\n",
    "            oob_score = True # oob_score = out-of-bag score\n",
    ")\n",
    "reg_bag.fit(X_train, y_train)\n",
    "y_pred = reg_bag.predict(X_test)\n",
    "```\n",
    "* Note: default number of estimators is ten, when left undefined as above\n",
    "\n",
    "#### Out-of-bag score\n",
    "* In a bagging ensemble, each estimator is trained on a bootstrap sample. Therefore, each of the samples will leave out some of the instances, which are then used to evaluate each estimator, similar to a train-test split\n",
    "* To get the out-of-bag score for each instance, we calculate the predictions using all the estimators for which it was out of the sample\n",
    "* Then, combine individual predictions\n",
    "* Evaluate the metric on those predictions\n",
    "    * For **classification** the default metric is accuracy\n",
    "    * For **regression** the default metric is R2 (aka **coefficient of determination**)\n",
    "* **Out of bag score helps avoid the need for an independent test set**\n",
    "    * However, it's often lower than the actual performance\n",
    "* To get the out-of-bag score from a Bagging ensemble, we need to set the parameter `oob_score` to True\n",
    "* After training the model, we can access the oob score using the attribute `.oob_score_`\n",
    "* It's good to compare oob score to actual metric (for example, R2 or accuracy)\n",
    "    * The two values being close is a good indicator of the model's ability to generalize to new data\n",
    "    \n",
    "```\n",
    "# Instantiate the base model\n",
    "clf_dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Build and train the Bagging classifier\n",
    "clf_bag = BaggingClassifier(\n",
    "  base_estimator=clf_dt,\n",
    "  n_estimators=21,\n",
    "  random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "pred = clf_bag.predict(X_test)\n",
    "\n",
    "# Show the F1-score\n",
    "print('F1-Score: {:.3f}'.format(f1_score(y_test, pred)))\n",
    "```\n",
    "\n",
    "```\n",
    "# Build and train the bagging classifier\n",
    "clf_bag = BaggingClassifier(\n",
    "  base_estimator=clf_dt,\n",
    "  n_estimators=21,\n",
    "  oob_score=True,\n",
    "  random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Print the out-of-bag score\n",
    "print('OOB-Score: {:.3f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Evaluate the performance on the test set to compare\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, pred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3f375",
   "metadata": {},
   "source": [
    "#### Bagging parameters: tips and tricks\n",
    "* **Basic parameters for bagging:**\n",
    "    * **`base_estimator`**: the \"weak\" model which will be built for each sample\n",
    "    * **`n_estimtors`**: specifies the number of estimators to use; 10 by default; in practice you'll want to use more than 10 (the larger the better-- usually between 100 and 500 trees are enough)\n",
    "    * **`oob_score`**: T/F \n",
    "* **Additional parameters for bagging:**\n",
    "    * **`max_samples`**: the number of samples to draw for each estimator; default is 1.0, the equivalent of 100%\n",
    "    * **`max_features`**: the number of features to draw (randomly) for each estimator; default is 1.0, the equivalent of 100%\n",
    "        * Using lower values of both of the above provides more diversity for the individual models and reduces the correlation among them, as each will get a different sample of both features and instances.\n",
    "        * For classification, the optimal value lies around the square root of the number of features\n",
    "        * For regression, the optimal value is usually close to one third of the number of features\n",
    "    * **`bootstrap`**: boolean; indicates whether samples are drawn with replacement; default is `True`\n",
    "        * If passed as True, it is recommended to use max_samples of 100%\n",
    "        * If False, then max_samples should be lower than 100%, because otherwise all the samples would be identical\n",
    "        \n",
    "## Random Forest Bagging\n",
    "* Random Forests are a special case of bagging where the base estimators are decision trees\n",
    "* If you want to use decision trees as base estimators, it is recommended to use the Random Forest classes instead, as they are specifically designed for trees.\n",
    "* The **sklearn implementation for RFs combines the models using averaging instead of voting, so there is no need to use an odd number of estimators**\n",
    "* For classification: **`RandomForestClassifier`**\n",
    "* For regression: **`RandomForestRegressor`**\n",
    "* **Some of the most important parameters:**\n",
    "    * **Parameters shared with bagging:**\n",
    "        * **`n_estimators`**\n",
    "        * **`max_features`**\n",
    "        * **`oob_score`**\n",
    "    * **Tree-specific parameters:**\n",
    "        * **`max_depth`**\n",
    "        * **`min_samples_split`** (min samples required to split a node)\n",
    "        * **`min_samples_leaf`** (min samples required in a leaf)\n",
    "        * **`class_weight`** (`\"balanced\"`): allows you to specify the weights for each class using a dictionary: balanced will use the class distribution to calculate balanced weights $\\Rightarrow$ therefore RFs are able to deal with imbalanced targets\n",
    "        \n",
    "        \n",
    "#### Recap: Bias-variance trade-off\n",
    "* A simple model has low variance, but high bias\n",
    "* Adding more complexity to the model may reduce the bias but increase the variance of predictions\n",
    "* This is why it's important to optimize the parameters of the ensemble models that minimize the total error and find the balance between bias and variance\n",
    "    \n",
    "```\n",
    "# Build a balanced logistic regression\n",
    "clf_lr = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "# Build and fit a bagging classifier\n",
    "clf_bag = BaggingClassifier(clf_lr, max_features=10, oob_score=True, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set and show the out-of-bag score\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, pred)))\n",
    "print('OOB-Score: {:.2f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "```\n",
    "\n",
    "```\n",
    "# Build a balanced logistic regression\n",
    "clf_base = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Build and fit a bagging classifier with custom parameters\n",
    "clf_bag = BaggingClassifier(base_estimator=clf_base, n_estimators=500, max_samples=0.65, max_features=10, bootstrap=False, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Calculate predictions and evaluate the accuracy on the test set\n",
    "y_pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e163a5",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a53a0c",
   "metadata": {},
   "source": [
    "* **Boosting** is class of ensemble learning algorithms based on a technique known as **gradual learning**\n",
    "* Collective learning vs gradual learning:\n",
    "    * **Collective learning:**\n",
    "        * the \"wisdom of the crowd\" principle\n",
    "        * idea that the combined prediction of individual models is superior to any of the individual predictions on their own.\n",
    "        * For collective learning to be efficient, the estimators need to be independent and uncorrelated\n",
    "        * All the estimators are learning the same task for the same goal\n",
    "        * Because the estimators are independent, they can be trained in parallel to speed up the model building\n",
    "    * **Gradual learning:**\n",
    "        * Based on the principle of iterative learning\n",
    "        * In this approach, each subsequent model tries to fix the errors of the previous model \n",
    "        * Gradual learning creates dependent estimators, as each model takes advantage of the knowledge from the previous estimator\n",
    "        * Each model is learning a different task, but each one contributes to the same goal of accurately predicting the target variable\n",
    "        * As gradual learning follows a sequential model building process, models cannot be trained in parallel\n",
    "        * Intuitively, gradual learning is similar to the way we, as humans, learn\n",
    "        * In gradual learning, instead of the same model being corrected in every iteration, a new model is built that tries to fix the errors of the previous model\n",
    "        * **Careful of fitting to noise!**\n",
    "        * You want to avoid having an estimator that is fitting to noise, which will lead to overfitting\n",
    "            * One way to control this is to stop training after the errors of an estimator start to display white noise\n",
    "\n",
    "* **White noise:**\n",
    "    * Errors are uncorrelated with the input features \n",
    "    * Errors are unbiased and have constant variance\n",
    "    \n",
    "* **Another approach to control fitting to white noise is Improvement Tolerance:**\n",
    "    * **Improvement tolerance:**\n",
    "        * If the difference in performance does not meet a defined threshold, then the training is stopped\n",
    "        * **?** Cross search for threshold defintion **?**\n",
    "        \n",
    "* You'll build another linear regression, but this time the target values are the errors from the base model, calculated as follows:\n",
    "\n",
    "* `y_train_error = pred_train - y_train`\n",
    "* `y_test_error = pred_test - y_test`\n",
    "\n",
    "```\n",
    "# Fit a linear regression model to the previous errors\n",
    "reg_error = LinearRegression(normalize=True)\n",
    "reg_error.fit(X_train_pop, y_train_error)\n",
    "\n",
    "# Calculate the predicted errors on the test set\n",
    "pred_error = reg_error.predict(X_test_pop)\n",
    "\n",
    "# Evaluate the updated performance\n",
    "rmse_error = np.sqrt(mean_squared_error(y_test_error, pred_error))\n",
    "print('RMSE: {:.3f}'.format(rmse_error))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c186d",
   "metadata": {},
   "source": [
    "### AdaBoost: Adaptive Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9fae4",
   "metadata": {},
   "source": [
    "* Award winning model with a high potential to solve complex problems \n",
    "* The first practical boosting algorithm\n",
    "* Proposed in 1997, it remains highly used and well-known among machine learning practitioners\n",
    "* There are two distinctive properties of Adaptive Boosting compared to other boosting algorithms\n",
    "\n",
    "#### AdaBoost Properties\n",
    "* 1. Instances are drawn using a sample distribution of the training data into each subsequent dataset\n",
    "    * This sample distribution makes sure that instances which were harder to predict for the previous estimator have a higher chance to be included in the training set for the next estimator by giving them higher weights\n",
    "    * Distribution is initialized to be uniform\n",
    "* 2. The estimators are combined through weighted majority voting\n",
    "    * The voting weights are based on the estimators' training error\n",
    "    * Estimators which have shown good performance are rewarded with higher weights for voting\n",
    "    * **Good estimators are given higher weights.**\n",
    "* 3. Guaranteed to improve as the estimator grows\n",
    "    * **AdaBoost is guaranteed to improve as the ensemble grows if each estimator has an error rate less than 0.5.\n",
    "    \n",
    "* **Each estimator needs to be a \"weak\" \n",
    "* **Similar to bagging, AdaBoost can be used for both Classification and Regression with its two variations\n",
    "\n",
    "```\n",
    "from sklearn.ensembe import AdaBoostClassifier\n",
    "clf_ada = AdaBoostClassifier\n",
    "clf_ada = AdaBoostClassifier(base_estimator, \n",
    "                                n_estimators,\n",
    "                                learning_rate\n",
    ")\n",
    "```\n",
    "\n",
    "* **Parameters:**\n",
    "* `base_estimator`: the weak model template for all the estimators; default is a DecisionTreeClassifier with a max depth of 1, also known as a decision stump\n",
    "* `n_estimators`: number of estimators to use; default is 50\n",
    "    * If there's a perfect fit, or an estimator with error higher than 50%, no more estimators are built\n",
    "* `learning_rate`: which represents how much each estimator contributes to the ensemble; 1.0 by default\n",
    "    * There is a trade between `n_estimators` and `learning_rate`\n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "reg_ada = AdaBoostRegressor(\n",
    "            base_estimator,\n",
    "            n_estimators,\n",
    "            learning_rate,\n",
    "            loss\n",
    ")\n",
    "```\n",
    "* There is a difference with the parameter `base_estimator` between classification and regression versions of AdaBoost. If it's not specified, the default will be a Decision Tree Regressor with a max_depth of **3** (as compared with the AdaBoost Classifier with had as default a Decision Tree Classifier with max_depth of **1** by default.\n",
    "* `loss` parameter is the function used to update weights. By default it is linear, but you can also use the square of exponential loss\n",
    "\n",
    "```\n",
    "# Instantiate a normalized linear regression model\n",
    "reg_lm = LinearRegression(normalize=True)\n",
    "\n",
    "# Build and fit an AdaBoost regressor\n",
    "reg_ada = AdaBoostRegressor(base_estimator=reg_lm, n_estimators=12, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE: {:.3f}'.format(rmse))\n",
    "```\n",
    "\n",
    "```\n",
    "# Build and fit a tree-based AdaBoost regressor\n",
    "reg_ada = AdaBoostRegressor(n_estimators=12, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE: {:.3f}'.format(rmse))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9a131",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0d3e3",
   "metadata": {},
   "source": [
    "* 1. Initial model (weak estimator fit to the dataset)\n",
    "* 2. On each subsequent iteration, a new model is built and fitted to the residual error from the previous iteration.\n",
    "* 3. After each individual estimator is built, the resuly is a new **additive** model, which is an improvement on the previous estimate\n",
    "* 4. Repeat this process n times or until the error is small enough such that the difference in performance in negligible\n",
    "* 5. After the algorithm is finished, the result is a final improved additive model.\n",
    "        * This is a peculiarity of Gradient Boosting, as the individual estimators are not combined through voting or average, but by addition\n",
    "        * This is because only the first model is fitted to the target variable, and the rest are estimates of the residual errors \n",
    "        \n",
    "* Why \"Gradient Boosting?\"\n",
    "    * Because it's equivalent to applying **gradient descent** as the optimization algorithm.\n",
    "    * The residuals = negative gradient\n",
    "    \n",
    "* The residuals are defined as $F_i$(x)\n",
    "    * This ($F_i$(x)) represents the error that the model has at iteration *i*\n",
    "    \n",
    "* **Gradient Descent** is an iterative optimization algorithm that attempts to minimize the loss of an estimator\n",
    "* On every iteration steps are taken in the direction of the negative gradient, which points toward the minimum \n",
    "* The gradient is the derivative of the loss with respect to the approximate function\n",
    "* The result is $F_i$(x) - *y*\n",
    "* We are actually improving the model using Gradient Descent on each iteration \n",
    "* **Gradient Boosting Classifier:**\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_gbm = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            min_samples_split,\n",
    "            max_samples_leaf,\n",
    "            max_features\n",
    ")\n",
    "```\n",
    "\n",
    "* Unlike with other ensemble methods, here we don't specify the `base_estimator` as Gradient Boosting is implemented with regression trees as the individual estimators.\n",
    "* In classification, the trees are fitted to the class probabilities\n",
    "* `n_estimators`= 100 by default \n",
    "* `learning_rate`= 0.1 by default\n",
    "* `max_depth` = 3 by default\n",
    "* **In gradient boosting, it is recommended to use all the features.**\n",
    "* **Gradient Boosting Regressor:**\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg_gbm = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth= 3\n",
    "            min_samples_split,\n",
    "            min_samples_leaf,\n",
    "            max_features\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "# Build and fit a Gradient Boosting classifier\n",
    "clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n",
    "clf_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = clf_gbm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:.3f}'.format(acc))\n",
    "\n",
    "# Get and show the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "```                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de136f",
   "metadata": {},
   "source": [
    "### Gradient Boosting \"flavors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfb646",
   "metadata": {},
   "source": [
    "* Some variations or \"flavors\" in the gradient boosting family of algorithms (along with their implementations in Python)\n",
    "\n",
    "#### XGBoost\n",
    "* **Extreme Gradient Boosting**; implemented with **XGBoost**\n",
    "* a more advance implementation of the Gradient Boosting algorithm\n",
    "* optimized for distributed computing for both training and prediction phases\n",
    "* uses parallel processing for training each estimator, thus speeding up the processing\n",
    "* scalable, portable, accurate\n",
    "* can work with huge datasets\n",
    "\n",
    "```\n",
    "import xgboost as xgb\n",
    "clf_xgb = xgb.XGBClassifier(\n",
    "            n_estimtors= 100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth= 3,\n",
    "            random_state\n",
    ")\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "pred = clf_xgb.predict(X_test)\n",
    "```\n",
    "\n",
    "\n",
    "#### LightGBM\n",
    "* **Light Gradient Boosting Machine**; implemented with **LightGBM**\n",
    "* framework developed by Microsoft in 2017\n",
    "* Provides faster training and higher efficiency\n",
    "* Lighter in terms of space and memory usage\n",
    "* Being a distributed algorithm means it's optimized for parallel and GPU processing\n",
    "* Useful for problems involving big datasets and/or constraints of speed or memory\n",
    "* Note that for LGBM max_depth is **-1** by default (meaning **no limit**)\n",
    "\n",
    "```\n",
    "import lightgbm as lgb\n",
    "clf_lgb = lgb.LGBMClassifier(\n",
    "            n_estimtors=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth= -1,\n",
    "            random_state\n",
    ")\n",
    "clf_lgb.fit(X_train, y_train)\n",
    "pred = clf_lgb.predict(X_test)           \n",
    "```\n",
    "\n",
    "```\n",
    "# Build and fit an XGBoost regressor\n",
    "reg_xgb = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, objective='reg:squarederror', random_state=500)\n",
    "reg_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Build and fit a LightGBM regressor\n",
    "reg_lgb = lgb.LGBMRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, objective='mean_squared_error', seed=500)\n",
    "reg_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions and evaluate both regressors\n",
    "pred_xgb = reg_xgb.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))\n",
    "pred_lgb = reg_lgb.predict(X_test)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test, pred_lgb))\n",
    "\n",
    "print('Extreme: {:.3f}, Light: {:.3f}'.format(rmse_xgb, rmse_lgb))\n",
    "```\n",
    "\n",
    "#### CatBoost\n",
    "* **Categorical Boosting**; implemented with **CatBoost** (the newest \"flavor\")\n",
    "* alias = `cb`\n",
    "* `cb` gives us access to CatBoostClassifier **and** CatBoostRegressor\n",
    "* the most recent Gradient Boosting \"flavor\"\n",
    "* open sourced by **Yandex**, a Russian tech company, in April 2017\n",
    "* CatBoost has built-in capacity to handle categorical features, so you don't need to do the preprocessing yourself\n",
    "* It is a fast implementation which can scale to large datasets and run on a GPU if required\n",
    "* Accurate and robust\n",
    "* Fast and scalable\n",
    "* User-friendly interface/API that integrates well with scikit-learn\n",
    "* Similar set of parameters to other scikit-learn machine learning class, but the default values are somewhat different (see below)\n",
    "\n",
    "```\n",
    "import catboost as cb\n",
    "clf_cat = cb.CatBoostClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=6,\n",
    "            random_state\n",
    ")\n",
    "clf_cat.fit(X_train, y_train)\n",
    "pred = clf_cat.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5252e2",
   "metadata": {},
   "source": [
    "#### Movie revenue prediction with CatBoost\n",
    "\n",
    "```\n",
    "# Build and fit a CatBoost regressor\n",
    "reg_cat = cb.CatBoostRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=500)\n",
    "reg_cat.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the set set\n",
    "pred = reg_cat.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE (CatBoost): {:.3f}'.format(rmse_cat))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb7162",
   "metadata": {},
   "source": [
    "# Cat Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292eecb",
   "metadata": {},
   "source": [
    "* CatBoost is an open-sourced gradient boosting library.\n",
    "* CatBoost deals with the categorical data quite well out-of-the-box. However, it also has a huge number of training parameters, which provide fine control over the categorical features preprocessing. \n",
    "* According to [CatBoost documentation](https://catboost.ai/en/docs/features/categorical-features): **don't use one hot encoding on preprocessing of data fed to CatBoost** \"This affects both training speed and resulting quality.\"\n",
    "* Alternately, according to tds: \"CatBoost supports some traditional methods of categorical data preprocessing, such as One-hot Encoding and Frequency Encoding.\"\n",
    "* The core idea behind CatBoost categorical features preprocessing is Ordered Target Encoding: a random permutation of the dataset is performed and then target encoding of some type (for example just computing mean of the target for objects of this category) is performed on each example using only the objects that are placed before the current object.\n",
    "\n",
    "* Generally transforming categorical features to numerical features in CatBoost includes the following steps:\n",
    "    * 1. **Permutation** of the training objects in random order.\n",
    "    * 2. **Quantization** i.e. converting the target value from a floating-point to an integer depending on the task type:\n",
    "        * **Classification** — Possible values for target value are “0” (doesn’t belong to the specified target class) and “1” (belongs to the specified target class).\n",
    "        * **Multiclassification** — The target values are integer identifiers of target classes (starting from “0”).\n",
    "        * **Regression** — Quantization is performed on the label value. The mode and number of buckets are set in the starting parameters. All values located inside a single bucket are assigned a label value class — an integer in the range defined by the formula: <bucket ID — 1>.\n",
    "    * 3. **Encoding** the categorical feature values.\n",
    "    \n",
    "    \n",
    "* CatBoost creates four permutations of the training objects and for each permutation, a separate model is trained. Three models are used for the tree structure selection and the fourth is used to compute the leaves values of the final model that we save. At each iteration one of the three models is chosen randomly; this model is used to choose the new tree structure and to calculate the leaves values for all the four models.\n",
    "\n",
    "* **Another important point is that CatBoost can create new categorical features combining the existing ones. And it will actually do so unless you explicitly tell it not to**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce39df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93d0ede5",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace5b0a",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "* **change MSE evaluation metric to RMSE (or MAE) in capstone?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8738f47",
   "metadata": {},
   "source": [
    "* Common regression metrics: \n",
    "    * RMSE\n",
    "        * allows us to treat negative and positive differences equally \n",
    "        * but, it tends to punish larger differences between predicted and actual values much more than smaller ones \n",
    "    * MAE\n",
    "        * simply sums the absolute differences between predicted and actual values \n",
    "        * Although MAE isn't affected by large differences as much as RMSE, it lacks some nice mathematical properties that make it much less frequently used as an evaluation metric\n",
    "* Decision trees can be effectively applied to both classification and regression probems, an important property that makes them prime candidates to be the building blocks of XGBoost models \n",
    "\n",
    "* Objective (loss) functions and base learners\n",
    "    * Two critical concepts to understand in order to grasp why XGBoost is such a powerful approach to building supervised regression models\n",
    "    * The goal of any ML model is to find the model that yields the minimum value of the loss function\n",
    "    \n",
    "#### Common loss functions and XGBoost\n",
    "* Loss functions have specific naming conventions in XGBoost:\n",
    "    * **reg:linear** $\\Rightarrow$ use for regression problems \n",
    "    * **reg:logistic** $\\Rightarrow$ use for binary classification problems when you want just the decision, **not** the probability\n",
    "    * **binary:logistic** $\\Rightarrow$ use when you want probability rather than just the decision for binary classification problems. \n",
    "    \n",
    "#### Base learners\n",
    "* XGBoost is an ensemble learning method composed of many individual models that are added together to generte a single prediction\n",
    "* Each of the individual models that are trained and combined are called base learners\n",
    "* **The goal of XGBoost is to have base learners that are slightly better than random guessing on certain subsets of training examples and uniformly bad on the remainder** so tht when all of the predictions are combined, the uniformly bad predictions cancel out and those slightly better than chance combine into a single very good prediction.\n",
    "* Two kinds of base learners: tree and linear\n",
    "\n",
    "### Trees as base learners in XGBoost for a regression problem with sklearn's API:\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(\n",
    "                objective='reg:linear', \n",
    "                n_estimators=10, \n",
    "                seed=123\n",
    ")\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "```\n",
    "\n",
    "### Regularization and base learners in XGBoost\n",
    "\n",
    "* Loss functions in XGboost don't just take into account how close a model's predictions are to the actual values, but also take into account how complex the model is. \n",
    "* The idea of penalizing models as they become more complex is called **regularization**\n",
    "* So, loss functions in XGBoost are used to find models that are both accurate and as simple as they can possibly be\n",
    "* **Regularization parameters in XGBoost:**\n",
    "    * **gamma**: minimum loss reduction allowed for a split to occur; for tree-based learners \n",
    "        * higher values lead to fewer splits\n",
    "    * **alpha**: l1 regularization (penalty) on leaf weights (rather than on feature weights, as is the cse in linear or logistic regression)\n",
    "        * larger values mean more regularization\n",
    "        * higher alpha values lead to stronger L1 regularization, which causes many leaf weights in the base learners to go to 0.\n",
    "    * **lambda**: l2 regularization on leaf weights\n",
    "        * L2 regularization is a much smoother penalty than L1 and causes leaf weights to smoothly decrease, instead of enforcing strong sparsity constraints on the leaf weights as in L1\n",
    "* More about regularization in DC's Supervised Learning with Scikit Learn Course\n",
    "\n",
    "* **L1 regularization in XGBoost example:**\n",
    "* first: import necessary libraries and dataset, create feature matrix X and target vector y\n",
    "\n",
    "```\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params= {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "l1_params = [1,10,100]\n",
    "rmses_l1 = []\n",
    "for reg in l1_params:\n",
    "    params['aplha'] = reg\n",
    "    cv_results = xgb.cv(\n",
    "                    dtrain=boston_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=4,\n",
    "                    num_boost_round =10,\n",
    "                    metrics='rmse',\n",
    "                    as_pandas=True,\n",
    "                    seed=123\n",
    "    )\n",
    "    rmses_l1.append(cv_results['test-rmse-mean'].tail(1).values[0])\n",
    "print('Best rmse as a function of l1:')\n",
    "print(pd.DataFrame(list(zip(l1_params, rmse_l1)), columns=['l1', 'rmse']))\n",
    "```\n",
    "\n",
    "* **Tree base learner:**\n",
    "    * Decision tree\n",
    "    * Boosted model is weighted sum of decision trees (nonlinear)\n",
    "    * Almost exclusively used in XGBoost\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d6872",
   "metadata": {},
   "source": [
    "### Tunable parameters in XGBoost\n",
    "* The parameters that can be tuned are significantly different for each base learner\n",
    "* For the tree based learner, most frequentyly tuned parameters are:\n",
    "    * **learning rate:** affects how quickly the model fits the residual error using additional base learners \n",
    "        * A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an XGBoost with a high learning rate. \n",
    "    * **gamma:** min loss reduction to create new tree split\n",
    "    * **lambda:** L2 reg on leaf weights\n",
    "    * **alpha:** L1 reg on leaf weights\n",
    "    * **max_depth:** max depth per tree; must be a positive integer value\n",
    "    * **subsample:** % samples used per tree; must be a value between 0 and 1 \n",
    "        * if the value is low, then the fractino of your training data used per boosting round would be low (and you may run into underfitting problems\n",
    "        * a high value can lead to overfitting\n",
    "    * **colsample_bytree:** % features used per tree; must be a value between 0 and 1\n",
    "        * a large value means that almost all features can be used to build a tree during a given boosting round; large values may in certain cases overfit a trained model.\n",
    "        * a small value means that the fraction of features that can be selected from is very small; small values can be thought of as providing additional regularization to the model \n",
    "\n",
    "\n",
    "```\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=10, early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\n",
    "```\n",
    "\n",
    "```\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\n",
    "```\n",
    "\n",
    "```\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ae413",
   "metadata": {},
   "source": [
    "### Review of grid search and random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abd3ac",
   "metadata": {},
   "source": [
    "* How do we find the optimal values for several hyperparameters simultaneously, leading to the lowest loss possible, when their values interact in non-obvious, non-linear ways?\n",
    "* **GridSearch is a method of exhaustively searching through a collection of possible parameter values\n",
    "    * Number of models = number of distinct values per hyperparameter multiplied across hyperparameter\n",
    "    * In GridSearch, you try every parameter configuration, evaluate some metric for that configuration, and pick the parameter configuration that gave you the best value for the metric you were using.\n",
    "    \n",
    "```\n",
    "gbm_param_grid = {'learning_rate':[0.01, 0.1, 0.5, 0.9],\n",
    "                  'n_estimtors': [200],\n",
    "                  'subsample': [0.3, 0.5, 0.9]}\n",
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm, \n",
    "                        param_grid=gbm_param_grid, \n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv=4,\n",
    "                        verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "```\n",
    "\n",
    "#### RandomSearch: review\n",
    "* Create a (possibly infinite) range of hyperparameter values per hyperparameter that you would like to search over.\n",
    "* Set the number of iterations you would like for the random search to continue\n",
    "\n",
    "```\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5639d21",
   "metadata": {},
   "source": [
    "#### Limits of GridSearch and RandomSearch\n",
    "* Grid search: computationally expensive\n",
    "* Random search: parameter space to explore can be massive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d06db9",
   "metadata": {},
   "source": [
    "## Review of pipelines using sklearn\n",
    "* Pipelines in sklearn are objects that take a list of named tuples (name, pipline_step) as input\n",
    "* The named tuples must always contain a string name as the first element in each tuple\n",
    "* Tuples can contain any arbitrary scikit-learn compatible estimator or transformer object\n",
    "* Each named tuple in the pipeline is called a step and the list of transformations that are contained in it are executed in order once some data is passed through the pipeline.\n",
    "* Pipeline implements sklearn's standard fit/predict paradigm.\n",
    "* Pipelines can be used as input estimators into grid/randomized search and cross_val_score methods\n",
    "\n",
    "#### Pipeline example using Random Forest Regression\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = ...\n",
    "X, y = ...\n",
    "rf_pipeline = Pipeline[(\"st_scaler\", StandardScaler()), ('rf_model', RandomForestRegressor())]\n",
    "scores = cross_val_score(rf_pipeline, X, y, scoring = 'neg_mean_squared_error, cv=10)\n",
    "\n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "\n",
    "print(\"final RMSE: \", final_avg_rmse)\n",
    "```\n",
    "$\\Uparrow$ to get a root mean squared error across all 10 cross-validation folds\n",
    "\n",
    "* **neg_mean_squared_error**: sklearn's API-specific way of calculating the mean squared error (mse) in an API-comptible way; negative mean squared error don't eactually exist, as all squares must be positive when working with real numbers\n",
    "\n",
    "#### Dictvectorizer\n",
    "* DictVectorizer is a class found in scikit-learn's feature extraction submodule and is traditionally used in text processing pipelines by converting lists of feature mappings into vectors\n",
    "* Need to convert a DataFrame into a list of dictionary entries\n",
    "\n",
    "```\n",
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)\n",
    "```\n",
    "\n",
    "#### Incorporating XGBoost into pipelines\n",
    "* to get XGBoost to work within a pipeline, all that's really required is that you use XGBoost's scikit-learn API within a pipeline object\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = ...\n",
    "X, y = ...\n",
    " \n",
    "scores = cross_val_score(xgb_pipeline, X, y, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "print(\"Final XGB RMSE: \" final_avg_rmse)\n",
    "```\n",
    "\n",
    "### Additional components introduced for pipelines\n",
    "* `sklearn_pandas`: is a separate library that attempts to bridge the gap between working with pandas and working with scikit-learn, as they don't always work seamlessly together \n",
    "    * `DataFrameMapper`: Interoperability between `pandas` and `scikit-learn`\n",
    "    * `CategoricalImputer`: a class that allows us to impute missing categorical values directly \n",
    "* `sklearn.preprocessing`\n",
    "    * `Imputer`: Native imputation of numerical columns in scikit-learn\n",
    "* `sklearn.pipeline`:\n",
    "    * `FutureUnion`: combine multiple pipelines of features into a single pipeline of features \n",
    "        * as we would need to do, for example, if we had one set of preprocessing steps we needed to perform on categorical features of a dataset and a distinct set of preprocessing steps on the numeric features found in a dataset\n",
    "        \n",
    "```\n",
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n",
    "```\n",
    "\n",
    "```\n",
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )\n",
    "```\n",
    "\n",
    "```\n",
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])\n",
    "```\n",
    "\n",
    "```\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4f44a1",
   "metadata": {},
   "source": [
    "### Tuning XGBoost hyperparameters in a pipeline\n",
    "* **Note:** In order for the hyperparameters to be passed to the appropriate step, you have to name the parameters in the dictionary with the name of the step being referenced, followed by two underscore signs and then the name of the hyperparameter you want to iterate over\n",
    "* Since (below) the xgboost step is called xgb_model, all of our hyperparameter keys will start with xgboost_model__\n",
    "\n",
    "```\n",
    "data = ...\n",
    "x, y = ...\n",
    "xgb_pipeline = Pipeline[(\"st_scaler\", ...: StandardScaler()), ('xgb_model', xgb.XGBRegressor())]\n",
    "gbm_param_grid = {\n",
    "    ...:      'xgb_model__subsample': np.arange(.05, 1, .05),\n",
    "    ...:      'xgb_model__max_depth': np.arange(3, 20, 1),\n",
    "    ...:      'xgb_model__colsample_bytree': np.arange(0.1, 1.05, .05)}\n",
    "randomized_neg_mse = RandomizedSearchCV(estimator=xgb_pipeline, \n",
    "    ...:      param_distributions=gbm_param_grid, n_iter=10,\n",
    "    ...:      scoring='neg_mean_squared_error', cv=4)\n",
    "randomized_neg_mse.fit(X, y)\n",
    "\n",
    "print(\"Best rmse: \", np.sqrt(np.abs(randomized_neg_mse.best_score_)))\n",
    "\n",
    "print(\"Best model: \", randomized_neg_mse.best_estimtor_)\n",
    "```\n",
    "\n",
    "```\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
    "    'clf__max_depth': np.arange(3,10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
    "                                        param_distributions=gbm_param_grid,\n",
    "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c803b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db42bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22a4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc0846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f89f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
