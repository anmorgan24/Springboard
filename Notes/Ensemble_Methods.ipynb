{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c79885",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fbf9e",
   "metadata": {},
   "source": [
    "* Pre-reqs:\n",
    "    * Linear classifiers in Python\n",
    "    * Machine Learning with Tree-Based Models in Python\n",
    "    * Supervised Learning with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb04a2",
   "metadata": {},
   "source": [
    "### CAPSTONE TO DO: choose an evaluation metric for ensemble model: accuracy, sensitivity, specificty, other?\n",
    "* **Accuracy:** shows the percentage of the dataset instances correctly predicted by the model developed by the machine learning algorithm\n",
    "* **Sensitivity:** shows the percentage of COVID-19 positive patients correctly by the models\n",
    "* **Specificity:**  shows the percentage of COVID-19 negative patients correctly by the models\n",
    "\n",
    "* Create cross-val table of: CCI, TP, FP, Precision, Recall, F Measure/F1 Score, ROC/AUC?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd2501",
   "metadata": {},
   "source": [
    "### Combining multiple models\n",
    "* When you're building a model, you want to choose the one that performs the best according to some evaluation metric\n",
    "* Ensemble methods: form a new model by combining existing ones; the combined responses of different models will likely lead to a better decision than relying on a single response.\n",
    "    * the combined model will have better performance than any of the individual models (or at least be as good as the best individual model)\n",
    "* Ensemble learning is one of the most effective techniques in machine learning\n",
    "* A useful Python library for machine learning called `mlxtend`\n",
    "* Main scikit-learn module: `sklearn.ensemble`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b5557",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.ensemble import MetaEstimator\n",
    "# Base estimators\n",
    "est1 = Model1()\n",
    "est2 = Model1()\n",
    "estN = ModelN()\n",
    "# Meta estimator\n",
    "est_combined = MetaEstimator(estimators=[est1, est2,  ..., estN], \n",
    "                # Additional parameters (specific to the ensemble method)\n",
    ")\n",
    "# Train and test\n",
    "est_combined.fit(X_train, y_train)\n",
    "pred = est_combined.predict(X_test)\n",
    "```\n",
    "* The best feature of the meta estimator is that it works similarly to the scikit-learn estimators you already know, with the standard methods of fit and predict\n",
    "* Decision trees are the building block of many ensemble methods\n",
    "\n",
    "RECAP DT CODE:\n",
    "\n",
    "```\n",
    "# Split into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the regressor\n",
    "reg_dt = DecisionTreeRegressor(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "\n",
    "# Fit to the training set\n",
    "reg_dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = reg_dt.predict(X_test)\n",
    "print('MAE: {:.3f}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faaf037",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdc511",
   "metadata": {},
   "source": [
    "* Concept of \"wisdom of the crowds\": refers to the collective intelligence based on a group of individuals instead of a single expert \n",
    "* Also known as **collective intelligence**\n",
    "* The aggregated opinion of the crowd can be as good as (and is usually superior to) the answer of any individual, even that of an expert.\n",
    "* Useful technique commonly applied to problem solving, decision making, innovation, and prediction (we are particularly interested in prediction)\n",
    "\n",
    "* **Majority voting:** a technique that combines the output of many classifiers using a maority voting approach\n",
    "    * Properties:\n",
    "        * Classification problems\n",
    "        * Majority voting $\\Rightarrow$ Mode of individual predictions\n",
    "        * **It is recommended to use an odd number of classifiers:** Use at least **three** classifiers, and when problem constraints allow it, use five or more\n",
    "        \n",
    "        \n",
    "* **Wise Crowd Characteristics:**\n",
    "    * **The ensemble needs to be diverse:** do this by using different algorithms or different datasets.\n",
    "    * **Independent and uncorrelated:** Each prediction needs to be independent and uncorrelated from the rest.\n",
    "    * **Use individual knowledge:** Each model should be able to make its own predition without relying on the other predictions\n",
    "    * **Aggregate individual predictions:** into a collective one\n",
    "    \n",
    "**NOTE:** Majority Voting can only be applied to classification problems\n",
    "    * For regression problems, see: ensemble voting regressor\n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf_voting = VotingClassifier(\n",
    "       estmators = [\n",
    "           ('label1', clf_1),\n",
    "           ('label2', clf_2),\n",
    "           ('labelN', clf_N)])\n",
    "         \n",
    "```\n",
    "\n",
    "```\n",
    "# Create the individual models\n",
    "clf_knn = KNeightborsClassifier(5) # k=5 nearest neighbors (to avoid multi-modal predictions)\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_lr = LogisticRegression()\n",
    "# Create voting classifier\n",
    "clf_voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', clf_knn),\n",
    "        ('dt', clf_dt),\n",
    "        ('lr', clf_lr)])\n",
    "# Fit combined model to the training set and predict\n",
    "clf_voting.fit(X_train, y_train)\n",
    "y_pred = clf_voting.predict(X_test)\n",
    "```\n",
    "#### Evaluate the performance\n",
    "\n",
    "```\n",
    "# Get the accuracy score\n",
    "acc = accuracy_score(X_test, y_pred)\n",
    "print(\"Accuracy: {:0.3f}\".format(acc))\n",
    "```\n",
    "\n",
    "\n",
    "* The main input- with keyword \"estimators\" - is a list of (string, estimator) tuples.\n",
    "* Each string is a label and each estimtor is an sklearn classifier\n",
    "* **You do not need to fit the classifiers individually, as the voting classifier will take care of that.**\n",
    "    * But you do need to tune hyperparameters? Not sure..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2382e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dddbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745da5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cec38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59407683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7580b7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbf575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
