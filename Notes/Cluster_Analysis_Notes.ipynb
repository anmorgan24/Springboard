{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad674a5c",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e66cc",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning Basics\n",
    "* **Unsupervised learning:** a group of machine learning algorithms that find patterns in unlabeled data.\n",
    "* Data used in these algorithms has not been labeled, classified, or characterized in any way.\n",
    "* The objective of the algorithm is to interpret any inherent structure(s) in the data.\n",
    "* Common unsupervised learning algorithms: clustering, neural networks, anomaly detection\n",
    "\n",
    "#### Clustering\n",
    "* The process of grouping items with similar characteristics\n",
    "* The groups are formed as such that items in a single group are closer to eachother in terms of some characteristics as compared to items in other clusters\n",
    "* A **cluster** is a group of items with similar characteristics\n",
    "    * For example, Google News articles where similar words and word associations appear together\n",
    "    * Customer Segmentation\n",
    "* Clustering algorithms:\n",
    "    * Hierarchical clustering $\\Rightarrow$ Most common\n",
    "    * K means clustering $\\Rightarrow$ Most common\n",
    "    * Other clustering algorithms: DBSCAN (Density based), Gaussian Methods\n",
    "    \n",
    "```\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns, pandas as pd\n",
    "\n",
    "x_coordinates = [80.1, 93.1, 86.6, 98.5, 86.4, 9.5, 15.2, 3.4, 10.4, 20.3, 44.2, 56.8, 49.2, 62.5, 44.0]\n",
    "y_coordinates = [87.2, 96.1, 95.6, 92.4, 92.4, 57.7, 49.4, 47.3, 59.1, 55.5, 25.6, 2.1, 10.9, 24.1, 10.3]\n",
    "\n",
    "df = pd.DataFrame({'x_coordinate': x_coordinates, 'y_coordinate' : y_coordinates})\n",
    "\n",
    "Z = linkage(df, 'ward')\n",
    "df['cluster_labels'] = fcluster(Z, 3, criterion = 'maxclust')\n",
    "sns.scatterplot(x='x_coordinate', y='y_coordinate', hue = 'cluster_labels', data=df)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e9419",
   "metadata": {},
   "source": [
    "### K-means clustering in SciPy\n",
    "\n",
    "```\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns, pandas as pd\n",
    "\n",
    "import random\n",
    "random.seed((1000, 2000))\n",
    "\n",
    "x_coordinates = [80.1, 93.1, 86.6, 98.5, 86.4, 9.5, 15.2, 3.4, 10.4, 20.3, 44.2, 56.8, 49.2, 62.5, 44.0]\n",
    "y_coordinates = [87.2, 96.1, 95.6, 92.4, 92.4, 57.7, 49.4, 47.3, 59.1, 55.5, 25.6, 2.1, 10.9, 24.1, 10.3]\n",
    "\n",
    "df = pd.DataFrame({'x_coordinate': x_coordinates, 'y_coordinate' : y_coordinates})\n",
    "\n",
    "centroids,_ = kmeans(df, 3) # second argument is 'distortion' represented by dummy variable '_'\n",
    "df['cluster_labels'],_ = vq(df, centroids) # second argument is 'distortion' represented by dummy variable '_'\n",
    "\n",
    "sns.scatterplot(x='x_coordinate', y='y_coordinate', hue='cluster_labels', data=df)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c3c8e",
   "metadata": {},
   "source": [
    "#### Data preparation for cluster analysis\n",
    "Why prepare data for clustering?\n",
    "* Variables may have incomparable units (product dimensions in cm, price in dollars)\n",
    "* Even if variables have the same unit, they may be significantly different in terms of their scales and variances\n",
    "* Data in raw form may lead to bias in clustering\n",
    "* Clusters may be heavily dependent on one variable\n",
    "* **Solution:** normalization of variables\n",
    "\n",
    "* **Normalization:** process of rescaling data to a standard deviation of 1: `x_new = x / std(x)`\n",
    "    * normalization library: `from scipy.cluster.vq import whiten`\n",
    "    * `scaled_data = whiten(data)`\n",
    "    * output is an array of the same dimensions as original `data`\n",
    "**Illustration of the normalization of data:**\n",
    "\n",
    "```\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(data, label = \"original\")\n",
    "plt.plot(scaled_data, label = \"scaled\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "* By default, pyplot plots line graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b1d04",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb302a7",
   "metadata": {},
   "source": [
    "#### Creating a distance matrix using linkage\n",
    "\n",
    "`scipy.cluster.hierarchy.linkage(observations, method='single', metric='euclidean', optimal_ordering=False)`\n",
    "* This process computes the distances between clusters as we go from n clusters to one cluster where n is the number of points\n",
    "* `method`: how to calculate the proximity of clusters\n",
    "* `metric`: distance metric (Euclidean, Manhattan...)\n",
    "* `optimal_ordering`: order data points (optional argument)\n",
    "\n",
    "* **`method`**:\n",
    "    * **single:** based on two closest objects (clusters tend to be more dispersed)\n",
    "    * **complete:** based on two farthest objects\n",
    "    * **average:** based on the arithmetic mean of all objects\n",
    "    * **centroid:** based on the geometric mean of all objects\n",
    "    * **median:** based on the median of all objects\n",
    "    * **ward:** based on the sum of squares (clusters tend to be dense towards the centers)\n",
    "    \n",
    "* **Create cluster labels with fcluster:**\n",
    "`scipy.cluster.hierarchy.fcluster(distance_matrix, num_clusters, criterion)`\n",
    "* `distance_matrix`: output of `linkage` method\n",
    "* `num_clusters`: number of clusters\n",
    "* `criterion`: how to decide thresholds to form clusters\n",
    "\n",
    "* **Note** that in all seaborn plots, an extra cluster with label 0 is shown even though no objects are present in it. This can be removed it you store the cluster labels as strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8f277",
   "metadata": {},
   "source": [
    "#### Visualize Clusters\n",
    "* Visualizing clusters may help to make sense of clusters formed or identify number of clusters\n",
    "* Visualizing can serve as an additional step in the validation of clusters formed\n",
    "* May help you to spot trends in data\n",
    "* For clustering, we will often use pandas DataFrames to store our data, often adding a separate column for cluster centers\n",
    "\n",
    "```\n",
    "df = pd.DataFrame({'x':[2, 3, 5, 6, 2], 'y': [1, 1, 5, 5, 2], 'labels': ['A', 'A', 'B', 'B', 'A']})\n",
    "```\n",
    "#### Visualizing clusters with matplotlib\n",
    "\n",
    "```\n",
    "from matplotlib import pyplot as plt\n",
    "colors = {'A': 'red', 'B': 'blue'}\n",
    "df.plot.scatter(x='x', y='y', c = df['labels'].apply(lambda x: colors[x]))\n",
    "plt.show()\n",
    "```\n",
    "* We use the `c` argument of the scatter method to assign a color to each cluster\n",
    "* However, we first need to manually map each cluster to a color\n",
    "* Create dictionary `colors` with the cluster labels as keys and respectively associated colors as values\n",
    "\n",
    "#### Visualizing clusters with seaborn\n",
    "\n",
    "```\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'labels', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* Two reasons to prefer seaborn:\n",
    "    * 1) For implementation, using seaborn is more convenient once you have stored cluster labels in your dataframe\n",
    "    * 2) You do not need to manually select colors (there is a default palette that you can manually change if you so choose, but is not necessary)\n",
    "    \n",
    "#### Determining how many clusters with dendrograms\n",
    "* ** Dendrograms:**\n",
    "    * dendrograms help show progressions as clusters are merged\n",
    "    * a dendrogram is a branching diagram that demonstrates how each cluster is composed by branching out into its child nodes\n",
    "    \n",
    "```\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "Z = linkage(df[['x_whiten', 'y_whiten']], method = 'ward', metric = 'euclidean')\n",
    "dn = dendrogram(Z)\n",
    "plt.show()\n",
    "```\n",
    "* Recall the hierarchical clustering algorithm, where each step was a result of merging the two closest clusters in the earlier step\n",
    "* The x-axis of a dendrogram represents individual points, whereas the y-axis represents the distance or dissimilarity between clusters\n",
    "* The inverted U at the top of a dendrogram represents a single cluster of all datapoints\n",
    "* The width of the inverted U-shape represents the distance between the two child clusters. Therefore, a wider inverted-U shape means that the two child clusters were further away from each other as compared to a narrower inverted-U in the diagram.\n",
    "* If you draw a horizontal line at any part of the figure, the number of vertical lines it intersects with tells you the number of clusters at that stage and the distance between those vertical lines indicates the **intercluster distance.**\n",
    "* **Note:** There is no \"right\" metric to determine \"how many\" clusters are ideal.\n",
    "* An additional step of visualizing the data in a scatter plot (after visualizing it in a dendrogram) may be helpful before deciding on the number of clusters.\n",
    "\n",
    "```\n",
    "# Import the dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "\n",
    "# Display the dendogram\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0561393",
   "metadata": {},
   "source": [
    "#### Limitations of hierarchical clustering\n",
    "**Measuring speed in hierarchical clustering:**\n",
    "* Use `timeit` module\n",
    "* The most time-consuming step is constructing the distance matrix using `.linkage()` method\n",
    "* To check the time of a function in the interpreter, use `%` before the `timeit` keyword followed by the statement that you would like timed.\n",
    "```\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import pandas as pd\n",
    "import random, timeit\n",
    "points = 100\n",
    "df = pd.DataFrame({'x': random.sample(range(0, points), points),\n",
    "                   'y': random.sample(range(0, points), points)}) \n",
    "%timeit linkage(df[['x', 'y']], method = 'ward', metric = 'euclidean')\n",
    "```\n",
    "* if you plot the runtime as datapoints, you'll see there's a quadratic increase of runtime, making `.linkage()` not feasible for large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8c793",
   "metadata": {},
   "source": [
    "#### Basics of k-means clustering\n",
    "* **Why k-means clustering?**\n",
    "    * A critical drawback of hierarchical clustering: runtime\n",
    "    * K means runs significantly faster on large datasets\n",
    "    \n",
    "#### Step 1: Generate cluster centers\n",
    "* generate the cluster centers and then assign the cluster labels\n",
    "* `kmeans(obs, k_or_guess, iter, thresh, check_finite)`\n",
    "    * `obs`: standardized observations (which have been standardized with the `whiten` method)\n",
    "    * `k_or_guess`: number of clusters\n",
    "    * `iter`: number of iterations (default: 20)\n",
    "    * `thresh`: threshold (default: 1e -05)\n",
    "        * the idea behind this argument is that the algorithm is terminated if the change in distortion since the last k-means iteration is less than or equal to the threshold\n",
    "    * `check_finite`: boolean value; whether to check if observations contain only finite numbers (and *not* infinite or `NaN` values); default: `True`\n",
    "* `kmeans` function returns two arguments: cluster centers (also known as \"the code book index\"), distortion\n",
    "* kmeans runs really quickly as opposed to hierarchical clustering\n",
    "* `distortion` is calculated as the sum of squares of distances of points from cluster centers\n",
    "\n",
    "#### Step 2: Generate cluster labels\n",
    "* `vq(obs, code_book, check_finite=True)`\n",
    "    * `obs`: standardized observations which have been standardized with the `whiten` method.\n",
    "    * `code_book`: cluster centers\n",
    "    * `check_finite`: see above; default: `True`\n",
    "* the `vq()` function returns the cluster labels (aka \"the code book index\"), a list of distortions\n",
    "#### A note on distortions\n",
    "* `kmeans` returns a single value of distortions\n",
    "* `vq` returns a list of distortions\n",
    "    * the mean of the list of distortions from the `vq` method should approximately equal the distortion value of the `kmeans` method if the same list of observations is passed through\n",
    "\n",
    "```\n",
    "from scipy.cluster import kmeans, vq\n",
    "\n",
    "cluster_centers, _ = kmeans(df[['scaled_x', 'scaled_y']], 3)\n",
    "df['cluster_labels'], _ = vq(df[['scaled_x', 'scaled_y']], cluster_centers)\n",
    "\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', hue='cluster_labels', data=df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**kmeans** $\\Rightarrow$ to get **cluster *centers***\n",
    "\n",
    "**vq** $\\Rightarrow$ to get **cluster *labels***\n",
    "\n",
    "```\n",
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f738f9",
   "metadata": {},
   "source": [
    "#### Elbow method\n",
    "* No *absolute* method to find the right number of clusters (k), in k-means clustering\n",
    "* Constructing an elbow plot to decide the right number of clusters for your dataset; x-axis = 3 of clusters k, y-axis = distortion.\n",
    "* **distortions:** the sum of squares of the distances of points from their respective clusters\n",
    "* Ideally, distortion has an inverse relationship with the number of clusters\n",
    "    * In other words, **distortion decreases with increasing k.**\n",
    "    * distortion becomes zero when the number of clusters equals the number of points $\\Leftarrow$ This is the underlying logic of the elbow method\n",
    "* **Elbow method:** line plot between cluster centers and distortion\n",
    "    * First, run kmeans clustering with a varying number of clusters on the data and construct an elbow plot which has the number of clusters on the x-axis and the distortion on the y-axis\n",
    "    * The number of clusters can start at one and go up to the total number of data points\n",
    "    * The ideal point is one beyond which the distortion decreases relatively less (?) on increasing the number of clusters\n",
    "    \n",
    "```\n",
    "# Declaring variables for use\n",
    "distortions = []\n",
    "num_clusters = range(2, 7)\n",
    "\n",
    "# Populating distortions for various clusters\n",
    "for i in num_clusters:\n",
    "    centroids, distortion = kmeans(df[['scaled_x', 'scaled_y']], i)\n",
    "    distortions.append(distortion)\n",
    "    \n",
    "# Plotting elbow plot data\n",
    "elbow_plot_data = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "sns.lineplot(x='num_clusters', y='distortions', data= 'elbow_plot_data')\n",
    "plt.show()\n",
    "```\n",
    "* Note: Elbow method only gives an indication of ideal number of clusters\n",
    "* Occassionally, it may be insufficient to find an optimal k\n",
    "* For example, the elbow method fails when data is even distributed\n",
    "* Other methods: **average silhouette** and **gap statistic**\n",
    "\n",
    "```\n",
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists - num_clusters, distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Creat a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5786be",
   "metadata": {},
   "source": [
    "#### Limitations of kmeans clustering\n",
    "* kmeans clustering overcomes the biggest drawback of hierarchical clustering (run time)\n",
    "* However, it comes with its own set of **limitations** for consideration:\n",
    "    * **procedure for finding the \"right\" number of k**\n",
    "        * elbow method\n",
    "        * silhouette method\n",
    "        * gap statistic\n",
    "    * **impact of seeds**\n",
    "    * **biased towards equal-sized clusters**\n",
    "    \n",
    "* **Impact of seeds:**\n",
    "    * As the process of defining the intial cluster centers is random, this initialization can affect the final clusters\n",
    "    * Therefore, to get consistent results when running kmeans clustering on the same data set multiple times, it is a good idea to set the initializtion parameters for random number generation \n",
    "    * The seed is initialized through the seed method of `random` class in numpy\n",
    "    * You can pass a single integer or a 1-D argument as an array\n",
    "    * **Interestingly, the effect of seeds is only seen when the data to be clustered is fairly uniform.** If the data has distinct clusters before clustering is performed, the effect of seeds will not result in any changes in the formation of resulting clusters\n",
    "    \n",
    "```\n",
    "from numpy import random\n",
    "random.seed(12)\n",
    "```\n",
    "\n",
    "* **Bias towards equal-sized clusters:**\n",
    "    * Can get very non-intuitive results\n",
    "    * Bias is because the very idea of kmeans clustering is to minimize distortions. \n",
    "    * This results in clusters that have similar areas and not necessarily similar numbers of data points\n",
    "    * For very differently-sized clusters: hierarchical clustering will likely do a better job (if you can afford the run time)\n",
    "    \n",
    "\n",
    "* Each technique has its pros and cons\n",
    "* Consider your data size and patterns before deciding on an algorithm\n",
    "* Clustering is still exploratory phase of analysis\n",
    "\n",
    "```\n",
    "# Set up a random seed in numpy\n",
    "random.seed([1000,2000])\n",
    "\n",
    "# Fit the data into a k-means algorithm\n",
    "cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "fifa['cluster_labels'], _ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)\n",
    "\n",
    "# Display cluster centers \n",
    "print(fifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "# Create a scatter plot through seaborn\n",
    "sns.scatterplot(x='scaled_def', y='scaled_phy', hue='cluster_labels', data=fifa)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2127011",
   "metadata": {},
   "source": [
    "## Dominant colors in images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421e221",
   "metadata": {},
   "source": [
    "* Let's use clustering on some **real world problems**\n",
    "* **Analyze images to determine dominant colors**\n",
    "* Each pixel consists of three values: each value is a number between 0 and 255, representing: R, G, B\n",
    "* Pixel color = combination of these RGB values\n",
    "* Goal: perform k-means clustering on standardized RGB values to find cluster centers\n",
    "* Uses: Identifying features in satellite images\n",
    "\n",
    "**Tools to find dominant colors:**\n",
    "* Convert image to pixels: **`matplotlib.image.imread`**\n",
    "    * Converts a jpeg image into a matrix which contains the RGB values of ech pixel \n",
    "* Display colors of cluster centers: **`matplotlib.pyplot.imshow`**\n",
    "\n",
    "* **First step: Convert image to RBG matrix:**\n",
    "\n",
    "```\n",
    "import matplotlib.image as img\n",
    "image = img.imread('sea.jpg')\n",
    "image.shape\n",
    "```\n",
    "* **Note** the output of this call is a MxNx3 matrix (pronounced \"M cross N cross three\"), where M and N are the dimensions of the image.\n",
    "* In this analysis, we are going to collectively look at all pixels and their positions would not matter. Hence, we will just extract all RGB values and store them in their corresponding lists\n",
    "\n",
    "```\n",
    "r = []\n",
    "g = []\n",
    "b = []\n",
    "\n",
    "for row in image:\n",
    "    for pixel in row:\n",
    "        # A pixel contains RGB values \n",
    "        temp_r, temp_g, temp_b = pixel \n",
    "        r.append(temp_r)\n",
    "        g.append(temp_g)\n",
    "        b.append(temp_b)\n",
    "        \n",
    "pixels = pd.DataFrame({'red': r, 'green': g, 'blue': b})\n",
    "pixels.head()\n",
    "```\n",
    "Create an elbow plot from the pixel color data\n",
    "\n",
    "```\n",
    "distortions = []\n",
    "num_clusters = range(1,11)\n",
    "\n",
    "# Create a list of distortions from the kmeans method\n",
    "for i in num_clusters:\n",
    "    cluster_centers, _= kmeans(pixels[['scaled_red', 'scaled_blue', 'scaled_green']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a dataframe with two lists: number of clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data=elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()\n",
    "```\n",
    "* Elbow plot successfully shows the number of dominant colors in the image\n",
    "* recall that the cluster centers obtained are standardized RGB values. A standardized value of a variable is its actual value divided by the standard deviation\n",
    "\n",
    "```\n",
    "colors = []\n",
    "\n",
    "# Find standard deviations\n",
    "r_std, g_std, b_std = pixels[['red', 'green', 'blue']].std()\n",
    "\n",
    "# Scale actual RGB values in range of 0-1\n",
    "for cluster_center in cluster_centers:\n",
    "    scaled_r, scaled_g, scaled_b = cluster_center\n",
    "    colors.append((\n",
    "        scaled_r * r_std/255,\n",
    "        scaled_g * g_std/255,\n",
    "        scaled_b * b_std/255\n",
    "    ))\n",
    "```\n",
    "* **Display dominant colors:**\n",
    "\n",
    "```\n",
    "# Dimensions: 2 x 3 (N x 3 matrix)\n",
    "print(colors)\n",
    "\n",
    "# Dimensions: 1 x 2 x 3 (1 x N x 3 matrix)\n",
    "plt.imshow([colors])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Batman example:\n",
    "\n",
    "```\n",
    "# Import image class of matplotlib\n",
    "import matplotlib.image as img\n",
    "\n",
    "# Read batman image and print dimensions\n",
    "batman_image = img.imread('batman.jpg')\n",
    "print(batman_image.shape)\n",
    "\n",
    "# Store RGB values of all pixels in lists r, g and b\n",
    "for row in batman_image:\n",
    "    for temp_r, temp_g, temp_b in row:\n",
    "        r.append(temp_r)\n",
    "        g.append(temp_g)\n",
    "        b.append(temp_b)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(batman_df[['scaled_red', 'scaled_green', 'scaled_blue']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists, num_clusters and distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions':distortions})\n",
    "\n",
    "# Create a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Get standard deviations of each color\n",
    "r_std, g_std, b_std = batman_df[['red', 'green', 'blue']].std()\n",
    "\n",
    "for cluster_center in cluster_centers:\n",
    "    scaled_r, scaled_g, scaled_b = cluster_center\n",
    "    # Convert each standardized value to scaled value\n",
    "    colors.append((\n",
    "        scaled_r * r_std / 255,\n",
    "        scaled_g * g_std / 255,\n",
    "        scaled_b * b_std / 255\n",
    "    ))\n",
    "\n",
    "# Display colors of cluster centers\n",
    "plt.imshow([colors])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24f91d",
   "metadata": {},
   "source": [
    "#### Document clustering\n",
    "* Document clustering uses some concepts from NLP\n",
    "* Steps:\n",
    "    * 1) Clean data before processing\n",
    "    * 2) Determine the importance of the terms in a document (in TF-IDF matrix)\n",
    "    * 3) Cluster the TF-IDF matrix\n",
    "    * 4) Find top terms, documents in each cluster\n",
    "    \n",
    "#### Step 1: Clean and tokenize data\n",
    "* Convert text into smaller parts, called tokens, clean data for processing \n",
    "\n",
    "```\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def remove_noise(text, stop_words=[]):\n",
    "    tokens = word_tokenize(text\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub('[^A-Za-z0-9]+', '', token)\n",
    "        if len(token) > 1 and token.lower() not in stop_words:\n",
    "        #Get lowercase\n",
    "        cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "```\n",
    "#### Document matrices and sparse matrices\n",
    "* Once relevant terms have been extracted, a **document term matrix** is formed \n",
    "    * An element of this matrix signifies how many times a term has occurred in each document\n",
    "    * Most elements are zeroes, hence a **sparse matrix** is formed\n",
    "    \n",
    "* A **sparse matrix** only contains terms which have non-zero elements\n",
    "    * A sparse matrix often consists of observations: **row** (of non-zero value), **column** (of non-zero value), **value** (itself).\n",
    "    \n",
    "    \n",
    "#### TF-IDF: Term frequency - Inverse Document Frequency\n",
    "* A weighted measure: evaluate how important a word is to a document in a collection\n",
    "* `max_df` and `min_df` signify the maximumm and minimum fraction of documents a word should occur in.\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(mx_df = 0.8, max_features = 50, min_df = 0.2, tokenize = remove_noise)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "```\n",
    "\n",
    "#### Clustering with sparse matrix\n",
    "* `kmeans` in SciPy does not work with sparse matrices\n",
    "* Use `.todense()` to convert sparse matrix to its expanded form\n",
    "\n",
    "`cluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)`\n",
    "* Note: we do not make an elbow plot, as it will take an erratic form due to the high number of variables\n",
    "\n",
    "* Each **cluster center** is a list with a size equal to the number of terms\n",
    "* Each value in the cluster center is its importance\n",
    "* Create a dictionary and print top terms\n",
    "\n",
    "```\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    center_terms = dict(zip(terms, list(cluster_centers[i])))\n",
    "    sorted_terms = sorted(center_terms, key= center_terms.get, reverse= True)\n",
    "    print(sorted_terms[:3})\n",
    "```\n",
    "* The above example is a **very simple** example of NLP.\n",
    "* Other consideration include:\n",
    "    * Work with hyperlinks, emoticons, etc\n",
    "    * Normalize words to their base forms (run, ran, running $\\Rightarrow$ run)\n",
    "    * `todense()` may not work with large datasets, so you may need to consider an implementation of kmeans that works with sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5213f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c97a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f7e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
