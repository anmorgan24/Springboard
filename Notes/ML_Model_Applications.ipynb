{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027b8a0e",
   "metadata": {},
   "source": [
    "# Machine Learning Model Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd47496",
   "metadata": {},
   "source": [
    "It's crucial that you develop an ability to determine which modeling metric to use based on the type of response variable you have and the business problem you're trying to solve. Likewise, it's also essential to be able to communicate your model's performance in terms that a stakeholder can understand. For example, if you can say your model predicts home values with an error range of \\\\$5,000 to $10,000 instead of an error rate of 5-10%, your stakeholders will quickly understand the impact of what you've created. \n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the machine learning process starts; it is a value that controls the learning process. Hyperparameter tuning is when you determine a set of hyperparameters to govern a learning algorithm. Hyperparameter tuning can make a world of difference â€” it can make or break your model's prediction abilities. For example, you may be able to improve your model's accuracy by 5% just by optimizing the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1eebb",
   "metadata": {},
   "source": [
    "#### Supervised Machine Learning Model Evaluation Metrics\n",
    "* You should consider your response variable when choosing a machine learning model to build. \n",
    "* There are a variety of models out there, so it's important to choose one that's right for your data and the business problem you're trying to solve. \n",
    "\n",
    "* What's an evaluation metric?\n",
    "    * A way to quantify performance of a machine learning model\n",
    "    * Evaluation metric $\\neq$ Loss function\n",
    "        * The two can be the same thing, but don't have to be and aren't always.\n",
    "    * A loss function is something that you use while you are training your model, while you're optimizing, while evaluation metrics are used on an alredy trained machine learning model \n",
    "    #### Supervised Learning Metrics:\n",
    "    * **Classification:** Classification accuracy, Precision, Recall, F1 Score, ROC/AUC, Precision/Recall AUC, Matthews Correlation Coefficient, Log loss... etc...\n",
    "    * **Regression:** $R^{2}$, MAE, MSE, RMSE, RMSLE, etc...\n",
    "    \n",
    "### Classification Metrics:\n",
    "#### Binary Classificstion:\n",
    "\n",
    "* **Accuracy** = (Number of correct predictions) / (Total number of predictions)\n",
    "    * Ranges from 0%-100% or 0 to 1\n",
    "    * Very intuitive\n",
    "    * **Easily calculate with `sklearn` with `.score` method**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7fb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b162d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06746eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605f032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566bee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
