{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af121f07",
   "metadata": {},
   "source": [
    "# Case Study: School Budgeting with Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6908bfb",
   "metadata": {},
   "source": [
    "**DataDriven:** runs online data science challenges for non-profits, NGOs and social enterprises\n",
    "* Goals of the case study:\n",
    "    * **Use data to have a social impact**\n",
    "    * **Build a machine learning algorithm that can automate the school budgeting process**\n",
    "    \n",
    "* Introduction to the challenge:\n",
    "    * NLP\n",
    "    * Feature Engineering\n",
    "    * Efficiency Boosting hashing tricks\n",
    "    * Supervised learning problem (labeled)\n",
    "    * Over 100 target variables\n",
    "    * Classification problem: we want to predict a category for each line item\n",
    "    * Predictions will be probabilities for each label\n",
    "\n",
    "* A **human-in-the-loop machine-learning system (HITL):** is a branch of artificial intelligence that leverages both human and machine intelligence to create machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758f522",
   "metadata": {},
   "source": [
    "#### Exploring the Data:\n",
    "* Encode labels as categories\n",
    "* ML algorithms work on numbers, not strings\n",
    "    * Need a numeric representation of these strings\n",
    "* Strings can be slow compared to numbers\n",
    "    * We never know ahead of time how long a string is, so our computer has to take more time to process strings than numbers (which have a precise number of bits)\n",
    "* In pandas, `category` dtype encodes categorical data numerically\n",
    "    * Can help speed up code\n",
    "    * use `.astype('category')\n",
    "    * to see the numerical representation of 'categories' use:\n",
    "        * `dummies = pd.get_dummies(sample_df[['label']], prefix_sep=' ')`\n",
    "        * Dummy encoding also called a **`binary indicator` representation**\n",
    "        \n",
    "#### Lambda functions\n",
    "* Alternative to `def` syntax\n",
    "* Easy way to make simple, one line functions\n",
    "* `square = lambda x: x*x`\n",
    "    * $\\Uparrow$ We can define a lambda function that takes a paramater (the variable, *x*), the function itself just multiplies *x* by *x* and returns the result.\n",
    "    * Call this function, just like any other function:\n",
    "        * `square(2)`\n",
    "        \n",
    "* In the budget data, there are multiple columns that need to be made categorical\n",
    "* To make multiple columns into categories, we need to apply the function to each column separately:\n",
    "    * `categorize_label = lambda x: x.astype('category')`\n",
    "    * `sample_df.label =sample_df[['label']].apply(categorize_label, axis = 0)`\n",
    "* `df.dtypes.value_counts()`\n",
    "\n",
    "```\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)\n",
    "```\n",
    "* `num_unique_labels = df[LABELS].apply(pd.Series.nunique)`\n",
    "\n",
    "```\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cef3d",
   "metadata": {},
   "source": [
    "#### How do we measure success?\n",
    "* Choosing how to evaluate your machine learning model is one of the most important decisions an analyst makes. \n",
    "* The decision balances the real world use of the algorithm, the mathematical properties of the evaluation function, and the interpretability of the measure\n",
    "* **Accuracy** Tells us what percentage of rows we got right\n",
    "    * Accuracy can be misleading, especially when classes are imbalanced\n",
    "    * Think of email spam example: accuracy not good for measuring success of this model\n",
    "* Metric used for spam example: **log loss**\n",
    "    * loss function/measure of error\n",
    "    * In contrast to success measures, like accuracy, we want our measures of error to be as small as possible\n",
    "    * Log loss penalizes confidently wrong over inconfidence\n",
    "        * **Better to be less confident than wrong**  \n",
    "        \n",
    "#### Computing log loss with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973ecc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    \"\"\" Comuptes the logarithmic loss between predicted and actual when these are 1D arrays.\n",
    "        \n",
    "        :param predicted: The predicted probabilities as floats between 0-1\n",
    "        :param actual: The actual binary labels. Either 0 or 1.\n",
    "        :params eps (optional): log(0) is inf, so we need to offset our predicted values slightly by eps from 0 or 1.\n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1-eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d7c0b",
   "metadata": {},
   "source": [
    "* **`.clip()`** function which sets a maximum and a minimum value for the elements in an array. \n",
    "* Since log of 0 is negative infinity, we want to offset our predictions ever so slightly from being exactly 1 or exactly 0 so that our score remains a real number. \n",
    "* In the above example we use eps to achieve this. \n",
    "* computing log loss with numpy:\n",
    "\n",
    "* `compute_log_loss(predicted= 0.9, actual= 0)`\n",
    "* `compute_log_loss(predicted= 0.5, actual= 1)`\n",
    "\n",
    "```\n",
    "# Compute and print log loss for 1st case\n",
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bac5c",
   "metadata": {},
   "source": [
    "### Creating a simple first model\n",
    "* It's always a good approach to start with a very simple model \n",
    "* Creating a simple model first helps to give us a sense of how challenging the problem is and also where our baseline is\n",
    "* Many more things can go wrong in complex models \n",
    "* How much signal can we pull out using basic methods\n",
    "\n",
    "* Train basic model on numeric model \n",
    "    * We want to go from raw data to predictions as quickly as possible\n",
    "* In this case we'll use multi-class logistic regression\n",
    "    * treats each label column as independent\n",
    "    * train classifier on each label separately and use those to predict\n",
    "    * Format predictions and save to csv\n",
    "* **`StratifiedShuffleSplit()`**\n",
    "    * Only works if you have a single target variable\n",
    "    \n",
    "* **`multilabel_train_test_split()`** \n",
    "\n",
    "```\n",
    "data_to_train = df[NUMERIC_COLUMNS].fill_na(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size =0.2, seed=123)\n",
    "\n",
    "# Training the model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "```\n",
    "* `OneVsRestClassifier` treats each column of y independently \n",
    "    * fits a separate classifier for each of the columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4de8b",
   "metadata": {},
   "source": [
    "#### Make predictions\n",
    "* Predicting on holdout data\n",
    "\n",
    "```\n",
    "holdout = pd.read_csv('HoldoutData.csv', index_col =0)\n",
    "holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
    "predictions = clf.predict_proba(holdout)\n",
    "predictions = clf.predict_proba(holdout)\n",
    "\n",
    "```\n",
    "\n",
    "* In data science competitions, its a standard practice to write your predictions to a csv and then upload that csv to the competition platform\n",
    "* Read competition documentation for submission format for a particular challenge\n",
    "* All formatting can be done with the pandas `to_csv()` function\n",
    "\n",
    "* Format and submit predictions:\n",
    "\n",
    "```\n",
    "prediction_df = pd.DataFrame(columns = pd.get_dummies(df[LABELS], \n",
    "                                prefix_sep='__').columns, index=holdout.index, \n",
    "                                data=predictions)\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "```\n",
    "\n",
    "```\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('HoldoutData.csv')\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "```\n",
    "\n",
    "```\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "\n",
    "# Print score\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620c814",
   "metadata": {},
   "source": [
    "#### A very brief introduction to NLP\n",
    "* When we have data that is text we often want to process this text to create features for our algorithm\n",
    "* Data for NLP can be:\n",
    "    * Text, documents, speech, etc. ...\n",
    "* First step is: Tokenization\n",
    "* **Tokenization**: is the process of splitting a string into segments, called \"tokens\"\n",
    "    * Store segments as a list \n",
    "    \n",
    "#### Tokens and token patterns\n",
    "* Options for how to tokenizae;\n",
    "    * Tokenize on whitespace (i.e. split every time there is a space, tab, or return)\n",
    "    * Tokenize on punctuation\n",
    "    * Tokenize on commas\n",
    "    * **Tokenize on whitespace *and* punctuation**\n",
    "    * etc...\n",
    "* For some datasets, we may want to split on words based on characters other than whitespace\n",
    "\n",
    "#### Bag of words representation\n",
    "* We want to use these tokens as part of our machine learning algorithm; often, the first way to do this is to count the number of times a particular token appears in a row\n",
    "* **Bag of words:** \n",
    "    * Count the number of times a word was pulled out of the \"bag\" \n",
    "    * One of the simplest ways to represent text in ML\n",
    "    * Discards information about grammar and word order\n",
    "    * Computes frequency of occurrence\n",
    "* However, this approach discards information about word order\n",
    "* A slightly more sophisticated approach is to create what are called **n-grams**\n",
    "* **1-gram, 2-gram, ..., n-gram:** in addition to a column for every token we see (which is called a 1-gram), we may have an ordered pair of every ___ words.\n",
    "\n",
    "#### Representing text numerically\n",
    "* **Bag of words:** \n",
    "    * Count the number of times a word was pulled out of the \"bag\" \n",
    "    * One of the simplest ways to represent text in ML\n",
    "    * Discards information about grammar and word order\n",
    "    * Computes frequency of occurrence\n",
    "    * Scikit tools for bag of words: \n",
    "        * **`CountVectorizer()`**: works by taking in an array of strings and doing three things:\n",
    "            * 1) Tokenizes all the strings \n",
    "            * 2) Builds a \"vocabulary\": it makes a note of all of the words that appear\n",
    "            * 3) Counts the occurences of each token in the vocabulary\n",
    "            \n",
    "#### Using CountVectorizer() on column of main dataset\n",
    "* Define a regex that does the splits on whitespace\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "TOKENS_BASIC = '\\\\\\\\S+(?=\\\\\\\\s+)'\n",
    "df.Program_Description.fillna('', inplace=True)\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "vec_basic.fit(df.Program_Description)\n",
    "print(len(vec_basic.get_feature_names()))\n",
    "```\n",
    "* `fit` creates a vocabulary \n",
    "* `transform` will tokenize the text and then produce an array of counts\n",
    "\n",
    "```\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5946b17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUMERIC_COLUMNS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b501cf3e17f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define combine_text_columns()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_text_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUMERIC_COLUMNS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\" converts all text in each row of data_frame to single vector \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Drop non-text columns that are in the df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUMERIC_COLUMNS' is not defined"
     ]
    }
   ],
   "source": [
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis= 'columns')\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace = True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463e72e",
   "metadata": {},
   "source": [
    "#### Pipelines, feature, and text preprocessing\n",
    "* Now its time to combine what we've learned about NLP with our model pipeline and incorporate the text data into our algorithm\n",
    "\n",
    "#### The pipeline workflow\n",
    "* Repeatable way to go from raw data to trained model \n",
    "* Pipeline object takes sequential list of steps\n",
    "    * Output of one step is the input to the next step\n",
    "* Each step is a tuple with two elements:\n",
    "    * Name: string\n",
    "    * Transform: obj implementing `.fit()` and `.transform()`\n",
    "* Flexible: a step can itself be another pipeline\n",
    "* The beauty of the pipeline is that it encapsulates every transformation from raw data to a trained model\n",
    "\n",
    "#### Instantiate simple pipeline with one step\n",
    "* We start with a one-step pipeline\n",
    "    * Obviously, we don't need a pipeline for a single step, this is just an exercise as a simple example\n",
    "* We create a pipeline by passing it a series of named steps\n",
    "* In the case below, the name is the string `clf`\n",
    "* The step is the OneVsRest-Logistic Regression Classifier\n",
    "\n",
    "```\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "#### Train and test with sample numeric data\n",
    "\n",
    "```\n",
    "pl = Pipeline([('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']], \n",
    "                                   pd.get_dummies(smaple_df['label']),\n",
    "                                   random_state =2)\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "```\n",
    "* **Using series/dfs with `NaN` values:**\n",
    "\n",
    "```\n",
    "pl = Pipeline([('imp', Imputer()), ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']], \n",
    "                                   pd.get_dummies(smaple_df['label']),\n",
    "                                   random_state =2)\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "```\n",
    "* **Note:** `Imputer()` added to pipeline to fill in `NaN` values\n",
    "* The default imputation in sklearn is to fill missing values with the mean of the column in question \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabf49",
   "metadata": {},
   "source": [
    "### Text features and feature unions\n",
    "#### Preprocessing text features\n",
    "\n",
    "```\n",
    "pl = Pipeline([('imp', Imputer()), ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'], \n",
    "                                   pd.get_dummies(smaple_df['label']),\n",
    "                                   random_state =2)\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "```\n",
    "#### Preprocessing multiple dtypes\n",
    "* We want to use **all** available features in one pipeline\n",
    "* Problem:\n",
    "    * Pipeline steps for numeric and text preprocessing can't follow each other \n",
    "    * e.g., output of `CountVectorizer()` can't be input to `Imputer()`\n",
    "    * `CountVectorizer()` won't know what to do with numerical data and `Imputer()` won't know what to do with text data\n",
    "    * In order to build our pipeline, we need to separately operate on the text columns and on the numeric columns \n",
    "* Solution:\n",
    "    * **`FunctionTransformer()`** and **`FeatureUnion()`**\n",
    "    \n",
    "#### FunctionTransformer()\n",
    "* **Turns a Python function into an object that a sklearn pipeline can understand**\n",
    "* Need to write two functions for pipeline preprocessing\n",
    "    * 1) Take entire dataframe, return numeric columns \n",
    "    * 2) Take entire dataframe, return text columns\n",
    "* Can then preprocess numeric and text data in separate pipelines\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']], \n",
    "                                   pd.get_dummies(smaple_df['label']),\n",
    "                                   random_state =2)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "union = FeatureUnion([('numeric', numeric_pipeline), ('text', text_pipeline)])\n",
    "\n",
    "numreic_pipeline = Pipeline([\n",
    "                        ('selector', get_numeric_data),\n",
    "                        ('imputer', Imputer())\n",
    "                    ])\n",
    "text_pipeline = Pipeline([\n",
    "                        ('selector', get_text_data),\n",
    "                        ('vectorizer', CountVectorizer())\n",
    "                    ])\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion([\n",
    "            ('numeric', numeric_pipeline),\n",
    "            ('text', text_pipeline)\n",
    "         ])),\n",
    "         ('clf', OneVsRestClassifier(LogisticRegression()00\n",
    "          ])\n",
    "```\n",
    "* **Note:** That we've passed `FunctionTransformer` the argument `validate=False`: this simply tells sklearn it doesn't need to check for `NaN`s or validate the dtypes of the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc3682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5def4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea89106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
