{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e082ba5",
   "metadata": {},
   "source": [
    "# Case Study: School Budgeting with Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b76387",
   "metadata": {},
   "source": [
    "**DataDriven:** runs online data science challenges for non-profits, NGOs and social enterprises\n",
    "* Goals of the case study:\n",
    "    * **Use data to have a social impact**\n",
    "    * **Build a machine learning algorithm that can automate the school budgeting process**\n",
    "    \n",
    "* Introduction to the challenge:\n",
    "    * NLP\n",
    "    * Feature Engineering\n",
    "    * Efficiency Boosting hashing tricks\n",
    "    * Supervised learning problem (labeled)\n",
    "    * Over 100 target variables\n",
    "    * Classification problem: we want to predict a category for each line item\n",
    "    * Predictions will be probabilities for each label\n",
    "\n",
    "* A **human-in-the-loop machine-learning system (HITL):** is a branch of artificial intelligence that leverages both human and machine intelligence to create machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a919641",
   "metadata": {},
   "source": [
    "#### Exploring the Data:\n",
    "* Encode labels as categories\n",
    "* ML algorithms work on numbers, not strings\n",
    "    * Need a numeric representation of these strings\n",
    "* Strings can be slow compared to numbers\n",
    "    * We never know ahead of time how long a string is, so our computer has to take more time to process strings than numbers (which have a precise number of bits)\n",
    "* In pandas, `category` dtype encodes categorical data numerically\n",
    "    * Can help speed up code\n",
    "    * use `.astype('category')\n",
    "    * to see the numerical representation of 'categories' use:\n",
    "        * `dummies = pd.get_dummies(sample_df[['label']], prefix_sep=' ')`\n",
    "        * Dummy encoding also called a **`binary indicator` representation**\n",
    "        \n",
    "#### Lambda functions\n",
    "* Alternative to `def` syntax\n",
    "* Easy way to make simple, one line functions\n",
    "* `square = lambda x: x*x`\n",
    "    * $\\Uparrow$ We can define a lambda function that takes a paramater (the variable, *x*), the function itself just multiplies *x* by *x* and returns the result.\n",
    "    * Call this function, just like any other function:\n",
    "        * `square(2)`\n",
    "        \n",
    "* In the budget data, there are multiple columns that need to be made categorical\n",
    "* To make multiple columns into categories, we need to apply the function to each column separately:\n",
    "    * `categorize_label = lambda x: x.astype('category')`\n",
    "    * `sample_df.label =sample_df[['label']].apply(categorize_label, axis = 0)`\n",
    "* `df.dtypes.value_counts()`\n",
    "\n",
    "```\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)\n",
    "```\n",
    "* `num_unique_labels = df[LABELS].apply(pd.Series.nunique)`\n",
    "\n",
    "```\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03aad3a",
   "metadata": {},
   "source": [
    "#### How do we measure success?\n",
    "* Choosing how to evaluate your machine learning model is one of the most important decisions an analyst makes. \n",
    "* The decision balances the real world use of the algorithm, the mathematical properties of the evaluation function, and the interpretability of the measure\n",
    "* **Accuracy** Tells us what percentage of rows we got right\n",
    "    * Accuracy can be misleading, especially when classes are imbalanced\n",
    "    * Think of email spam example: accuracy not good for measuring success of this model\n",
    "* Metric used for spam example: **log loss**\n",
    "    * loss function/measure of error\n",
    "    * In contrast to success measures, like accuracy, we want our measures of error to be as small as possible\n",
    "    * Log loss penalizes confidently wrong over inconfidence\n",
    "        * **Better to be less confident than wrong**  \n",
    "        \n",
    "#### Computing log loss with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38993159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    \"\"\" Comuptes the logarithmic loss between predicted and actual when these are 1D arrays.\n",
    "        \n",
    "        :param predicted: The predicted probabilities as floats between 0-1\n",
    "        :param actual: The actual binary labels. Either 0 or 1.\n",
    "        :params eps (optional): log(0) is inf, so we need to offset our predicted values slightly by eps from 0 or 1.\n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1-eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57fb38",
   "metadata": {},
   "source": [
    "* **`.clip()`** function which sets a maximum and a minimum value for the elements in an array. \n",
    "* Since log of 0 is negative infinity, we want to offset our predictions ever so slightly from being exactly 1 or exactly 0 so that our score remains a real number. \n",
    "* In the above example we use eps to achieve this. \n",
    "* computing log loss with numpy:\n",
    "\n",
    "* `compute_log_loss(predicted= 0.9, actual= 0)`\n",
    "* `compute_log_loss(predicted= 0.5, actual= 1)`\n",
    "\n",
    "```\n",
    "# Compute and print log loss for 1st case\n",
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd363b",
   "metadata": {},
   "source": [
    "### Creating a simple first model\n",
    "* It's always a good approach to start with a very simple model \n",
    "* Creating a simple model first helps to give us a sense of how challenging the problem is and also where our baseline is\n",
    "* Many more things can go wrong in complex models \n",
    "* How much signal can we pull out using basic methods\n",
    "\n",
    "* Train basic model on numeric model \n",
    "    * We want to go from raw data to predictions as quickly as possible\n",
    "* In this case we'll use multi-class logistic regression\n",
    "    * treats each label column as independent\n",
    "    * train classifier on each label separately and use those to predict\n",
    "    * Format predictions and save to csv\n",
    "* **`StratifiedShuffleSplit()`**\n",
    "    * Only works if you have a single target variable\n",
    "    \n",
    "* **`multilabel_train_test_split()`** \n",
    "\n",
    "```\n",
    "data_to_train = df[NUMERIC_COLUMNS].fill_na(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size =0.2, seed=123)\n",
    "\n",
    "# Training the model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "```\n",
    "* `OneVsRestClassifier` treats each column of y independently \n",
    "    * fits a separate classifier for each of the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70495f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bfaed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4bff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
