{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6645006",
   "metadata": {},
   "source": [
    "# Machine Learning for Time Series Data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826475bd",
   "metadata": {},
   "source": [
    "Time series data is ubiquitous. Whether it be stock market fluctuations, sensor data recording climate change, or activity in the brain, any signal that changes over time can be described as a time series. Machine learning has emerged as a powerful method for leveraging complexity in data in order to generate predictions and insights into the problem one is trying to solve. This course is an intersection between these two worlds of machine learning and time series data, and covers feature engineering, spectograms, and other advanced techniques in order to classify heartbeat sounds and predict stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453eb57a",
   "metadata": {},
   "source": [
    "## I. Time Series and Machine Learning Primer\n",
    "* Intro to the basics of machine learning, time series data, and the intersection between the two.\n",
    "\n",
    "### Timeseries kinds and applications\n",
    "* Put simply, a **timeseries** means data that changes over time.\n",
    "* This can take many different forms; from atmospheric CO2 over time, to the waveform of spoken word, to climate sensor data, the fluctuation of a stock's value over the year, demographic information about a city\n",
    "* **Timeseries data** consists of at least two things: \n",
    "    * One: an array of numbers that represents the data itself.\n",
    "    * Two: another array that contains a timestamp for each datapoint.\n",
    "* In other words, each datapoint should have a corresponding time point (whether that be a month, year, hour, or any combination of these). Note: multiple data points may have the same time point\n",
    "\n",
    "#### Plotting a pandas timeseries\n",
    "\n",
    "```\n",
    "import matplotlib.pyploy as plt\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "data.plot('date', 'close', ax=ax)\n",
    "ax.set(title='AAPL daily closing price')\n",
    "```\n",
    "* **The amount of time that passes between timestamps defines the *period* of the timeseries.**\n",
    "    * This often helps us infer what kind of timeseries we're dealing with.\n",
    "* One crucial part of machine learning is that we can build a model of the world that formalizes our knowledge of the problem at hand. We can...\n",
    "    * Predict the future\n",
    "    * Automate this process\n",
    "    * can be a critical component of an organization's decision making\n",
    "    \n",
    "* We treat timeseries data slightly differently than other types of datasets\n",
    "    * Timeseries data always change over time, which turns out to be a useful pattern to utilize\n",
    "    * Using timeseries-specific features lets us see a much richer representation of the raw data.\n",
    "    \n",
    "* This course will focus on a simple machine learning pipeline in the context of timeseries data.\n",
    "\n",
    "* **A machine learning pipeline:**\n",
    "    * Feature extraction: What kind of special features leverage a signal that changes over time?\n",
    "    * Model fitting: What kinds of models are suitable for asking questions with timeseries data?\n",
    "    * Prediction and Validation: How can we validate a model that uses timeseries data? What considerations must we make because it changes in time?\n",
    "\n",
    "```\n",
    "# Plot the time series in each dataset\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "data.iloc[:1000].plot(y='data_values', ax=axs[0])\n",
    "data2.iloc[:1000].plot(y='data_values', ax=axs[1])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Plot the time series in each dataset\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "data.iloc[:1000].plot(x='time', y='data_values', ax=axs[0])\n",
    "data2.iloc[:1000].plot(x='time', y='data_values', ax=axs[1])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Machine Learning Basics\n",
    "* Always begin by looking at your data:\n",
    "    * `array.shape`\n",
    "    * `array[:3]`\n",
    "    * `dataframe.head()`\n",
    "    * `dataframe.info()`\n",
    "    * `dataframe.describe()`\n",
    "* It is also crucial to visualize your data:\n",
    "\n",
    "```\n",
    "# Using matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(...)\n",
    "\n",
    "# Using pandas\n",
    "fig, ax = plt.subplots()\n",
    "df.plot(..., ax=ax)\n",
    "```\n",
    "* The proper visualization will depend on the kind of data you've got, though histograms and scatterplots are a good place to start.\n",
    "\n",
    "* The most popular library for machine-learning in Python is `scikit-learn`.\n",
    "    * Standardized API so that you can fit many different models with a similar code structure\n",
    "    \n",
    "#### Preparing data for scikit-learn\n",
    "* `scikit-learn` expects a particular structure of data:\n",
    "    * **`(samples, features)`**\n",
    "* Make sure that your data is *at least two-dimensional.*\n",
    "* Make sure the first dimension is *samples*.\n",
    "* The first axis should correspond to sample number, and the second axis should correspond to feature number.\n",
    "\n",
    "* If the axes are swapped: **transpose**\n",
    "    * `array.transpose().shape`\n",
    "    * `dataframe.T.shape`\n",
    "    * will swap first and last axis\n",
    "   \n",
    "* Use **`.reshape()`** method:\n",
    "    * lets you specify the shape you want\n",
    "\n",
    "```\n",
    "array.shape\n",
    "array.reshape([-1, 1]).shape\n",
    "```\n",
    "   * `-1` will automatically fill that axis with remaining values\n",
    "   \n",
    "* **Investigating the model:**\n",
    "    * It is often useful to investigate what kind of pattern the model has found.\n",
    "    * Most models will store this information in attributes that are created after calling `.fit()`\n",
    "        * `model.coef_`\n",
    "        * `model.intercept_`\n",
    "    * Call `.predict()` on the model to determine labels for unseen datapoints\n",
    "\n",
    "```\n",
    "# Generate predictions with the model using those inputs\n",
    "predictions = model.predict(new_inputs.reshape(-1, 1))\n",
    "\n",
    "# Visualize the inputs and predicted values\n",
    "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('predictions')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Combining timeseries data with machine learning\n",
    "* Interaction between ML and timeseries data; introduce why they're worth thinking about in tandem.\n",
    "#### The heartbeat acoustic data\n",
    "* Why acoustic data? Audio is a very common kind of timeseries data\n",
    "* Many recordings of heart sounds from different patients\n",
    "* Some had normally-functioning hearts, others had abnormalities\n",
    "* Data comes in the form of audio files + labels for each file\n",
    "* Goal? Can we find the \"abnormal\" heart beats?\n",
    "\n",
    "* Audio tends to have a very high sampling frequency (often above 20,000 samples per second)\n",
    "* Audio data is often stored in `.wav` files\n",
    "* list all of these files using the `glob` function:\n",
    "    * lists files that match a certain pattern\n",
    "\n",
    "```\n",
    "from glob import glob\n",
    "files = glob('data/heartbeat-sounds/files/*.wav')\n",
    "print(files)\n",
    "```\n",
    "* We'll use a library called **`librosa`** to read in the audio dataset:\n",
    "\n",
    "```\n",
    "import librosa as lr\n",
    "# 'load' accepts a path to an audio file\n",
    "audio, sfreq = lr.load('data/heartbeat-sounds/proc/files/murmur__201101051104.wav')\n",
    "print(sfreq)\n",
    "```\n",
    "* Output: `2205`\n",
    "\n",
    "* **`librosa`** has functions for extracting features, visualizations, and analysis for auditory data\n",
    "* Import the data using the `load` function\n",
    "* The data is stored as `audio` and the sampling frequency is stored in `sfreq`\n",
    "* In this case, the sampling frequency is `2205`, meaning there are `2205` samples per second.\n",
    "\n",
    "#### Inferring time from samples\n",
    "* If we know the sampling rate of a timeseries, then we know the timestamp of each datapoint *relative to the first datapoint*.\n",
    "* Note: this assumes the sampling rate is fixed and also that no data points are lost.\n",
    "* Now, **we can create an array of timestamps for out data:**\n",
    "    * Create an array of indices, one for each sample, and divide by the sample frequency.\n",
    "    * To do so, two options:\n",
    "        * 1. Genereate a range of indices from zero to the number of data points in your audio file; divide each index by the sampling frequency, and you have a timepoint for each data point.\n",
    "        * 2. Calculate the final timepoint of your audio data using a similar method; Find the time stamp for the *N-1*th data point. Then use `linspace()` to interpolate from zero to that time.\n",
    "    \n",
    "```\n",
    "indices = np.arange(0, len(audio))\n",
    "time = indices / sfreq\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "final_time = (len(audio) - 1) / sfreq\n",
    "time = np.linspace(0, final_time, sfreq)\n",
    "```\n",
    "\n",
    "* In either case, you should have an array of numbers of the same length as your audio data\n",
    "\n",
    "#### The New York Stock Exchange dataset\n",
    "* This dataset consists of company stock values for 10 years\n",
    "* This dataset runs over a much longer timespan that the audio data, and has a sampling frequency on the order of one sample per day (compared with 2,205 samples per second with the audio data).\n",
    "* Can we detect any patterns in historical records that allow us to predict the value of companies in the future.\n",
    "* As we are predicting a continuous output value, this is a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7a3ba",
   "metadata": {},
   "source": [
    "```\n",
    "import librosa as lr\n",
    "from glob import glob\n",
    "\n",
    "# List all the wav files in the folder\n",
    "audio_files = glob(data_dir + '/*.wav')\n",
    "\n",
    "# Read in the first audio file, create the time array\n",
    "audio, sfreq = lr.load(audio_files[0])\n",
    "time = np.arange(0, len(audio)) / sfreq\n",
    "\n",
    "# Plot audio over time\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, audio)\n",
    "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Read in the data\n",
    "data = pd.read_csv('prices.csv', index_col=0)\n",
    "\n",
    "# Convert the index of the DataFrame to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "print(data.head())\n",
    "\n",
    "# Loop through each column, plot its values over time\n",
    "fig, ax = plt.subplots()\n",
    "for column in data:\n",
    "    data[column].plot(ax=ax, label=column)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f624c4",
   "metadata": {},
   "source": [
    "### II. Timeseries as Inputs to a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee4dc4",
   "metadata": {},
   "source": [
    "#### Classification and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90486e46",
   "metadata": {},
   "source": [
    "* One of the most common categories for machine learning problems is classification.\n",
    "\n",
    "* **Always visualize raw data before fitting models!**\n",
    "* To plot raw audio, we need two things:\n",
    "    * the raw audio waveform, usually in a one- or two- dimensional array.\n",
    "    * we also need the timepoint of each sample\n",
    "    \n",
    "    \n",
    "```\n",
    "ixs = np.arange(audio.shape[-1])\n",
    "time = ixs / sfreq\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, audio)\n",
    "\n",
    "```\n",
    "* We can calculate the time by dividing the index of each sample by the sampling frequency of the timeseries.\n",
    "* This gives us the time for each sample relative to the beginning of the audio:\n",
    "\n",
    "* **What features to use?**\n",
    "* Using raw timeseries data is too noisy for classification\n",
    "* We need to calculate features!\n",
    "* An easy start: summarize your audio data\n",
    "    * summary statistics removes the \"time\" dimension and gives us a more traditional classification dataset.\n",
    "    * For each timeseries, we calculate several summary statistics (min, max, avg, etc)\n",
    "    * We have expanding a single feature (raw audio amplitude) to several features $\\Rightarrow$ min, max, avg of each sample.\n",
    "    \n",
    "* How to calculate multiple features for several timeseries \n",
    "\n",
    "```\n",
    "# print: (n_files, time)\n",
    "print(audio.shape)\n",
    "\n",
    "means = np.mean(audio, axis=-1)\n",
    "maxs = np.max(audio, axis = -1)\n",
    "stds = np.std(audio, axis =-1)\n",
    "\n",
    "# print: (n_files,)\n",
    "print(means.shape)\n",
    "```\n",
    "* By using the \"axis equals -1\" keyword, we collapse across the last dimension, which is time.\n",
    "\n",
    "#### Fitting a classifier with scikit-learn\n",
    "* We've just collapsed a 2-D dataset (samples x time) into several features of a 1-D dataset (samples)\n",
    "* We can combine each feature, and use it as input to a model\n",
    "* If we have a label for each sample, we can use scikit-learn to create and a fit a classifier\n",
    "\n",
    "* **In the case of classification, we also need a label for each timeseries that allows us to build a classifier.**\n",
    "\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "# Note that means are reshaped to work with sklearn\n",
    "X = np.column_stack([means, maxs, stds])\n",
    "y = labels.reshape([-1, 1])\n",
    "model = LinearSVC()\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "* The **`.column_stack()`** function lets us stack 1-D arrays by turning them into the columns of a 2-D array\n",
    "* Additionally, the labels array is 1-D, so we reshae it so that it is 2-D\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Different input data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Score our model with % correct\n",
    "# Manually\n",
    "percent_score = sum(predictions == labels_test) / len(labels_test)\n",
    "# Using an sklearn scorer\n",
    "percent_score = accuracy_score(labels_test, predictions)\n",
    "```\n",
    "\n",
    "```\n",
    "# Average across the audio files of each DataFrame\n",
    "mean_normal = np.mean(normal, axis=1)\n",
    "mean_abnormal = np.mean(abnormal, axis=1)\n",
    "\n",
    "# Plot each average over time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "ax1.plot(time, mean_normal)\n",
    "ax1.set(title=\"Normal Data\")\n",
    "ax2.plot(time, mean_abnormal)\n",
    "ax2.set(title=\"Abnormal Data\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and score them manually\n",
    "predictions = model.predict(X_test)\n",
    "print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc17a73",
   "metadata": {},
   "source": [
    "#### Improving the features we use for classification\n",
    "* A few more features that are unique to timeseries data\n",
    "\n",
    "#### The auditory envelope\n",
    "    * Smooth the data to calculate the auditory envelope\n",
    "    * Related to the total amount of audio energy present at each moment\n",
    "    * The envelope throws away information about the fine-grained changes in the signal, focusing on the general shape of the audio waveform \n",
    "    * To do this, we'll need to calculate the audio's amplitude, then smooth it over time. \n",
    "    \n",
    "#### Smoothing over time\n",
    "* Instead of averaging over *all* time can do a *local* average\n",
    "* This is called *smoothing* your timeseries\n",
    "* It removes short-term noise, while retaining the general pattern\n",
    "\n",
    "#### Calculating a rolling window statistic\n",
    "\n",
    "```\n",
    "# Audio is a pandas df\n",
    "print(audio.shape)\n",
    "# (n_times, n_audio_files)\n",
    "```\n",
    "* Output: `(5000, 20)`\n",
    "\n",
    "```\n",
    "# Smooth our data by taking the rolling mean in a window of 50 samples\n",
    "window_size = 50\n",
    "windowed = audio.rollin(window=window_size)\n",
    "audio_smooth = windowed.mean()\n",
    "```\n",
    "* The `.rolling()` method returns **an object that can be used to calculate many different statistics within each window.**\n",
    "* The `window` parameter tells us how many timepoints to include in each window\n",
    "    * The larger the window, the smoother the result will be\n",
    "    \n",
    "#### Calculating the auditory envelope\n",
    "* First *rectify* (calculate absolute value) your audio, then smooth it\n",
    "* Called \"rectification,\" because you ensure that all time points are positive.\n",
    "\n",
    "```\n",
    "audio_rectified = audio.apply(np.abs)\n",
    "audio_envelope = audio_rectified.rolling(50).mean()\n",
    "```\n",
    "* Once we've calculated the acoustic envelope, we can create better features for our classifier:\n",
    "\n",
    "#### Feature engineering the envelope\n",
    "\n",
    "```\n",
    "# Calculate several features of the envelope, one per second\n",
    "envelope_mean = np.mean(audio_envelope, axis=0)\n",
    "envelope_std = np.std(audio_envelope, axis=0)\n",
    "envelope_max = np.max(audio_envelope, axis=0)\n",
    "\n",
    "# Create our training data for a classifier\n",
    "X = np.column_stack([envelope_mean, envelope_std, envelope_max])\n",
    "y = labels.reshape([-1, 1])\n",
    "```\n",
    "\n",
    "* `cross_val_score` automates the process of:\n",
    "    * Splitting data into training/ validation sets\n",
    "    * Fitting the model on training data\n",
    "    * Scoring it on validation data\n",
    "    * Repeating this process\n",
    "    \n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LinearSVC()\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "print(scores)\n",
    "```\n",
    "\n",
    "#### Auditory features: The Tempogram\n",
    "* We can summarize more complex temporal information with timeseries-specifc functions\n",
    "* `librosa` is a great library for auditory and timeseries feature engineering\n",
    "* Tempogram attempts to detect particular patterns over time, and summarize them statistically\n",
    "* Here we'll caluclate the *tempogram*, which estimates the tempo of a sound over time.\n",
    "* We can calculate summary statistics of tempo in the same way that we can for the envelope\n",
    "\n",
    "#### Computing the Tempogram\n",
    "\n",
    "```\n",
    "# Import librosa and calculate the tempo of a 1-D sound array\n",
    "import librosa as lr\n",
    "audio_tempo = lr.beat.tempo(audio, sr=sfreq, hop_length=2**6, aggregate= None\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Plot the raw data first\n",
    "audio.plot(figsize=(10, 5))\n",
    "plt.show()\n",
    "\n",
    "# Rectify the audio signal\n",
    "audio_rectified = audio.apply(np.abs)\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified.plot(figsize=(10, 5))\n",
    "plt.show()\n",
    "\n",
    "# Smooth by applying a rolling mean\n",
    "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified_smooth.plot(figsize=(10, 5))\n",
    "plt.show()\n",
    "\n",
    "# Calculate stats\n",
    "means = np.mean(audio_rectified_smooth, axis=0)\n",
    "stds = np.std(audio_rectified_smooth, axis=0)\n",
    "maxs = np.max(audio_rectified_smooth, axis=0)\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "\n",
    "# Calculate the tempo of the sounds\n",
    "tempos = []\n",
    "for col, i_audio in audio.items():\n",
    "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# Convert the list to an array so you can manipulate it more easily\n",
    "tempos = np.array(tempos)\n",
    "\n",
    "# Calculate statistics of each tempo\n",
    "tempos_mean = tempos.mean(axis=-1)\n",
    "tempos_std = tempos.std(axis=-1)\n",
    "tempos_max = tempos.max(axis=-1)\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c85eff",
   "metadata": {},
   "source": [
    "#### The spectrogram- spectral changes to sound over time\n",
    "* The spectrogram is a special case of timeseries features\n",
    "* Spectrograms are common in timeseries analysis\n",
    "\n",
    "#### Fourier transforms\n",
    "* Timeseries data can be described as a combination of quickly-changing things and slowly-changing things\n",
    "* At each moment in time, we can describe the relative presence of fast- and slow-moving components\n",
    "* The simplest way to do this is called a *Fourier Transform*\n",
    "* This converts a single timeseries into an array that describes the timeseries as a combination of oscillations.\n",
    "* **Fourier Transform** or **FFT**\n",
    "\n",
    "#### Spectrograms: combinations of windows Fourier Transforms\n",
    "*  We can calculate multiple fourier transforms in a sliding window to see how it changes over time.\n",
    "* A **spectrogram** is a collection of windowed Fourier transforms over time\n",
    "* Similar to how a rolling mean was calculated:\n",
    "    * 1. Choose a window size and shape\n",
    "    * 2. At a timepoint, calculate the FFT for that window\n",
    "    * 3. Slide the window over by one\n",
    "    * 4. Aggregate the results\n",
    "* Called a **Short-Time Fourier Transform (STFT)**\n",
    "* Result: a description of the fourier transform as it changes through the time series\n",
    "* To calculate the spectrogram, we square each value of the STFT\n",
    "* Spectral content of the sound changes over time. With speech, we see interesting patterns that correspond to spoken words (for example, vowels or consonants)\n",
    "\n",
    "#### Calculating the STFT\n",
    "* We can calculate the STFT with `librosa`\n",
    "* There are a several parameters we can tweak (such as window size)\n",
    "* For our purposes, we'll convert into *decibels* which normalizes the average values of all frequencies\n",
    "* We can visualize it with the `specshow()` function (results in the visualized spectrogram)\n",
    "\n",
    "#### Calculating the STFT with code\n",
    "\n",
    "```\n",
    "# Import the functions we'll use for the STFT\n",
    "from librosa.core import stft, amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Calculate our STFT\n",
    "HOP_LENGTH = 2**4\n",
    "SIZE_WINDOW = 2**7\n",
    "audio_spec = stft(audio, hop_length= HOP_LENGTH, n_fft= SIZE_WINDOW)\n",
    "\n",
    "# Conver into decibels for visualization\n",
    "spec_db = amplitude_to_db(audio_spec)\n",
    "\n",
    "# Visualize\n",
    "specshow(spec_db, sr = sfreq, x_axis = 'time', y_axis= 'hz', hop_length= HOP_LENGTH)\n",
    "\n",
    "```\n",
    "\n",
    "#### Spectral feature engineering\n",
    "* Each timeseries has a different spectral pattern\n",
    "* We can calculate these spectral patterns by analyzing the spectrogram.\n",
    "* For example, **spectral bandwidth** and **spectral centroids** describe where most of the energy is at each moment in time.\n",
    "* This means we can use patterns in the spectrogram to distinguish classes from one another\n",
    "\n",
    "#### Calculating spectral features: spectral centroid and bandwidth \n",
    "\n",
    "```\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroids(S=spec)[0]\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "ax = specshow(spec, x_axis = 'time', y_axis='hz', hop_length= HOP_LENGTH)\n",
    "ax.plot(times_spec, centroids)\n",
    "ax.fill_between(times_spec, centroids - bandwidths / 2,\n",
    "                centroids + bandwidths / 2, alpha = 0.5)\n",
    "```\n",
    "\n",
    "#### Combining spectral and temporal features in a classifier\n",
    "\n",
    "```\n",
    "centroids_all = []\n",
    "bandwidths_all = []\n",
    "for spec in spectrograms:\n",
    "    bandwidths = lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
    "    centroids = lr.feature.spectral_centroids(S=lr.db_to_amplitude(spec))\n",
    "    # Calculate the mean spectral bandwidth\n",
    "    bandwidths_all.append(np.mean(bandwidths))\n",
    "    # Calculate the mean spectral centroid\n",
    "    centroids_all.append(np.mean(centroids))\n",
    "    \n",
    "# Create our X matrix\n",
    "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths_al, centroids_all)]\n",
    "```\n",
    "\n",
    "* In general, as we include more complex features into our model, we'll improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c06094",
   "metadata": {},
   "source": [
    "#### Spectrograms of heartbeat audio\n",
    "\n",
    "```\n",
    "# Import the stft function\n",
    "from librosa.core import stft\n",
    "\n",
    "# Prepare the STFT\n",
    "HOP_LENGTH = 2**4\n",
    "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert into decibels\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Compare the raw audio to the spectrogram of the audio\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "axs[0].plot(time, audio)\n",
    "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "plt.show()\n",
    "\n",
    "import librosa as lr\n",
    "\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert spectrogram to decibels for visualization\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "ax.plot(times_spec, centroids)\n",
    "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
    "ax.set(ylim=[None, 6000])\n",
    "plt.show()\n",
    "\n",
    "# Loop through each spectrogram\n",
    "bandwidths = []\n",
    "centroids = []\n",
    "\n",
    "for spec in spectrograms:\n",
    "    # Calculate the mean spectral bandwidth\n",
    "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
    "    # Calculate the mean spectral centroid\n",
    "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
    "    # Collect the values\n",
    "    bandwidths.append(this_mean_bandwidth)  \n",
    "    centroids.append(this_mean_centroid)\n",
    "    \n",
    "    # Create X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f9940",
   "metadata": {},
   "source": [
    "## III. Predicting Time Series Data\n",
    "If you want to predict patterns from data over time, there are special considerations to take in how you choose and construct your model. This chapter covers how to gain insights into the data before fitting your model, as well as best-practices in using predictive modeling for time series data.\n",
    "\n",
    "#### Predicting data over time\n",
    "* In this chapter, we shift our focus from classification to regresion.\n",
    "* Regression has several features and caveats that are unique to timeseries data.\n",
    "* **Regression** is similar to calculating correlation, with some key differences:\n",
    "    * **Regression:** A process that results in a formal model of the data\n",
    "    * **Correlation:** A statistic that describes the data. Less information that regression model.\n",
    "    \n",
    "* **Correlation between variables often changes over time\n",
    "    * Timeseries often have patterns that change over time\n",
    "    * Two timeseries that seem correlated at one moment may not remain so over time\n",
    "* To visualize how the data changes over time, you can either plot the whole timeseries at once, or directly compare two segments of a time. \n",
    "\n",
    "#### Visualizing relationships between timeseries: 2 ways\n",
    "\n",
    "```\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# Make a line plot for each timeseries\n",
    "axs[0].plot(x, c='k', lw=3, alpha=.2)\n",
    "axs[0].plot(y)\n",
    "axs[0].set(xlabel='time', title='X values = time')\n",
    "\n",
    "# Encode time as color in a scatterplot\n",
    "axs[1].scatter(x_long, y_long, c=np.arange(len(x_long)), cmap = 'viridis')\n",
    "axs[1].set(xlabel='x', ylabel='y', title = 'Color = 'time')\n",
    "```\n",
    "\n",
    "#### Regression models with sklearn\n",
    "\n",
    "```from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "model.predict(X)\n",
    "```\n",
    "\n",
    "#### Visualize predictions with sklearn\n",
    "```\n",
    "alphas = [.1, 1e2, 1e3]\n",
    "ax.plot(y_test, color='k', alpha=.3, lw=3)\n",
    "for ii, alpha in enumerate(alphas):\n",
    "    y_predicted = Ridge(alpha = alpha).fit(X_train, y_train).predict(X_test)\n",
    "    ax.plot(y_predicted, c=cmap(ii / len(alphas)))\n",
    "ax.legend(['True values', 'Model 1', 'Model 2', 'Model 3'])\n",
    "ax.set(xlabel='Time')\n",
    "```\n",
    "* Here (above), we visualize the predictions from sevreal different models fit on the same data\n",
    "* Ridge regression has parameter called \"alpha\" that causes coefficients to be smoother and smaller, and is useful if you have noisy or correlated variables.\n",
    "\n",
    "#### Scoring regression models\n",
    "* Visualizing is useful, but not quantifiable.\n",
    "* Two most common methods of scoring regression models:\n",
    "    * Correlation (*r*, or $\\rho$) -- simplest\n",
    "    * Coefficient of Determination ($R^2$) -- most common\n",
    "        * Value bounded on the top by 1, and can be infinitely low (since models can be infinitely bad)\n",
    "        * Values closer to 1 mean the model does a better job of predicting outputs\n",
    "\n",
    "```\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_predicted, y_test))\n",
    "```\n",
    "\n",
    "```\n",
    "# Scatterplot with color relating to time\n",
    "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
    "                    cmap=plt.cm.viridis, colorbar=False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use stock symbols to extract training data\n",
    "X = all_prices[['EBAY', 'NVDA', 'YHOO']]\n",
    "y = all_prices[['AAPL']]\n",
    "\n",
    "# Fit and score the model with cross-validation\n",
    "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
    "print(scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec9f5a",
   "metadata": {},
   "source": [
    "#### Cleaning and improving your data\n",
    "\n",
    "#### Interpolation: uing time to fill in missing data\n",
    "    * A common way to deal with missing data is to *interpolate* missing values\n",
    "    * With timeseries data, you can use time to assist in interpolation\n",
    "    * In this case, **interpolation** means using the *known* values on either side of a gap in the data to make assumptions about what's missing.\n",
    "    \n",
    "#### Interpolation in pandas\n",
    "\n",
    "```\n",
    "# Return a boolean that notes where missing values are\n",
    "missing = prices.isna()\n",
    "\n",
    "# Interpolate linearly within missing windows\n",
    "prices_interp = prices.interpolate('linear')\n",
    "\n",
    "# Plot the interpolated data in red and the data with missing values in black\n",
    "ax= price_interp.plot(c='r')\n",
    "prices.plot(c='k', ax=ax, lw=2)\n",
    "```\n",
    "* Above we used the `linear` method to interpolate, but other arguments (like `quadratic`, etc) will obviously result in different behavior.\n",
    "\n",
    "#### Using a rolling window to transform data\n",
    "* Another common use of rolling windows is to transform the data\n",
    "* We've already done this once, in order to *smooth* the data\n",
    "* However, we can also use this to do more complex transformations\n",
    "\n",
    "#### Transforming data to standardize variance\n",
    "* A common transformation to apply to data is to standardize its mean and variance over time. There are many ways to do this.\n",
    "* Here, we'll show how to convert your dataset so that each point represents the *% change over a previous window*.\n",
    "* This makes timepoints more comparable to one another if the absolute valus of data change a lot.\n",
    "* This standardizes our data and reduces long-term drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e0f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_change(values):\n",
    "    \"\"\"Calculates the % change between the last values and the mean of previous values\"\"\"\n",
    "    # Separate the last value and all previous values into variables\n",
    "    previous_values = values[:-1]\n",
    "    last_value = values[-1]\n",
    "    \n",
    "    # Calculate the % difference between the last value and the mean of earlier values\n",
    "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "    return percent_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503c88",
   "metadata": {},
   "source": [
    "* In the above function, we first separate out the final value of the input array \n",
    "* Then, we calculate the mean of all but the last data point\n",
    "* Finally, we subtract the mean from the final datapoint, and divide by the mean \n",
    "* The result is the percent change for the final value\n",
    "\n",
    "#### Applying this to the data\n",
    "\n",
    "```\n",
    "# Plot the raw data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax = prices.plot(ax=axs[0])\n",
    "\n",
    "# Calculate % change and plot\n",
    "ax = prices.rolling(window=20).aggregate(percent_change).plot(ax=axs[1])\n",
    "ax.legend_.set_visible(False)\n",
    "```\n",
    "* Apply the percent_change function to our data using the `.aggregate` method, passing out function as an input\n",
    "* The data will now be roughly centered at zero, and periods of high and low change are easier to spot.\n",
    "* Use this transformation to detect outliers\n",
    "\n",
    "#### Finding outliers in your data\n",
    "* Outliers are datapoints that are significantly statistically different from the dataset\n",
    "* They can have negative effects on the predictive power of your model, biasing it away from its \"true\" value\n",
    "* One solution is to *remove* or *replace* outliers with a more representative value\n",
    "    * *Be very careful* about doing this - often it is difficult to determine what is a legitimately extreme value vs an abberation\n",
    "* A common definition:\n",
    "    * **Outliers:** Any datapoint that is more than three standard deviations away from the mean of the dataset.\n",
    "    \n",
    "#### Plotting a threshold on our data\n",
    "\n",
    "```\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for data, ax in zip([prices, prices_perc_change], axs):\n",
    "    # Calculate the mean / standard deviation for the data\n",
    "    this_mean = data.mean()\n",
    "    this_std = data.std()\n",
    "    \n",
    "    # Plot the data, with a window that is 3 standard deviations around the mean\n",
    "    data.plot(ax = ax)\n",
    "    ax.axhline(this_mean + this_std * 3, ls= '--', c ='r')\n",
    "    ax.axhline(this_mean - this_std * 3, ls = '--', c = 'r')\n",
    "```\n",
    "\n",
    "* We calculate the mean and standard deviation of each dataset, then plot outlier \"thresholds\" (three times the standard deviation from the mean) on the raw and transformed data)\n",
    "* Note that the datapoints deemed an outlier depend on the transformation of the data.\n",
    "* Next, replace outliers with the median of the remaining values:\n",
    "\n",
    "```\n",
    "# Center the data so the mean is 0\n",
    "prices_outlier_centered = prices_outlier_perc - prices_outlier_perc.mean()\n",
    "\n",
    "# Calculate standard deviation\n",
    "std = prices_outlier_perc.std()\n",
    "\n",
    "# Use the absolute value of each datapoint to make it easier to find the outliers\n",
    "outliers = np.abs(prices_outlier_centered) > (std * 3)\n",
    "\n",
    "# Replace outliers with the median value\n",
    "# We'll use np.nanmean since there may be nans around the outliers\n",
    "prices_outlier_fixed = prices_outlier_centered.copy()\n",
    "prices_outlier_fixed[outliers] = np.nanmedian(prices_outlier_fixed)\n",
    "```\n",
    "#### Visualize the results\n",
    "\n",
    "```\n",
    "fig, axs = plt.subplots(1, 2, figsize= (10,5))\n",
    "prices_outlier_centered.plot(ax=axs[0])\n",
    "prices_outlier_fixed.plot(ax=axs[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bffa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_plot(prices, interpolation):\n",
    "    \"\"\"function we'll use to interpolate and plot\"\"\"\n",
    "    # Create a boolean mask for missing values\n",
    "    missing_values = prices.isna()\n",
    "\n",
    "    # Interpolate the missing values\n",
    "    prices_interp = prices.interpolate(interpolation)\n",
    "\n",
    "    # Plot the results, highlighting the interpolated values in black\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
    "    \n",
    "    # Now plot the interpolated values on top in red\n",
    "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5e301",
   "metadata": {},
   "source": [
    "```\n",
    "# Interpolate using the latest non-missing value\n",
    "interpolation_type = 'zero'\n",
    "interpolate_and_plot(prices, interpolation_type)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3287f",
   "metadata": {},
   "source": [
    "```\n",
    "# Your custom function\n",
    "def percent_change(series):\n",
    "    # Collect all *but* the last value of this window, then the final value\n",
    "    previous_values = series[:-1]\n",
    "    last_value = series[-1]\n",
    "\n",
    "    # Calculate the % difference between the last value and the mean of earlier values\n",
    "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "    return percent_change\n",
    "\n",
    "# Apply your custom function and plot\n",
    "prices_perc = prices.rolling(20).apply(percent_change)\n",
    "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e324ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers(series):\n",
    "    \"\"\"Define function to replace outliers with median of a series\"\"\"\n",
    "    # Calculate the absolute difference of each timepoint from the series mean\n",
    "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
    "    \n",
    "    # Calculate a mask for the differences that are > 3 standard deviations from zero\n",
    "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
    "    \n",
    "    # Replace these values with the median accross the data\n",
    "    series[this_mask] = np.nanmedian(series)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c47dcd",
   "metadata": {},
   "source": [
    "```\n",
    "# Apply your preprocessing function to the timeseries and plot the results\n",
    "prices_perc = prices_perc.apply(replace_outliers)\n",
    "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
