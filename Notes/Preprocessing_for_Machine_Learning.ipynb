{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d05d1e",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc1717",
   "metadata": {},
   "source": [
    "### I. Introduction to preprocessing\n",
    "\n",
    "#### Preprocessing data for machine learning\n",
    "* What is data preprocessing?\n",
    "    * Beyond cleaning and exploratory data analysis\n",
    "    * Prepping data for modeling\n",
    "    * Modeling in Python requires numerical input\n",
    "        * So if your dataset has categorical variables, you'll need to transform them.\n",
    "    * Data preprocessing is a prerequisite to modeling.\n",
    "\n",
    "* One of the first steps you can take to preprocess your data is to remove missing data\n",
    "* Drop specific rows by passing index labels to the drop function, which defaults to dropping rows \n",
    "* Usually you'll want to focus on dropping a particular column, especially if all or most of its values are missing\n",
    "    * `df.drop(\"A\", axis=1)`\n",
    "\n",
    "* drop rows where data is missing in a particular column:\n",
    "    * do this with the help of boolean indexing, which is a way to filter a dataframe based on certain values\n",
    "    * `df[df[\"B\"] == 7])`\n",
    "    \n",
    "```\n",
    "df[\"B\"].isnull().sum())\n",
    "df[df[\"B\"].notnull()])\n",
    "```\n",
    "\n",
    "* To remove rows with **at least 3 missing values**:\n",
    "    * `volunteer2 = volunteer.dropna(axis=1, thresh=3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673214f",
   "metadata": {},
   "source": [
    "#### Working with Data Types\n",
    "* Why are types important?\n",
    "    * Recall that you can check the types of a dataframe by using the `.dtypes` attribute\n",
    "* Pandas datatypes are similar to native python types, but there are a couple of things to be aware of.\n",
    "    * The `object` type is what pandas uses to refer to a column that consists of string values or is of mixed types.\n",
    "* Converting column types:\n",
    "    * `df.dtypes`\n",
    "    * `df[\"C\"] = df[\"C\"].astype(\"float\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53816811",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "* We split our data into training and test sets to avoid the issue of overfitting\n",
    "* Holding out a test set allows us to preserve some data the model hasn't seen yet. \n",
    "* In many scenarios, the default splitting parameters of `train_test_split()` will work well. However, if your labels have an uneven distribution, your test and training sets might not be representative samples of your dataset and could bias the model you're trying to train.\n",
    "* A good technique for sampling more accurately when you have imbalanced classes is **stratified sampling.**\n",
    "* **Stratified sampling** is a way of sampling that takes into account the distribution of classes or features in your dataset\n",
    "* We want the distribution of our training and testing samples to be on par with the distribution of the classes in the original dataset\n",
    "\n",
    "#### Stratified sampling\n",
    "* There's a really easy way to stratify samples of *classification* variables in train_test_split function:\n",
    "    * `X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb6550",
   "metadata": {},
   "source": [
    "### II. Standardizing Data\n",
    "* You may come across datasets with lots of numerical noise built in, such as lots of variance or differently-scaled data\n",
    "    * The preprocessing solution for that is standardization\n",
    "* **Standardization** is a preprocessing method used to transform continuous data to make it look normally distributed\n",
    "* In sklearn, this is often a necessary step, because many models assume that the data you are training on is normally distributed, and if it isn't, you risk biasing your model \n",
    "* **sklearn models assume normally distributed data.**\n",
    "* There are different ways to standardize data; in this course we'll focus on **log normalization** and **scaling.**\n",
    "* Standardization is a preprocessing method applied to **continuous, numerical data.**\n",
    "\n",
    "* **When to standardize**\n",
    "* Model in linear space\n",
    "    * K-nearest neighbors\n",
    "    * Linear regression\n",
    "    * k-means clustering\n",
    "    * etc\n",
    "* Dataset with features that have high variance\n",
    "    * If a feature in your dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset.\n",
    "    * Dataset with features that are continuous and on different scales\n",
    "    * Linearity assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cf90b",
   "metadata": {},
   "source": [
    "#### Log normalization\n",
    "* **Log normalization** is a method for standardizing your data that can be useful when you have a particular column with high variance\n",
    "* **Log normalization** applies a log transformation to your values, which transforms your values onto a scale that approximates normality (an assumption about your data that a lot of models make).\n",
    "* **Log normalization:**\n",
    "    * applies log transformation\n",
    "    * Natural log using the constant _e_(2.718)\n",
    "    * Captures relative changes, the magnitude of change, and keeps everything in the positive space.\n",
    "* It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "\n",
    "#### Log normalization in Python\n",
    "* Fairly straightforward\n",
    "* Use log function from numpy\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "df['log_2'] = np.log(df['col2'])\n",
    "print(df)\n",
    "```\n",
    "* Pass the column we want to log normalize directly into the function.\n",
    "\n",
    "```\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb33ff",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "* **Scaling** is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors).\n",
    "* **Feature scaling:**\n",
    "    * Features on different scales\n",
    "    * Model with linear characteristics\n",
    "    * transforms the features in your dataset so they have a mean of zero and a variance of one\n",
    "    * Transforms to approximately normal distribution\n",
    "    * This will make it easier to linearly compare features\n",
    "    \n",
    "* **StandardScaler():**\n",
    "    * This method works by removing the mean and scaling each feature to have unit variance\n",
    "    \n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), \n",
    "                         columns = df.columns)\n",
    "```\n",
    "* There's a simpler `scale` function in sklearn, but the benefit of using the `StandardScaler()` object is that you can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything.\n",
    "\n",
    "```\n",
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)\n",
    "```\n",
    "\n",
    "#### Standardized data and modeling\n",
    "* Many models in scikit-learn require your data to be scaled appropriately across columns, otherwise you risk biasing your results\n",
    "* This unit will be dedicated to modeling data on both unscaled and scaled data, so you can see the difference in model performance.\n",
    "* Recap: KNearestNeighors: a model that classifies data based on its distance to the training set data\n",
    "    * A new data point is assigned a label based on the class that the majority of surrounding data points belong to.\n",
    "* **K-Nearest Neighbors:**\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Preprocessing first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "* **KNN on un-scaled data:\n",
    "\n",
    "```\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "```\n",
    "* Output: `0.64444`\n",
    "\n",
    "```\n",
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "print(knn.score(X_test, y_test))\n",
    "```\n",
    "* Output: `0.95556`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017be9aa",
   "metadata": {},
   "source": [
    "#### Medium article: How data normalization affects your Random Forest algorithm\n",
    "* **Data normalization won’t affect the output for Random Forest *classifiers* while it will affect the output for Random Forest *regressors*.**\n",
    "* Regarding the regressor, the algorithm will be more affected by the high-end values if the data is not transformed. This means that they will probably be more accurate in predicting high values than low values.\n",
    "* Consequently, transformations such as log-transform will reduce the relative importance of these high values, hence generalizing better.\n",
    "* **MinMax scaler generally performs better than StandardScaler for RF Regression models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0124978",
   "metadata": {},
   "source": [
    "### III. Feature Engineering\n",
    "\n",
    "#### Feature Engineering \n",
    "* A very important part of the preprocessing workflow: feature engineering\n",
    "* **Feature engineering** is the creation of new features based on existing features, and it adds information to your dataset that is useful in some way.\n",
    "     * It adds features that are usefull for your prediction or clustering task, or;\n",
    "     * It sheds light or insight into relationships between features\n",
    "     * Extract and expand data\n",
    "     * Feature engineering is also something that is very dependent on the particular dataset you're analyzing.\n",
    "\n",
    "* Real world data is often not neat and tidy, and in addition to preprocessing steps like standardization, you'll likely have to extract and expand information that exists in the columns in your dataset.\n",
    "\n",
    "#### Encoding Categorical Variables\n",
    "* Because models in sklearn require numerical input, if your dataset contains categorical variables, you'll have to encode them \n",
    "\n",
    "* **Encoding binary variables:**\n",
    "* Can be done in both pandas and sklearn.\n",
    "* In **pandas**, we can use the apply function to encode 1s and 0s in a dataframe column\n",
    "* If we have a column of the df `users`, called `subscribed`, containing a list of binary values (`y` or `n`):\n",
    "    * `users['sub_enc'] = users['unsubscribed'].apply(lambda val: 1 if val == 'y' else 0)`\n",
    "* You may want to encode binary variables if you're not finished preprocessing, or if oyu're interested in further exploratory work once you've encoded your categorical variables.\n",
    "\n",
    "* You can also do this in `sklearn` using **`LabelEncoder`**.\n",
    "* It's useful to know both methods if, for example, you're implementing encoding as part of sklearn's pipeline functionality.\n",
    "* Creating a `LabelEncoder` object also allows you yo reuse this encoding on other data, such as on new data or a test set.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "users['sub_enc_le'] = le.fit_transform(users['subscribed'])\n",
    "```\n",
    "\n",
    "* Use the `get_dummies` function in pandas to directly encode categorical values \n",
    "* `print(pd.get_dummies(users['fav_color']))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3de38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6b867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
