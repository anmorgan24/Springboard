{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a39866d",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b0b7c",
   "metadata": {},
   "source": [
    "#### Preprocessing data for machine learning\n",
    "* What is data preprocessing?\n",
    "    * Beyond cleaning and exploratory data analysis\n",
    "    * Prepping data for modeling\n",
    "    * Modeling in Python requires numerical input\n",
    "        * So if your dataset has categorical variables, you'll need to transform them.\n",
    "    * Data preprocessing is a prerequisite to modeling.\n",
    "\n",
    "* One of the first steps you can take to preprocess your data is to remove missing data\n",
    "* Drop specific rows by passing index labels to the drop function, which defaults to dropping rows \n",
    "* Usually you'll want to focus on dropping a particular column, especially if all or most of its values are missing\n",
    "    * `df.drop(\"A\", axis=1)`\n",
    "\n",
    "* drop rows where data is missing in a particular column:\n",
    "    * do this with the help of boolean indexing, which is a way to filter a dataframe based on certain values\n",
    "    * `df[df[\"B\"] == 7])`\n",
    "    \n",
    "```\n",
    "df[\"B\"].isnull().sum())\n",
    "df[df[\"B\"].notnull()])\n",
    "```\n",
    "\n",
    "* To remove rows with **at least 3 missing values**:\n",
    "    * `volunteer2 = volunteer.dropna(axis=1, thresh=3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa5452",
   "metadata": {},
   "source": [
    "#### Working with Data Types\n",
    "* Why are types important?\n",
    "    * Recall that you can check the types of a dataframe by using the `.dtypes` attribute\n",
    "* Pandas datatypes are similar to native python types, but there are a couple of things to be aware of.\n",
    "    * The `object` type is what pandas uses to refer to a column that consists of string values or is of mixed types.\n",
    "* Converting column types:\n",
    "    * `df.dtypes`\n",
    "    * `df[\"C\"] = df[\"C\"].astype(\"float\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befb101",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "* We split our data into training and test sets to avoid the issue of overfitting\n",
    "* Holding out a test set allows us to preserve some data the model hasn't seen yet. \n",
    "* In many scenarios, the default splitting parameters of `train_test_split()` will work well. However, if your labels have an uneven distribution, your test and training sets might not be representative samples of your dataset and could bias the model you're trying to train.\n",
    "* A good technique for sampling more accurately when you have imbalanced classes is **stratified sampling.**\n",
    "* **Stratified sampling** is a way of sampling that takes into account the distribution of classes or features in your dataset\n",
    "* We want the distribution of our training and testing samples to be on par with the distribution of the classes in the original dataset\n",
    "\n",
    "#### Stratified sampling\n",
    "* There's a really easy way to stratify samples of *classification* variables in train_test_split function:\n",
    "    * `X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e2351",
   "metadata": {},
   "source": [
    "#### Standardizing Data\n",
    "* You may come across datasets with lots of numerical noise built in, such as lots of variance or differently-scaled data\n",
    "    * The preprocessing solution for that is standardization\n",
    "* **Standardization** is a preprocessing method used to transform continuous data to make it look normally distributed\n",
    "* In sklearn, this is often a necessary step, because many models assume that the data you are training on is normally distributed, and if it isn't, you risk biasing your model \n",
    "* **sklearn models assume normally distributed data.**\n",
    "* There are different ways to standardize data; in this course we'll focus on **log normalization** and **scaling.**\n",
    "* Standardization is a preprocessing method applied to **continuous, numerical data.**\n",
    "\n",
    "* **When to standardize**\n",
    "* Model in linear space\n",
    "    * K-nearest neighbors\n",
    "    * Linear regression\n",
    "    * k-means clustering\n",
    "    * etc\n",
    "* Dataset with features that have high variance\n",
    "    * If a feature in your dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset.\n",
    "    * Dataset with features that are continuous and on different scales\n",
    "    * Linearity assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3e4c2",
   "metadata": {},
   "source": [
    "#### Log normalization\n",
    "* **Log normalization** is a method for standardizing your data that can be useful when you have a particular column with high variance\n",
    "* **Log normalization** applies a log transformation to your values, which transforms your values onto a scale that approximates normality (an assumption about your data that a lot of models make).\n",
    "* **Log normalization:**\n",
    "    * applies log transformation\n",
    "    * Natural log using the constant _e_(2.718)\n",
    "    * Captures relative changes, the magnitude of change, and keeps everything in the positive space.\n",
    "* It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "\n",
    "#### Log normalization in Python\n",
    "* Fairly straightforward\n",
    "* Use log function from numpy\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "df['log_2'] = np.log(df['col2'])\n",
    "print(df)\n",
    "```\n",
    "* Pass the column we want to log normalize directly into the function.\n",
    "\n",
    "```\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833870d",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "* **Scaling** is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors).\n",
    "* **Feature scaling:**\n",
    "    * Features on different scales\n",
    "    * Model with linear characteristics\n",
    "    * transforms the features in your dataset so they have a mean of zero and a variance of one\n",
    "    * Transforms to approximately normal distribution\n",
    "    * This will make it easier to linearly compare features\n",
    "    \n",
    "* **StandardScaler():**\n",
    "    * This method works by removing the mean and scaling each feature to have unit variance\n",
    "    \n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), \n",
    "                         columns = df.columns)\n",
    "```\n",
    "* There's a simpler `scale` function in sklearn, but the benefit of using the `StandardScaler()` object is that you can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything.\n",
    "\n",
    "```\n",
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4cc22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f70fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370023c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05060c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
