{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9149f6cc",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5705",
   "metadata": {},
   "source": [
    "### I. Introduction to preprocessing\n",
    "\n",
    "#### Preprocessing data for machine learning\n",
    "* What is data preprocessing?\n",
    "    * Beyond cleaning and exploratory data analysis\n",
    "    * Prepping data for modeling\n",
    "    * Modeling in Python requires numerical input\n",
    "        * So if your dataset has categorical variables, you'll need to transform them.\n",
    "    * Data preprocessing is a prerequisite to modeling.\n",
    "\n",
    "* One of the first steps you can take to preprocess your data is to remove missing data\n",
    "* Drop specific rows by passing index labels to the drop function, which defaults to dropping rows \n",
    "* Usually you'll want to focus on dropping a particular column, especially if all or most of its values are missing\n",
    "    * `df.drop(\"A\", axis=1)`\n",
    "\n",
    "* drop rows where data is missing in a particular column:\n",
    "    * do this with the help of boolean indexing, which is a way to filter a dataframe based on certain values\n",
    "    * `df[df[\"B\"] == 7])`\n",
    "    \n",
    "```\n",
    "df[\"B\"].isnull().sum())\n",
    "df[df[\"B\"].notnull()])\n",
    "```\n",
    "\n",
    "* To remove rows with **at least 3 missing values**:\n",
    "    * `volunteer2 = volunteer.dropna(axis=1, thresh=3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1547292",
   "metadata": {},
   "source": [
    "#### Working with Data Types\n",
    "* Why are types important?\n",
    "    * Recall that you can check the types of a dataframe by using the `.dtypes` attribute\n",
    "* Pandas datatypes are similar to native python types, but there are a couple of things to be aware of.\n",
    "    * The `object` type is what pandas uses to refer to a column that consists of string values or is of mixed types.\n",
    "* Converting column types:\n",
    "    * `df.dtypes`\n",
    "    * `df[\"C\"] = df[\"C\"].astype(\"float\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a80c2",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "* We split our data into training and test sets to avoid the issue of overfitting\n",
    "* Holding out a test set allows us to preserve some data the model hasn't seen yet. \n",
    "* In many scenarios, the default splitting parameters of `train_test_split()` will work well. However, if your labels have an uneven distribution, your test and training sets might not be representative samples of your dataset and could bias the model you're trying to train.\n",
    "* A good technique for sampling more accurately when you have imbalanced classes is **stratified sampling.**\n",
    "* **Stratified sampling** is a way of sampling that takes into account the distribution of classes or features in your dataset\n",
    "* We want the distribution of our training and testing samples to be on par with the distribution of the classes in the original dataset\n",
    "\n",
    "#### Stratified sampling\n",
    "* There's a really easy way to stratify samples of *classification* variables in train_test_split function:\n",
    "    * `X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ff3b4",
   "metadata": {},
   "source": [
    "### II. Standardizing Data\n",
    "* You may come across datasets with lots of numerical noise built in, such as lots of variance or differently-scaled data\n",
    "    * The preprocessing solution for that is standardization\n",
    "* **Standardization** is a preprocessing method used to transform continuous data to make it look normally distributed\n",
    "* In sklearn, this is often a necessary step, because many models assume that the data you are training on is normally distributed, and if it isn't, you risk biasing your model \n",
    "* **sklearn models assume normally distributed data.**\n",
    "* There are different ways to standardize data; in this course we'll focus on **log normalization** and **scaling.**\n",
    "* Standardization is a preprocessing method applied to **continuous, numerical data.**\n",
    "\n",
    "* **When to standardize**\n",
    "* Model in linear space\n",
    "    * K-nearest neighbors\n",
    "    * Linear regression\n",
    "    * k-means clustering\n",
    "    * etc\n",
    "* Dataset with features that have high variance\n",
    "    * If a feature in your dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset.\n",
    "    * Dataset with features that are continuous and on different scales\n",
    "    * Linearity assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1824e",
   "metadata": {},
   "source": [
    "#### Log normalization\n",
    "* **Log normalization** is a method for standardizing your data that can be useful when you have a particular column with high variance\n",
    "* **Log normalization** applies a log transformation to your values, which transforms your values onto a scale that approximates normality (an assumption about your data that a lot of models make).\n",
    "* **Log normalization:**\n",
    "    * applies log transformation\n",
    "    * Natural log using the constant _e_(2.718)\n",
    "    * Captures relative changes, the magnitude of change, and keeps everything in the positive space.\n",
    "* It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "\n",
    "#### Log normalization in Python\n",
    "* Fairly straightforward\n",
    "* Use log function from numpy\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "df['log_2'] = np.log(df['col2'])\n",
    "print(df)\n",
    "```\n",
    "* Pass the column we want to log normalize directly into the function.\n",
    "\n",
    "```\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447cf97",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "* **Scaling** is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors).\n",
    "* **Feature scaling:**\n",
    "    * Features on different scales\n",
    "    * Model with linear characteristics\n",
    "    * transforms the features in your dataset so they have a mean of zero and a variance of one\n",
    "    * Transforms to approximately normal distribution\n",
    "    * This will make it easier to linearly compare features\n",
    "    \n",
    "* **StandardScaler():**\n",
    "    * This method works by removing the mean and scaling each feature to have unit variance\n",
    "    \n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), \n",
    "                         columns = df.columns)\n",
    "```\n",
    "* There's a simpler `scale` function in sklearn, but the benefit of using the `StandardScaler()` object is that you can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything.\n",
    "\n",
    "```\n",
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)\n",
    "```\n",
    "\n",
    "#### Standardized data and modeling\n",
    "* Many models in scikit-learn require your data to be scaled appropriately across columns, otherwise you risk biasing your results\n",
    "* This unit will be dedicated to modeling data on both unscaled and scaled data, so you can see the difference in model performance.\n",
    "* Recap: KNearestNeighors: a model that classifies data based on its distance to the training set data\n",
    "    * A new data point is assigned a label based on the class that the majority of surrounding data points belong to.\n",
    "* **K-Nearest Neighbors:**\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Preprocessing first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "* **KNN on un-scaled data:\n",
    "\n",
    "```\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "```\n",
    "* Output: `0.64444`\n",
    "\n",
    "```\n",
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "print(knn.score(X_test, y_test))\n",
    "```\n",
    "* Output: `0.95556`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638c003",
   "metadata": {},
   "source": [
    "#### Medium article: How data normalization affects your Random Forest algorithm\n",
    "* **Data normalization won’t affect the output for Random Forest *classifiers* while it will affect the output for Random Forest *regressors*.**\n",
    "* Regarding the regressor, the algorithm will be more affected by the high-end values if the data is not transformed. This means that they will probably be more accurate in predicting high values than low values.\n",
    "* Consequently, transformations such as log-transform will reduce the relative importance of these high values, hence generalizing better.\n",
    "* **MinMax scaler generally performs better than StandardScaler for RF Regression models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99a615",
   "metadata": {},
   "source": [
    "### III. Feature Engineering\n",
    "\n",
    "#### Feature Engineering \n",
    "* A very important part of the preprocessing workflow: feature engineering\n",
    "* **Feature engineering** is the creation of new features based on existing features, and it adds information to your dataset that is useful in some way.\n",
    "     * It adds features that are usefull for your prediction or clustering task, or;\n",
    "     * It sheds light or insight into relationships between features\n",
    "     * Extract and expand data\n",
    "     * Feature engineering is also something that is very dependent on the particular dataset you're analyzing.\n",
    "\n",
    "* Real world data is often not neat and tidy, and in addition to preprocessing steps like standardization, you'll likely have to extract and expand information that exists in the columns in your dataset.\n",
    "\n",
    "#### Encoding Categorical Variables\n",
    "* Because models in sklearn require numerical input, if your dataset contains categorical variables, you'll have to encode them \n",
    "\n",
    "* **Encoding binary variables:**\n",
    "* Can be done in both pandas and sklearn.\n",
    "* In **pandas**, we can use the apply function to encode 1s and 0s in a dataframe column\n",
    "* If we have a column of the df `users`, called `subscribed`, containing a list of binary values (`y` or `n`):\n",
    "    * `users['sub_enc'] = users['unsubscribed'].apply(lambda val: 1 if val == 'y' else 0)`\n",
    "* You may want to encode binary variables if you're not finished preprocessing, or if oyu're interested in further exploratory work once you've encoded your categorical variables.\n",
    "\n",
    "* You can also do this in `sklearn` using **`LabelEncoder`**.\n",
    "* It's useful to know both methods if, for example, you're implementing encoding as part of sklearn's pipeline functionality.\n",
    "* Creating a `LabelEncoder` object also allows you yo reuse this encoding on other data, such as on new data or a test set.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "users['sub_enc_le'] = le.fit_transform(users['subscribed'])\n",
    "```\n",
    "\n",
    "* Use the `get_dummies` function in pandas to directly encode categorical values \n",
    "* `print(pd.get_dummies(users['fav_color']))`\n",
    "\n",
    "#### LabelEncoder()\n",
    "\n",
    "```\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())\n",
    "```\n",
    "\n",
    "#### pd.get_dummies()\n",
    "\n",
    "```\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc58188",
   "metadata": {},
   "source": [
    "#### Engineering numerical features \n",
    "* **Aggregate statistics** and **dates** and **how engineering numerical features can add value to your dataset.**\n",
    "* If you had, say, a collection of features related to a single feature, like temperature or running time, you might want to take an average or median to use as a feature for modeling instead\n",
    "* A common method of feature engineering is to take an aggregate of a set of numbers to use in place of those features\n",
    "* This can be helpful in reducing the dimensionality of your feature space\n",
    "\n",
    "* For example: you have a df with four cities along columns and 3 days' worth of temperatures as rows\n",
    "\n",
    "```\n",
    "columns = ['day1', 'day2', 'day3']\n",
    "df['mean'] = df.apply(lambda row: row[columns].mean(), axis=1)\n",
    "```\n",
    "* We set `axis = 1` in order to operate across a row\n",
    "* The above code adds a column (\"mean\") of the mean of the three days' worth of temperatures per city\n",
    "\n",
    "* **Dates:**\n",
    "* Dates and timestamps are another area where you may want to reduce granularity in your dataset\n",
    "    * If you're doing timeseries analysis, that's likely a different story, but if you're running a prediction task, you may need higher-level information like the month or the year, or both\n",
    "* Extract the month from the date (you can also use attributes like `day` and `year`\n",
    "\n",
    "```\n",
    "df['date_converted'] = pd.to_datetime(df['date'])\n",
    "df['month'] = df['date_converted'].apply(lambda row: row.month)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create a list of the columns to average\n",
    "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
    "\n",
    "# Use apply to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k)\n",
    "```\n",
    "\n",
    "***\n",
    "```\n",
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[['start_date_converted', 'start_date_month']].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff7cf3",
   "metadata": {},
   "source": [
    "#### Engineering features from text\n",
    "* Though text data is a little more complicated to work with, there's a lot of useful feature engineering you can do with it.\n",
    "* One method is to extract the pieces of information that you need and transforming it into a feature\n",
    "* Here we learn how to extract data from text fields\n",
    "* Extract from strings using **regular expression**, or **regex**\n",
    "* **Regular expressions are patterns that can be used to extract patterns from text data.**\n",
    "* More on regex in **Cleaning data in Python**\n",
    "* Below we have a string, and we want to extract the temperature digits from it (note that it is a float)\n",
    "\n",
    "```\n",
    "import re\n",
    "my_string = \"temperature:75.6 F\"\n",
    "pattern = re.compile(\"\\d+\\.\\d+\")\n",
    "temp = re.match(pattern, my_string)\n",
    "print(float(temp.group(0))\n",
    "```\n",
    "\n",
    "* `\\d` means that we want to grab digits\n",
    "* `+` means that we want to grab as many as possible\n",
    "* `\\.` means that we want to grab the decimal point \n",
    "* `\\d+` again to grab the digit(s) on the right-hand-side of the decimal\n",
    "\n",
    "* We then search the string for a matching pattern using `re.match`\n",
    "* extract it using `group()`\n",
    "\n",
    "\n",
    "#### Vectorizing text\n",
    "* If you're working with text, you may want to model it in some way \n",
    "* Maybe you want to use document text for classification: in order to do that we'll need to vectorize the text and transform it into a numerical input that sklearn can use. \n",
    "* `tf/idf` is a way of vectorizing text that reflects how important a word is in a document beyond how frequently it occurs.\n",
    "* `tf/idf` stands for **Term frequency inverse document frequency** and places the weight on words that are ultimately more significant in the entire corpus of words\n",
    "\n",
    "```\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(documents)\n",
    "```\n",
    "* Naive Bayes classifier works well on text classification tasks\n",
    "* Naive Bayes treats each feature as independent from the others (which can be a naive assumption, but this works out well on text data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e887e6",
   "metadata": {},
   "source": [
    "```\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa9d36",
   "metadata": {},
   "source": [
    "```# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a00a7b",
   "metadata": {},
   "source": [
    "### III. Selecting features for modeling\n",
    "#### Feature selection\n",
    "* **Feature selection** is a method of selecting features from your feature set to be used for modeling\n",
    "    * Draws on a set of existing features; doesn't create new features\n",
    "    * The goal of feature selection is to improve your model's performance\n",
    "    * Feature selection can be automated or manual, or a combination of both\n",
    "    \n",
    "* Several sklearn methods of automated feature selection\n",
    "    * choosing a variance threshold and using univariate statistical tests (as an example)\n",
    "    \n",
    "#### Removing redundant features \n",
    "* Feature selection's main goal is to remove unnecessary features from your dataset that might create noise when modeling\n",
    "* One of the easiest ways to determine if a feature is unnecessary is if it is redundat in some way \n",
    "* **Redundant features:**\n",
    "    * Remove noisy features\n",
    "    * Remove correlated features\n",
    "    * Remove duplicated features\n",
    "    \n",
    "* Some redundant features can be identified manually, by simply having an understanding of the features in your dataset\n",
    "* Feature selection is an iterative process\n",
    "    * You may have to continually reassess your feature selection choices- and that's okay!\n",
    "* **Scenarios for manual removal:**\n",
    "    * If your dataset contains repeated information in its feature set\n",
    "    * When extracting numbers (or text) from text, it may be unlikely that we'd need to keep the original text feature\n",
    "    * When generating an aggregate statistic, it's likely that we could drop the values that generated the aggregate statistic\n",
    "    * When two or more features are **statistically correlated**, meaning they move together directionally.\n",
    "        * Linear models in particular assume that features are independent of each other; if features are strongly correlated, that could introduce bias into your model\n",
    "        * Use Pearson's correlation coefficient to check a feature set for correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333b934",
   "metadata": {},
   "source": [
    "#### Selecting features using text vectors\n",
    "* You could potentially take something like the top 20% of weighted words across the vector, rather than including all words in the vectorizer\n",
    "* This would be a scenario where iteration would be important and it might be helpful to test out different subsets of your tf-idf vector to see what works\n",
    "* Let's look at how to pull out the words and their weights on a per document basis\n",
    "* It isn't especially straightforward to do this in sklearn, but it's very useful\n",
    "* The row data from the vector contains two components we'll need: the word weights and the index of the word. \n",
    "\n",
    "```\n",
    "print(tfidf_vec.vocabulary_)\n",
    "print(text_tfidf[3].data)\n",
    "print(text_tfidf[3].indices)\n",
    "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "zipped_row = dict(zip(text_tfidf[3].indices, text_tfidf[3].data))\n",
    "print(zipped_row)\n",
    "```\n",
    "\n",
    "* After you've vectorized your text, the vocabulary and weights will be stored in the vectorizer\n",
    "* To pull out the vocabulary list, which you'll need in order to look at word weights, you can use the `.vocabulary_` attribute\n",
    "* To take a look at the weight of the fourth row, for example, we use the data attribute on a specific row, accessed like you would access items in a list\n",
    "* To get the indices of the words that have been weighted, we use the indices attribute.\n",
    "* Before putting together the vocabulary, the word indices, and their weights, we want to reverse the key value pairs in the vocabulary \n",
    "* It will be easire later on if we have the index number in the key position in the dictionary\n",
    "* To reverse the vocabulary dictionary, you can swap the key-value pairs by grabbing the items from the vocabulary dictionary and reversing the order\n",
    "* Finally, we can also zip together the row indices and weights, pass it into the dict function, and turn that into a dictionary\n",
    "* Below we turn this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5304e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_weights(vocab, vector, vector_index):\n",
    "    zipped = dict(zip(vector[vector_index].indices,\n",
    "                     vector[vector_index].data))\n",
    "    return {vocab[i]:zipped[i] for i in vector[vector_index].indices}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f73bf8",
   "metadata": {},
   "source": [
    "* For example use:\n",
    "* `print(return_weights(vocab, text_tfidf, 3))`\n",
    "* At this point you could sort by score, or eliminate words below a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7028b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
