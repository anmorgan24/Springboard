{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029c65ec",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a4cc7",
   "metadata": {},
   "source": [
    "## Classification with XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2528a9f",
   "metadata": {},
   "source": [
    "* A classification problem involves predicting the category a given data point belongs to out of a finite set of possible categories. Depending on how many possible categories there are to predict, a classification problem can be either binary or multi-class. \n",
    "* XG Boost is an ensemble learning method\n",
    "* XG Boost: \"The hottest library in supervised ML\"\n",
    "* XG Boost originally written in C++, but has API's in several other languages:\n",
    "    * Python\n",
    "    * R\n",
    "    * Scala\n",
    "    * Julia\n",
    "    * Java\n",
    "* What makes XG Boost so popular?\n",
    "    * It's speed and performance \n",
    "    * Core algorithm is parallelizable \n",
    "    * Also parallelizable to GPUs and networks of computers\n",
    "    * Consistently outperforms single-algorithm models\n",
    "    * Achieves state of the art performance in many ML tasks \n",
    "* Remember: You always build an ML model using train_test_split\n",
    "* You can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\n",
    "* A typical setup for a churn prediction problem: Use the first month's worth of data to predict whether an app's users will remain users of the service at the 5 month mark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179f623",
   "metadata": {},
   "source": [
    "```\n",
    "#Import xgboost\n",
    "import xgboost as xgb\n",
    "#Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "#Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "#Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "#Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "#Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "#Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31d995",
   "metadata": {},
   "source": [
    "### Decision trees as \"base learners\"\n",
    "* __Base learner:__ any individual learning algorithm in an ensemble algorithm\n",
    "* Composed of a series of binary questions\n",
    "* Predictions happen at the \"leaves\" of the trees. \n",
    "### Decisions trees and CART\n",
    "* Decision trees are constructed iteratively (one decision at a time) until some stopping criterion is met (the depth reaches some pre-defined maximum value \n",
    "* Individual decision trees tend to overfit\n",
    "    * Low bias\n",
    "    * High variance\n",
    "* XG Boost uses a slightly different kind of a decision tree called a\n",
    "    * __CART:__ Classification And Regression Trees:\n",
    "        * Whereas the decision tree leaves always contained decision values;\n",
    "        * CART leaves __always__ contain a real-valued score (regardless of whether they are used for classification or regression). \n",
    "        * The real-valued scores can then be thresholded to convert into categories for classifiction problems if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de06bf6",
   "metadata": {},
   "source": [
    "```\n",
    "#Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "#Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth= 4)\n",
    "#Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "#Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "#Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54878a07",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "* At bottom, boosting isn't a specific ML algorithmm but rather a concept that can be applied to a set of ML models\n",
    "    * \"Meta-algorithm\"\n",
    "* Ensemble meta-algorithm used to convert many weak learners into a strong learner. \n",
    "* __Weak learner:__ ML algorithm that is slightly better than chance\n",
    "    * Example: Decistion tree whose predictions are slightly better than 50%\n",
    "* Boosting converts a collection of weak learners into a strong learner\n",
    "* __Strong learner:__ Any algorithm that can be tuned to achieve good performance\n",
    "* How Boosting is accomplished:\n",
    "    * By iteratively learning a set of weak models on subsets of the data\n",
    "    * Weighing each weak prediction according to each weak learner's performance.\n",
    "    * Combine the weighted predictions to form a single weighted prediction\n",
    "* __Model evaluation using cross-validation with XG Boost's Learning API:__\n",
    "    * (different from the sklearn compatible API we used in the first DC examples above.\n",
    "    * Cross-validation capabilities baked in: generates many non-overlapping train/test splits on training data, reports the average test set performance across all data splits\n",
    "    * needs to be converted to DMatrix\n",
    "    * for previous .fit, .predict etc: data was automatically transformed into required DMatrix\n",
    "    * For the following steps, data MUST be explicitly transformed to DMatrix\n",
    "* XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a __DMatrix__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f0dda",
   "metadata": {},
   "source": [
    "***\n",
    "```\n",
    "#Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "#Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1], label=churn_data.month_5_still_here)\n",
    "#Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "#Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"error\", as_pandas=True, seed=123)\n",
    "#Print cv_results\n",
    "print(cv_results)\n",
    "#Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1224025",
   "metadata": {},
   "source": [
    "***\n",
    "```\n",
    "#Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "#Print cv_results\n",
    "print(cv_results)\n",
    "#Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffe03d",
   "metadata": {},
   "source": [
    "When to use XG Boost: \\\n",
    "For any supervised machine learning task that fits the following criteria: \\\n",
    "* You have a large number of training samples and few fetures\n",
    "    * Greater than 1,000 training samples and less 100 features \n",
    "    * As long as the number of features < the number of training samples\n",
    "* XG Boost tends to do well when you have a mixture of categorical and numeric features \n",
    "    * Or just numeric features \n",
    "\n",
    "When to NOT use XG Boost: \\\n",
    "XGBoost is a powerful library that scales very well to many samples and works for a variety of supervised learning problems. That said, as Sergey described in the video, you shouldn't always pick it as your default machine learning library when starting a new project, since there are some situations in which it is not the best option. \\\n",
    "* Not ideally suited for image recognition\n",
    "* Computer vision\n",
    "* Natural Language Processing and understanding problems\n",
    "* (All of the above much better for Deep Learning)\n",
    "* When the number of training samples is significantly smaller than the number of features (for example fewer than 100 training samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78545247",
   "metadata": {},
   "source": [
    "## Regression with XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6422e0",
   "metadata": {},
   "source": [
    "### Objective functions and why we use them:\n",
    "* Quantifies how far off a prediction is from the actual result\n",
    "* Measures the distance between estimated and true values for some collection of data\n",
    "* Goal: Find the model that yields the minimum value of the loss function\n",
    "\n",
    "### Common loss functions and XBG:\n",
    "* For regression models:\n",
    "    * `reg:linear`\n",
    "* For binary classification:\n",
    "    * `reg:logistic` : use for classification problems when you want just decision, not probability.\n",
    "    * `binary:logistic` : use when you want probability and not just decision\n",
    "* linear base learners: learning API only (no sklearn API)\n",
    "\n",
    "```\n",
    "#Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "#Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", seed=123, n_estimators=10)\n",
    "#Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "#Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "#Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29e87f",
   "metadata": {},
   "source": [
    "```\n",
    "#Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(X_train, y_train)\n",
    "DM_test =  xgb.DMatrix(X_test, y_test)\n",
    "#Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "#Train the model: xg_reg\n",
    "xg_reg = xgb.train(dtrain = DM_train, params=params, num_boost_round=5)\n",
    "#Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "#Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc6600",
   "metadata": {},
   "source": [
    "```\n",
    "#Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "#Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "#Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "#Print cv_results\n",
    "print(cv_results)\n",
    "#Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69174305",
   "metadata": {},
   "source": [
    "```\n",
    "#Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "#Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "#Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "#Print cv_results\n",
    "print(cv_results)\n",
    "#Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21119cb7",
   "metadata": {},
   "source": [
    "## Regularization in XG Boost\n",
    "* Also takes into account how complex the model is. \n",
    "* __Regularization:__ Idea of penalizing models as they become more complex\n",
    "* __Regularization parameters in XG Boost:__\n",
    "    * __gamma:__ is a parameter for tree-based learners; minimum loss reduction allowed for a split to occur\n",
    "    * __alpha (L1):__ alpha is another name for L1 regularization; __L1 = Lasso__; L1 regularization on leaf weights, larger values mean more regularization; higher alpha values lead to stronger L1 regularization, which causes many leaf weights in the base learners to go to zero. \n",
    "    * __lambda:__ another name for L2 regularization; __L2 = Ridge__; a much smoother penalty than L1 and causes leaf weights to smoothly decrease, instead of enforcing strong sparcity constraints on the leaf weights.\n",
    "* *See DataCamp's Supervised Learning with Scikit Learn*\n",
    "### Base Learners in XG Boost:\n",
    "* __Linear Base Learner:__\n",
    "    * sum of linear terms\n",
    "    * when you combine many linear base learners together into a boosted base model, you get a weighted sum of linear models (and thus, is linear itself).\n",
    "    * Ensemble linear base learners are rarely used, as you won't/can't get any non-linear combination of features in the final model, as you get can identical performance from a regularized linear model.\n",
    "* __Tree Base Learner:__\n",
    "    * Decision Tree\n",
    "    * Boosted model is weighted sum of decision trees (non-linear)\n",
    "    * Almost exclusively used in XG Boost\n",
    "\n",
    "### Creating DataFrames from multiple equal-length lists:\n",
    "* recap of `zip()` and `list()`\n",
    "* `zip()` creates a generator of parallel values \n",
    "* `zip([1,2,3],[\"a\",\"b\",\"c\"])` = `[1, \"a\"], [2, \"b\"], [3,\"c\"]`\n",
    "* generators need to be completely instantiated before they can be used in DataFrame objects.\n",
    "* `list()` instantiates the full generator and passing that into the DataFrame converts the whole expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a032f80",
   "metadata": {},
   "source": [
    "```\n",
    "#Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "reg_params = [1, 10, 100]\n",
    "#Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "#Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "#Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "    #Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    #Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)    \n",
    "    #Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72826ee",
   "metadata": {},
   "source": [
    "```\n",
    "#Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "#Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "#Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "#Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees= 0)\n",
    "plt.show()\n",
    "#Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees= 4)\n",
    "plt.show()\n",
    "#Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees= 9, rankdir= \"LR\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4515e20",
   "metadata": {},
   "source": [
    "### Visualizing feature importances: What features are most important in my dataset:\n",
    "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
    "\n",
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you'll get a chance to use it in this exercise!\n",
    "```\n",
    "#Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(X, y)\n",
    "#Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "#Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix ,num_boost_round=10)\n",
    "#Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61625d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72babab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d7b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bc1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWND\n",
    "GHCND:USW00054704 MA\n",
    "    \n",
    "GHCND:USW00014740 CT\n",
    "\n",
    "GHCND:USW00014742 VT\n",
    "\n",
    "GHCND:USW00094626 ME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
