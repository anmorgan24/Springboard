{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5f438a",
   "metadata": {},
   "source": [
    "# Apache PySpark by Example\n",
    "**Instructor**: Jonathan Fernandes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e897d09",
   "metadata": {},
   "source": [
    "* Spark runs locally, as well as in a cluster, on premise, or in a cloud.\n",
    "* Spark runs on top of Hadoop YARN\n",
    "* Spark is primarily written in Scala on JVMs\n",
    "    * Scala is the default language, but you can also use:\n",
    "        * Java\n",
    "        * Python\n",
    "        * R\n",
    "        * SQL\n",
    "       \n",
    "* Apache Mesos stand-alone\n",
    "* Apache Mesos (in the cloud-Amazon)\n",
    "* Spark Core API is the foundation of the Spark ecosystem:\n",
    "    * Spark SQL\n",
    "    * Streaming\n",
    "    * MLlib\n",
    "    * GraphX: lets you make sense of graph-structured data at scale\n",
    "    \n",
    "#### Spark Core API\n",
    "* Task scheduling\n",
    "* Memory management\n",
    "* Fault recovery\n",
    "* Interacting with storage systems\n",
    "\n",
    "#### Spark SQL and DataFrames\n",
    "* Spark SQL can act as a Distributed Query Engine\n",
    "* DataFrame programming abstraction\n",
    "* Complex analytics with SQL $\\Downarrow$\n",
    "* Spark SQL allows you to intermix SQL's queries with the programmatic data manipulations supported by lower-level APIs in Python, Java, and Scala all within a single application and so combining SQL with complex analytics\n",
    "\n",
    "#### Spark Streaming\n",
    "* Process real-time data\n",
    "* Analyze streaming and historical data\n",
    "* Use similar code for batch data and real-time data\n",
    "\n",
    "#### MLlib\n",
    "* The machine-learning model built on top of Spark is a scalable ML library that delivers both high-quality algorithms  and a fast speed\n",
    "* Works in memory\n",
    "* Fast; can be 100x faster than MapReduce\n",
    "\n",
    "#### GraphX\n",
    "* A graph computation engine built on top of Spark that enables users to interactively build, transform, and reason about graph structure data at scale\n",
    "* Comes with a library of common algorithms\n",
    "* Introduces a new graph abstraction, the **directed multigraph** (*recall from Intro to Networks with NetworkX*)\n",
    "* Useful for visualizing social networks, etc\n",
    "\n",
    "#### PySpark\n",
    "* Recall that Spark is written in Scala\n",
    "* PySpark is just a Python wrapper around the Spark core\n",
    "* Alternatives to PySpark:\n",
    "    * **pandas:**\n",
    "        * tabular data\n",
    "        * can handle hundreds of thousands to millions of rows\n",
    "        * mature and feature-rich\n",
    "        * Limited to a single machine\n",
    "    * **Hadoop:** (a distributed cluster, why not Hadoop instead of Spark?)\n",
    "        * Until a couple of years ago, Hadoop was THE big data platform\n",
    "        * Compute system: MapReduce\n",
    "        * Compute system and storage system (Hadoop file system) closely integrated\n",
    "        * Closely integrated and really difficult to run one without the other; public cloud is one example of this (whereas Spark can be used on Hadoop storage or on a cloud).\n",
    "        * HDFS (Hadoop Distributed File System) and replication (are self-healing)\n",
    "        * One of the key differences between Spark and Hadoop lies in their approach to processing; Spark can process in memory, while Hadoop MapReduce has to read from and write to a disk.\n",
    "            * As a result the processing speed of each can differ significantly (with Spark being up to 100x faster)\n",
    "            * General rule of thumb: Hadoop requires more memory on disk and Spark requires more RAM.\n",
    "                * Meaning that setting up Spark clusters can be more expensive\n",
    "                * Hadoop to be used mainly for disc heavy operations with the MapReduce paradigm \n",
    "                * Spark tends to be more flexible, but more costly in memory processing architecture\n",
    "    * **Dask:** a library for parallel computing in Python\n",
    "        * Only written in Python\n",
    "        * only really supports Python\n",
    "\n",
    "* **Recall:** PySpark is written in Scala, but has support for Java, Python, R, and Scala, and interoperates well with JVM code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bca25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21696bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f26c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f71b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcce5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
