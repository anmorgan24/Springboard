{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e437ab",
   "metadata": {},
   "source": [
    "# Model Validation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab449cfe",
   "metadata": {},
   "source": [
    "Without proper validation, the results of running new data through a model might not be as accurate as expected. Model validation allows analysts to confidently answer the question, how good is your model? Goal: cover the basics of model validation, discuss various validation techniques, and begin to develop tools for creating validated and high performing models.\n",
    "\n",
    "* **Model Validation** consists of various steps and processes that ensure your model performs as expected on new data. The most common way to do this is to test your model's accuracy (or, insert evaluation metric of your choice) on data it has never seen before (called a **holdout set**). If your model's accuracy is similar for the data it was trained on and the holdout data. You can claim that your model is validated. The ultimate goal of model validation is to end up with the best performing model possible that achieves high accuracy on new data. \n",
    "\n",
    "#### Model validation consists of:\n",
    "    * Ensuring your model performs as expected on new data\n",
    "    * Testing model performance on holdout datasets\n",
    "    * Selecting the best model, parameters, and accuracy metrics\n",
    "    * Achieving the best accuracy for the data given\n",
    "    \n",
    "### Scikit-learn modeling review\n",
    "#### Basic modeling steps\n",
    "* 1. Create a model by specifying model type and its parameters\n",
    "* 2. Fit the model using the `.fit()` method\n",
    "* 3. To assess model accuracy, we generate predictions for data using the `.predict()` method. \n",
    "* 4. Look at accuracy metrics.\n",
    "\n",
    "* **The process of generating a model, fitting, predicting, and then reviewing model accuracy was introduced earlier in:**\n",
    "    * Intermediate Python\n",
    "    * Supervised Learning with scikit-learn\n",
    "\n",
    "```\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=1111)\n",
    "model.fit(X= X_train, y= y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"{0:.2f}\".format(mae(y_true = y_test, y_pred= predictions)))\n",
    "```\n",
    "* **Model validation's main goal is to ensure that a predictive model will perform as expected on new data.**\n",
    "* Training data = seen data\n",
    "\n",
    "```\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=1111)\n",
    "model.fit(X_train, y_train)\n",
    "train_predictions = model.predict(X_train)\n",
    "```\n",
    "\n",
    "* Testing data = unseen data\n",
    "\n",
    "```\n",
    "model = RandomForestRegressor(n_estimators = 500, random_state=1111)\n",
    "model.fit(X_train, y_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "* If your training and testing errors are vastly different, it may be a sign that your model is overfitted\n",
    "* Use model validation to make sure you get the best testing error possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ed2cf",
   "metadata": {},
   "source": [
    "### Regression models\n",
    "* More specifically: Random Forest Regression models using scikit-learn\n",
    "* Random forest algorithms have a lot of parameters, but here we focus on three:\n",
    "    * **`n_estimators`:** is the number of trees in the forest\n",
    "    * **`max_depth`:** the maximum depth of the trees (or how many times we can split the data). Also described as the maximum length from the beginning of a tree to the tree's end nodes \n",
    "    * **`random_state`:** random seed; allows us to create reproducible models\n",
    "* The most common way to set model parameters is to do so when initiating the model\n",
    "* However, they can also be set later, by assigning a new value to a model's attribute.\n",
    "    * This second method could be helpful when testing out different sets of parameters\n",
    "    \n",
    "```\n",
    "rfr = RandomForestRegressor(random_state=1111)\n",
    "rfr.n_estimators = 50\n",
    "rfr.max_depth = 10\n",
    "```\n",
    "\n",
    "#### Feature importance\n",
    "* After a model is created, we can assess how important different features (or columns) of the data were in the model by using the **`.feature_importances_`** attribute.\n",
    "\n",
    "* Use code below, so long as data is in pandas DataFrame\n",
    "\n",
    "```\n",
    "for i, item in enumerate(rfr.feature_importances_):\n",
    "    print(\"{0:s}: {1:.2f}\".format(X.columns[i], item))\n",
    "```\n",
    "\n",
    "* **The larger this number is, the more important that column was in the model.\n",
    "\n",
    "```\n",
    "# Set the number of trees\n",
    "rfr.n_estimators = 100\n",
    "\n",
    "# Add a maximum depth\n",
    "rfr.max_depth = 6\n",
    "\n",
    "# Set the random state\n",
    "rfr.random_state = 1111\n",
    "\n",
    "# Fit the model\n",
    "rfr.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Classification models\n",
    "* Several methods are shared across all scikit-learn models, but some are unique to the specific type of model\n",
    "* View how many observations were assigned to each class by turning the array of predictions into a pandas Series and then using the method `.value_counts()`:\n",
    "* `pd.Series(rfc.predict(X_test)).value_counts()`\n",
    "\n",
    "* Another prediction method is `.predict_proba()`, which returns an arry of predicted probabilities for each class\n",
    "* Sometimes in model validation, we want to know the probability values and not just the classification\n",
    "* Each entry of the array returned by `.predict_proba()` contains probabilities that sum to 1\n",
    "\n",
    "* **`rfc.get_params()`:**\n",
    "    * is used to review which parameters went into a scikit-learn model \n",
    "    * will print out a dictionary of parameters and their values, allowing us to see exactly which parameters were used\n",
    "    * Knowing a model's parameters is essential when assessing model quality, rerunning models, and even parameter tuning\n",
    "* **`.score()`:**\n",
    "    * A quick way to look at the overall accuracy of the classification model\n",
    "    \n",
    "* Replicating model performance is vital in model validation. Replication is also important when sharing models with co-workers, reusing models on new data or asking questions on a website such as Stack Overflow. \n",
    "* The best way to do this is to replicate your work by reusing model parameters.\n",
    "\n",
    "```\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)\n",
    "\n",
    "# Print the classification model\n",
    "print(rfc)\n",
    "\n",
    "# Print the classification model's random state parameter\n",
    "print('The random state is: {}'.format(rfc.random_state))\n",
    "\n",
    "# Print all parameters\n",
    "print('Printing the parameters dictionary: {}'.format(rfc.get_params()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536688f",
   "metadata": {},
   "source": [
    "#### Creating train, test, and validation datasets\n",
    "* \"Seen\" data (used for training)\n",
    "* \"Unseen\" data (unavailable for training)\n",
    "* In model validation, we use **holdout samples** to replicate this idea\n",
    "* We define a **holdout dataset** as any data that is not used for training and is only used to assess model performance\n",
    "    * The available data is split into two datasets:\n",
    "        * One used for training\n",
    "        * one that is simply off-limits while we are training our models (called a test--or holdout-- dataset)\n",
    "        \n",
    " * When evaluating model's performance using different parameter values: use **validation set**\n",
    " * **Validation set** should be same size as the testing set.\n",
    " \n",
    " * To create **training**, **validation**, and **testing** datasets, we use `.train_test_split()` **TWICE**:\n",
    " \n",
    "\n",
    " * The first call will create training and testing datasets like normal.\n",
    " * The following second call will split this so-called temporary training dataset into the final training and validation datasets.\n",
    " \n",
    "``` \n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X,y, test_size=0.2, random_state=1111)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, test_size=0.25, random_state=1111)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2294f82",
   "metadata": {},
   "source": [
    "## Accuracy metrics: Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c25061",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "* The simplest and most intuitive error metric\n",
    "* This metric treats all points equally and is not sensitive to outliers\n",
    "* When dealing with applications where we don't want large errors to have a major impact, the mean absolute error can be used\n",
    "* As an example: An MAE of 10 would mean that we are about 10 percentage points off (on average) when predicting the values of our dataset\n",
    "\n",
    "#### Mean squared error (MSE)\n",
    "* The most widely used regression error metric for regression models \n",
    "* It is calculated similarly to the mean absolute error, but this time we square the difference term\n",
    "* The MSE allows larger errors to have a larger impact on the model\n",
    "* Allows outlier errors to contribute more to the overall error \n",
    "\n",
    "* **Choosing between MAE and MSE comes down to the application**\n",
    "* Accuracy metrics are always application-specific\n",
    "* MAE and MSE error terms are in different units entirely and should not be directly compared \n",
    "\n",
    "#### Accuracy for a subset of data\n",
    "* For example, how does this particular model perform on *only chocolate* candies from the Ultimate Halloween candy dataset?\n",
    "* Filter dataset based on chocolate and not-chocolate candies and run accuracy score of choice on each\n",
    "\n",
    "```\n",
    "# Find the East conference teams\n",
    "east_teams = labels == \"E\"\n",
    "\n",
    "# Create arrays for the true and predicted values\n",
    "true_east = y_test[east_teams]\n",
    "preds_east = predictions[east_teams]\n",
    "\n",
    "# Print the accuracy metrics\n",
    "print('The MAE for East teams is {}'.format(mean_absolute_error(true_east, preds_east)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6a42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73461318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a366c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00da8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b467e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d90a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40ec8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
