{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8ceeb1",
   "metadata": {},
   "source": [
    "# Model Validation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcd280",
   "metadata": {},
   "source": [
    "# TO ADD TO CAPSTONE:\n",
    "* $\\star$ Compare how well the model performs on the data it *has* seen, as compared to the data it *hasn't* seen to help determine if over- or underfitting. \n",
    "    * Goal: eval metrics should be similar when performed on data model has seen to data model has not seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b3786",
   "metadata": {},
   "source": [
    "Without proper validation, the results of running new data through a model might not be as accurate as expected. Model validation allows analysts to confidently answer the question, how good is your model? Goal: cover the basics of model validation, discuss various validation techniques, and begin to develop tools for creating validated and high performing models.\n",
    "\n",
    "* **Model Validation** consists of various steps and processes that ensure your model performs as expected on new data. The most common way to do this is to test your model's accuracy (or, insert evaluation metric of your choice) on data it has never seen before (called a **holdout set**). If your model's accuracy is similar for the data it was trained on and the holdout data. You can claim that your model is validated. The ultimate goal of model validation is to end up with the best performing model possible that achieves high accuracy on new data. \n",
    "\n",
    "#### Model validation consists of:\n",
    "    * Ensuring your model performs as expected on new data\n",
    "    * Testing model performance on holdout datasets\n",
    "    * Selecting the best model, parameters, and accuracy metrics\n",
    "    * Achieving the best accuracy for the data given\n",
    "    \n",
    "### Scikit-learn modeling review\n",
    "#### Basic modeling steps\n",
    "* 1. Create a model by specifying model type and its parameters\n",
    "* 2. Fit the model using the `.fit()` method\n",
    "* 3. To assess model accuracy, we generate predictions for data using the `.predict()` method. \n",
    "* 4. Look at accuracy metrics.\n",
    "\n",
    "* **The process of generating a model, fitting, predicting, and then reviewing model accuracy was introduced earlier in:**\n",
    "    * Intermediate Python\n",
    "    * Supervised Learning with scikit-learn\n",
    "\n",
    "```\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=1111)\n",
    "model.fit(X= X_train, y= y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"{0:.2f}\".format(mae(y_true = y_test, y_pred= predictions)))\n",
    "```\n",
    "* **Model validation's main goal is to ensure that a predictive model will perform as expected on new data.**\n",
    "* Training data = seen data\n",
    "\n",
    "```\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=1111)\n",
    "model.fit(X_train, y_train)\n",
    "train_predictions = model.predict(X_train)\n",
    "```\n",
    "\n",
    "* Testing data = unseen data\n",
    "\n",
    "```\n",
    "model = RandomForestRegressor(n_estimators = 500, random_state=1111)\n",
    "model.fit(X_train, y_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "* If your training and testing errors are vastly different, it may be a sign that your model is overfitted\n",
    "* Use model validation to make sure you get the best testing error possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62094022",
   "metadata": {},
   "source": [
    "### Regression models\n",
    "* More specifically: Random Forest Regression models using scikit-learn\n",
    "* Random forest algorithms have a lot of parameters, but here we focus on three:\n",
    "    * **`n_estimators`:** is the number of trees in the forest\n",
    "    * **`max_depth`:** the maximum depth of the trees (or how many times we can split the data). Also described as the maximum length from the beginning of a tree to the tree's end nodes \n",
    "    * **`random_state`:** random seed; allows us to create reproducible models\n",
    "* The most common way to set model parameters is to do so when initiating the model\n",
    "* However, they can also be set later, by assigning a new value to a model's attribute.\n",
    "    * This second method could be helpful when testing out different sets of parameters\n",
    "    \n",
    "```\n",
    "rfr = RandomForestRegressor(random_state=1111)\n",
    "rfr.n_estimators = 50\n",
    "rfr.max_depth = 10\n",
    "```\n",
    "\n",
    "#### Feature importance\n",
    "* After a model is created, we can assess how important different features (or columns) of the data were in the model by using the **`.feature_importances_`** attribute.\n",
    "\n",
    "* Use code below, so long as data is in pandas DataFrame\n",
    "\n",
    "```\n",
    "for i, item in enumerate(rfr.feature_importances_):\n",
    "    print(\"{0:s}: {1:.2f}\".format(X.columns[i], item))\n",
    "```\n",
    "\n",
    "* **The larger this number is, the more important that column was in the model.\n",
    "\n",
    "```\n",
    "# Set the number of trees\n",
    "rfr.n_estimators = 100\n",
    "\n",
    "# Add a maximum depth\n",
    "rfr.max_depth = 6\n",
    "\n",
    "# Set the random state\n",
    "rfr.random_state = 1111\n",
    "\n",
    "# Fit the model\n",
    "rfr.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Classification models\n",
    "* Several methods are shared across all scikit-learn models, but some are unique to the specific type of model\n",
    "* View how many observations were assigned to each class by turning the array of predictions into a pandas Series and then using the method `.value_counts()`:\n",
    "* `pd.Series(rfc.predict(X_test)).value_counts()`\n",
    "\n",
    "* Another prediction method is `.predict_proba()`, which returns an arry of predicted probabilities for each class\n",
    "* Sometimes in model validation, we want to know the probability values and not just the classification\n",
    "* Each entry of the array returned by `.predict_proba()` contains probabilities that sum to 1\n",
    "\n",
    "* **`rfc.get_params()`:**\n",
    "    * is used to review which parameters went into a scikit-learn model \n",
    "    * will print out a dictionary of parameters and their values, allowing us to see exactly which parameters were used\n",
    "    * Knowing a model's parameters is essential when assessing model quality, rerunning models, and even parameter tuning\n",
    "* **`.score()`:**\n",
    "    * A quick way to look at the overall accuracy of the classification model\n",
    "    \n",
    "* Replicating model performance is vital in model validation. Replication is also important when sharing models with co-workers, reusing models on new data or asking questions on a website such as Stack Overflow. \n",
    "* The best way to do this is to replicate your work by reusing model parameters.\n",
    "\n",
    "```\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)\n",
    "\n",
    "# Print the classification model\n",
    "print(rfc)\n",
    "\n",
    "# Print the classification model's random state parameter\n",
    "print('The random state is: {}'.format(rfc.random_state))\n",
    "\n",
    "# Print all parameters\n",
    "print('Printing the parameters dictionary: {}'.format(rfc.get_params()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e033a",
   "metadata": {},
   "source": [
    "#### Creating train, test, and validation datasets\n",
    "* \"Seen\" data (used for training)\n",
    "* \"Unseen\" data (unavailable for training)\n",
    "* In model validation, we use **holdout samples** to replicate this idea\n",
    "* We define a **holdout dataset** as any data that is not used for training and is only used to assess model performance\n",
    "    * The available data is split into two datasets:\n",
    "        * One used for training\n",
    "        * one that is simply off-limits while we are training our models (called a test--or holdout-- dataset)\n",
    "        \n",
    " * When evaluating model's performance using different parameter values: use **validation set**\n",
    " * **Validation set** should be same size as the testing set.\n",
    " \n",
    " * To create **training**, **validation**, and **testing** datasets, we use `.train_test_split()` **TWICE**:\n",
    " \n",
    "\n",
    " * The first call will create training and testing datasets like normal.\n",
    " * The following second call will split this so-called temporary training dataset into the final training and validation datasets.\n",
    " \n",
    "``` \n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X,y, test_size=0.2, random_state=1111)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, test_size=0.25, random_state=1111)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4e002",
   "metadata": {},
   "source": [
    "## Accuracy metrics: Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dffec8",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "* The simplest and most intuitive error metric\n",
    "* This metric treats all points equally and is not sensitive to outliers\n",
    "* When dealing with applications where we don't want large errors to have a major impact, the mean absolute error can be used\n",
    "* As an example: An MAE of 10 would mean that we are about 10 percentage points off (on average) when predicting the values of our dataset\n",
    "\n",
    "#### Mean squared error (MSE)\n",
    "* The most widely used regression error metric for regression models \n",
    "* It is calculated similarly to the mean absolute error, but this time we square the difference term\n",
    "* The MSE allows larger errors to have a larger impact on the model\n",
    "* Allows outlier errors to contribute more to the overall error \n",
    "\n",
    "* **Choosing between MAE and MSE comes down to the application**\n",
    "* Accuracy metrics are always application-specific\n",
    "* MAE and MSE error terms are in different units entirely and should not be directly compared \n",
    "\n",
    "#### Accuracy for a subset of data\n",
    "* For example, how does this particular model perform on *only chocolate* candies from the Ultimate Halloween candy dataset?\n",
    "* Filter dataset based on chocolate and not-chocolate candies and run accuracy score of choice on each\n",
    "\n",
    "```\n",
    "# Find the East conference teams\n",
    "east_teams = labels == \"E\"\n",
    "\n",
    "# Create arrays for the true and predicted values\n",
    "true_east = y_test[east_teams]\n",
    "preds_east = predictions[east_teams]\n",
    "\n",
    "# Print the accuracy metrics\n",
    "print('The MAE for East teams is {}'.format(mean_absolute_error(true_east, preds_east)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c2535",
   "metadata": {},
   "source": [
    "#### Classification metrics\n",
    "* There are a lot of accuracy metrics available for classification problems\n",
    "    * Accuracy\n",
    "    * Precision \n",
    "    * Recall (Sensitivity)\n",
    "    * F1 Score\n",
    "    * Alternate F Scores\n",
    "    * Specificity\n",
    "* One way to calculate these metrics is to use the values from the confusion matrix\n",
    "* When making predictions, especially if there is a binary outcome, this matrix is one of the first outputs you should review\n",
    "* When we have a binary outcome, the confusion matrix is a 2x2 matrix that shows how your predictions faired across the two outcomes\n",
    "* **Create a confusion matrix using: `confusion_matrix()`**\n",
    "\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predictions)\n",
    "print(cm)\n",
    "```\n",
    "* In this matrix, the row index represents the true category and the column index represents the predicted category.\n",
    "* **Accuracy** represents the overall ability of your model to correctly predict the correct classification.\n",
    "* **Precision** is the number of true positives out of all predicted positive values.\n",
    "    * Precision is used when we don't want to overpredict positive values\n",
    "* **Recall** is about finding all positive values\n",
    "    * Recall is used when we can't afford to miss any positive values\n",
    "    \n",
    "* Accuracy, precision, and recall, are called similarly\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "accuracy_score(y_test, test_predictions)\n",
    "precision_score(y_test, test_predictions)\n",
    "recall_score(y_test, test_predictions)\n",
    "```\n",
    "\n",
    "```\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (TN + TP) / (953)\n",
    "print(\"The overall accuracy is {0: 0.2f}\".format(accuracy))\n",
    "\n",
    "# Calculate and print the precision\n",
    "precision = (TP) / (TP + FP)\n",
    "print(\"The precision is {0: 0.2f}\".format(precision))\n",
    "\n",
    "# Calculate and print the recall\n",
    "recall = (TP) / (TP + FN)\n",
    "print(\"The recall is {0: 0.2f}\".format(recall))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create predictions\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "print(cm)\n",
    "\n",
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "print(\"The number of true positives is: {}\".format(cm[1, 1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa63526",
   "metadata": {},
   "source": [
    "#### Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6c9f5",
   "metadata": {},
   "source": [
    "* How to identify when we have a good-fitting model? One way is to consider bias and variance\n",
    "* **Variance** occurs when a model pays too close attention to the training data and fails to generalize to the testing data\n",
    "    * Models with high variance perform well on only the training data, but not the testing data, and are considered to be **overfit**\n",
    "    * Overfitting occurs when our model starts to attach meaning to the noise in the training data\n",
    "    * Overfitting is easy to identify, as the training error will be a lot lower than the testing error\n",
    "* **Bias** occurs when the model fails to find the relationships between the data and the response value.\n",
    "    * Bias leas to high errors on both the training and testing datasets and is associated with an **underfit** model.\n",
    "    * Underfitting occurs when the model could not find the underlying patterns available in the data\n",
    "    * Underfitting might occur if we don't have enough trees or if the trees aren't deep enough\n",
    "    * Underfitting is more difficult to identify because the training and testing errors will both be high, and it's difficult to know if we got the most out of the data, or if we can improve the testing error.\n",
    "    \n",
    "* When our model is getting the most out of the training data, while still performing on the testing data, we have **optimal performance**\n",
    "* How do we tell if we have a good fit, or if we are just underfitting?\n",
    "    * For random forest models, some parameters that affect performance are max depth and max features \n",
    "    * One way to check for a poorly fit model is to try additional parameter sets and check both the training and testing error metrics\n",
    "   \n",
    "* **As you run more random forest models, you will get a better sense of which parameters you should tweak.**\n",
    "* **We always compare how well the model performed on the data it has seen to the data it has not seen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645691c",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "#### The problem with holdout sets\n",
    "* Repeating the validation process with a different random seed (or if you don't specify one at all), will result in different results\n",
    "* **The split matters.** \n",
    "* Cross-validation is the gold-standard for model-validation\n",
    "#### Cross-validation\n",
    "* Cross validation uses multiple training/validation splits; cv runs a single model on various training-validation combinations and gives us a lot more confidence in our final metrics \n",
    "    * And we can do this is such a manner thaat all of the data will only be used in one of the validation sets- i.e. sampling *without* replacement. In this way we can ensure that every point is used for validation exactly one time\n",
    "    * Using each point in only one validation set is not required in cross-validation, it is often good practice to do so. \n",
    "    \n",
    "* sklearn's **`KFold()`** function gives us a few option for splitting data into several training and validation sets \n",
    "    * **`n_splits`:** specify the number of splits that we want\n",
    "    * **`shuffle`:** boolean indicating to shuffle data before splitting\n",
    "    * **`random_state`:** random seed\n",
    "    \n",
    "```\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array(range(40))\n",
    "y = np.array([0] * 20 + [1] * 20)\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "splits = kf.split(X)\n",
    "```\n",
    "* **This only generates indices for us to use (*not* training and validation datasets)**\n",
    "* Here, we create a list of indices that can be used for our splits\n",
    "* **KFold** is generally used when we want to fit the same model using KFold cross-validation\\\n",
    "\n",
    "```\n",
    "rfr = RandomForestRegressor(n_estimators= 25, random_state=1111)\n",
    "errors = []\n",
    "for train_index, val_index in splits:\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_val, y_val = X[val_index], y[val_index]\n",
    "    \n",
    "    rfr.fit(X_train, y_train)\n",
    "    predictions = rfr.predict(X_test)\n",
    "    errors.append(,some_accuracy_metric>)\n",
    "\n",
    "```\n",
    "```\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Use KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1111)\n",
    "\n",
    "# Create splits\n",
    "splits = kf.split(X)\n",
    "\n",
    "# Print the number of indices\n",
    "for train_index, val_index in splits:\n",
    "    print(\"Number of training indices: %s\" % len(train_index))\n",
    "    print(\"Number of validation indices: %s\" % len(val_index))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n",
    "\n",
    "# Access the training and validation indices of splits\n",
    "for train_index, val_index in splits:\n",
    "    # Setup the training and validation data\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_val, y_val = X[val_index], y[val_index]\n",
    "    # Fit the random forest model\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # Make predictions, and print the accuracy\n",
    "    predictions = rfc.predict(X_val)\n",
    "    print(\"Split accuracy: \" + str(mean_squared_error(y_val, predictions)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed78a82",
   "metadata": {},
   "source": [
    "#### sklearn's cross_val_score()\n",
    "* KFold is a great way to create indices that we can use for cross-validation\n",
    "* If you just want to just straight in to cross-validation, and don't want to mess with the indices, you can use sklearn's **`cross_val_score()`** method\n",
    "* **`cross_val_score()`:** \n",
    "    * requires four parameters:\n",
    "        * `estimator` or specific model you want to use\n",
    "        * `X` to specify the complete training data set\n",
    "        * `y` to specify the response values\n",
    "        * `cv` the number of cross-validation splits\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "cross_val_score(estimator=rfc, X=X, y=y, cv=5)\n",
    "```\n",
    "* By default, `cross_val_score()` will use a defauly scoring function for whichever model you have specified (for most classifiers, accuracy... for most regressors, R2)\n",
    "* If you want to use a different scoring function, you can create a scorer by using the `.make_scorer()` method and specifying the metric that you want to use.\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "cross_val_score(<estimator>, <X>, <y>, cv=5, scoring=mae_scorer)\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForetsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "rfc = RandomForestRegressor(n_estimators=20, max_depth=5, random_state=1111)\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "cv_results = cross_val_score(rfc, X, y, cv=5, scoring=mse)\n",
    "\n",
    "print(cv_results)\n",
    "```\n",
    "\n",
    "* When we use cross-validation, we usually report the mean of the errors; this is a much more realistic estimate for the out-of-sample accuracy that we can expect to see on new data\n",
    "\n",
    "```\n",
    "cv_results.mean()\n",
    "cv_results.std()\n",
    "```\n",
    "* Find the mean and standard deviation of your cross validation results to determine spread and average; the smaller the standard deviation, the tighter you 5 means were\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b9ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd7e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
