{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f026ee52",
   "metadata": {},
   "source": [
    "Note: This notebook was completed as part of DataCamp's course of the same name.\n",
    "\n",
    "# Feature Engineering for NLP in Python\n",
    "In this course, you will learn techniques that will allow you to extract useful information from text and process them into a format suitable for applying ML models. More specifically, you will learn about POS tagging, named entity recognition, readability scores, the n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. You will also learn to compute how similar two documents are to each other. In the process, you will predict the sentiment of movie reviews and build movie and Ted Talk recommenders. Following the course, you will be able to engineer critical features out of any text and solve some of the most challenging problems in data science!\n",
    "\n",
    "**Instructor:** Rounak Banik, Data Scientist at Fractal Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4727c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textatistic import Textatistic\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271e05b",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Basic features and readability scores\n",
    "Learn to compute basic features such as number of words, number of characters, average word length and number of special characters (such as Twitter hashtags and mentions). You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0487e",
   "metadata": {},
   "source": [
    "### Introduction to NLP feature engineering\n",
    "* Learn to extract useful features out of text and convert them into formats that are suitable for machine learning algorithms\n",
    "* Recall that for any ML algorithm, data fed into it must be in tabular form and all the training features must be numerical\n",
    "* ML algorithms can also work with categorical data provided the categories are converted into numerical form through one-hot-encoding.\n",
    "\n",
    "#### One-hot encoding with pandas\n",
    "\n",
    "```\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "df = pd.get_dummies(df, columns=['sex'])\n",
    "```\n",
    "* **Note** that *not* mentioning columns will lead pandas to automatically encde all non-numerical features\n",
    "* Consider the following movie reviews dataset:\n",
    "\n",
    "<img src='data/mov_rev_data.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eead12",
   "metadata": {},
   "source": [
    "* The above data cannot be utilized by any ML algorithm\n",
    "* The training feature `review` is not numerical\n",
    "    * Neither is it categorical to perform one-hot encoding on\n",
    "    \n",
    "#### Text pre-processing\n",
    "* We need to perform two steps to make this dataset suitable for ML:\n",
    "    * 1) **Standardize the text:**\n",
    "        * converting words to lowercase\n",
    "        * lemmatization/ converting to root form\n",
    "        * example: `Reduction` gets converted to `reduce`\n",
    "    * 2) **Vectorization:**\n",
    "        * After standardization, the reviews are converted into a set of numerical training features through a process known as **vectorization**.\n",
    "        * After vectoriztion, our original review dataset gets converted into something like this:\n",
    "        \n",
    "<img src='data/vect_ex.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee320c",
   "metadata": {},
   "source": [
    "* We will learn techniques to achieve this in later lessons\n",
    "\n",
    "#### Basic features\n",
    "* We can alo extract certain basic features from text like:\n",
    "    * **word count**\n",
    "    * **character count**\n",
    "    * **average word length**\n",
    "* When working with niche data, such as tweets, it also may be useful to know how many hashtags have been used in a given tweet\n",
    "\n",
    "#### POS tagging\n",
    "* Some NLP applications may require you to extract features for individual words\n",
    "* For instance, you may want to do **parts-of-speech** or **POS** tagging to know the different parts-of-speech present in your text as shown:\n",
    "\n",
    "<img src='data/POS_tagging.png' width=\"150\" height=\"75\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f5510",
   "metadata": {},
   "source": [
    "* Consider the example above; POS tagging will label each word with its corresponding part-of-speech\n",
    "\n",
    "#### Named Entity Recognition (NER)\n",
    "* You may also want to perform named entity recognition to find out if a particular noun is referring to a person, organization or country\n",
    "* Does noun refer to person, orginazion, or country (or other)?\n",
    "\n",
    "#### Concepts covered\n",
    "* Text preprocessing\n",
    "* Basic features \n",
    "* Word features\n",
    "* Vectorization\n",
    "\n",
    "#### Exercises: One-hot-encoding\n",
    "\n",
    "```\n",
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c44f5",
   "metadata": {},
   "source": [
    "## Basic feature extraction\n",
    "* While not very powerful, basic features can give us a good idea of the text we are dealing with\n",
    "* The most basic feature we can extract from text is **number of characters** (including whitespaces)\n",
    "\n",
    "### Number of characters\n",
    "* The most basic feature we can extract from text\n",
    "* **Includes whitespaces**\n",
    "* For exapmle, the string `I don't know.` has **13 characters**.\n",
    "* The number of characters is the length of the string, or: `len(string)`\n",
    "* If our dataframe `df` has a textual feature (say `review`), we can compute the number of characters for each review and store it as a new feature `num_chars` by using the pandas dataframe `apply()` method:\n",
    "    * **`df['num_chars'] = df['review'].apply(len)`**\n",
    "\n",
    "### Number of words\n",
    "* Assuming that every word is separated by a space, we can use a string's `split()` method to convert it into a list where every element is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef2e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'had', 'a', 'little', 'lamb.']\n"
     ]
    }
   ],
   "source": [
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "\n",
    "# Print the list containing words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094fe48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Print number of words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5be80",
   "metadata": {},
   "source": [
    "* To do this for a textual feature in a dataframe, we first define a function that takes in a string as an argument and returns the number of words in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db66646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in string\n",
    "def word_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return length of words list\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd87fb",
   "metadata": {},
   "source": [
    "* We can now pass this function, `word_count()` to `apply()` and create `df['num_words']`:\n",
    "\n",
    "```\n",
    "# Create num_words feature in df\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "```\n",
    "\n",
    "### Average word length\n",
    "* Let's define a function `avg_word_length()` which takes in a string and returns the average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c800ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns average word length\n",
    "def avg_word_length(x):\n",
    "    # Split the string into words\n",
    "    words = x.split()\n",
    "    # Compute length of each word and store in a separate list\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    # Compute average word length\n",
    "    avg_word_length = sum(word_lengths)/len(words)\n",
    "    # Return average word length\n",
    "    return(avg_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27361ce7",
   "metadata": {},
   "source": [
    "* We can now pass this function (`avg_word_length()`) into `apply()` to generate an average word length feature in the df\n",
    "\n",
    "```\n",
    "# Create a new feature avg_word_length\n",
    "df['avg_word_length'] = df['review'].apply(doc_density)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28697d4e",
   "metadata": {},
   "source": [
    "### Special features\n",
    "* When working with data such as tweets, it may be useful to compute the number of hashtags or mentions used.\n",
    "\n",
    "### Hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8823e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of hashtags\n",
    "def hashtag_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Create a list of hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    # Return number of hashtags\n",
    "    return len(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ce159",
   "metadata": {},
   "source": [
    "* The procedure to compute number or mentions is identical except that we check if a word starts with `@` instead of `#`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac71c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of mentions\n",
    "def mention_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Create a list of mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    # Return number of mentions\n",
    "    return len(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf972def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b43812f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f7510",
   "metadata": {},
   "source": [
    "#### Other features\n",
    "* There are other basic features we can compute such as:\n",
    "    * Number of sentences \n",
    "    * Number of paragraphs\n",
    "    * Number of words starting with an uppercase\n",
    "    * All-capital words\n",
    "    * Numeric quantities\n",
    "    * etc. ...\n",
    "* The procedure to extract the above features is extremely similar to the ones we've already covered\n",
    "\n",
    "#### Exercises: Character count of Russian tweets\n",
    "\n",
    "```\n",
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())\n",
    "```\n",
    "\n",
    "#### Exercises: Word count of TED talks\n",
    "\n",
    "```\n",
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())\n",
    "```\n",
    "\n",
    "#### Hashtags and mentions in Russian tweets\n",
    "\n",
    "```\n",
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564ca58",
   "metadata": {},
   "source": [
    "### Readability tests\n",
    "* Here we will look at a set of interesting features known as **readability tests**, which are used to determine the readability of a particular passage (in English)\n",
    "* In other words, it indicates at what educational level a person needs to be, in order to comprehend a particular piece of text\n",
    "* The scale usually ranges from **primary school** up to **college graduate level** and is in context of the American education system\n",
    "* **Readability tests** are done using a mathematical formula that utilizes the word, syllable, and sentence count of the passage.\n",
    "* Readability tests are routinely used by organizations to determine how difficult their publications are to understand (or not).\n",
    "* Readability tests have also found applications in domains such as **fake news**, and **opinion spam detection**.\n",
    "* There are a variety of readability tests in use\n",
    "\n",
    "#### Readability text examples\n",
    "* Some common examples:\n",
    "    * **Flesch reading ease**\n",
    "    * **Gunning fog index**\n",
    "    * **Simple Measure of Gobbledygook (SMOG)**\n",
    "    * **Dale-Chall score**\n",
    "* $\\star$ **Note** that all of these tests are used for texts in **English**\n",
    "* Tests for other languages also exist that take into consideration the nuances of that particular language\n",
    "* In this lesson, we will cover the first two scores (Flesch reading ease and Gunning fog index) in detail\n",
    "    * However, once you understand these two, you will be in a good position to understand and use the other scores as well.\n",
    "    \n",
    "### Flesch reading ease\n",
    "* The Flesch Reading Ease is one of the **oldest** and **most widely used** readability tests\n",
    "* Dependent on two factors:\n",
    "    * **1) The greater the average sentence length, the harder a text is to read.**\n",
    "    * **2) The greater the average number of syllables in a word, the harder a text is to read.**\n",
    "* The higher the Flesch Reading Ease score, the greater is the readability \n",
    "    * A higher score indicates that the text is easier to understand\n",
    "* **Higher the score, greater the readability**\n",
    "    \n",
    "<img src='data/flesch_scores.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "### Gunning fog index\n",
    "* Developed in 1954\n",
    "* Dependent on:\n",
    "    * **1) Average sentence length**\n",
    "    * **2) The greater the percentage of complex words, the harder the text is to read.**\n",
    "            * Here, \"complex words\" refer to all words that have three or more syllables\n",
    "* Unlike Flesch, the formula for Gunning fog index is such that the higher the score, the more difficult the passage is to understand\n",
    "* **Higher the index, lesser the readability.**\n",
    "\n",
    "<img src='data/gunning_scores.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e1da0",
   "metadata": {},
   "source": [
    "### The textatistic library \n",
    "* We can conduct these readability tests in Python using the Textatistic library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35cebc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textatistic\n",
      "  Downloading textatistic-0.0.1.tar.gz (29 kB)\n",
      "Collecting pyhyphen>=2.0.5\n",
      "  Downloading PyHyphen-4.0.3-cp37-abi3-macosx_10_14_x86_64.whl (37 kB)\n",
      "Requirement already satisfied: setuptools>=52.0 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests>=2.25 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (2.25.1)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (1.4.4)\n",
      "Requirement already satisfied: wheel>=0.36.0 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (0.36.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (4.0.0)\n",
      "Building wheels for collected packages: textatistic\n",
      "  Building wheel for textatistic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for textatistic: filename=textatistic-0.0.1-py3-none-any.whl size=29056 sha256=48e2605433b68a36e2057c6e5bb2e18dd29014aa948312c62909c461ee3a9758\n",
      "  Stored in directory: /Users/abigailmorgan/Library/Caches/pip/wheels/82/24/c4/de7882083c3530984f6eda43ae9e94875c84d906063ef10bcb\n",
      "Successfully built textatistic\n",
      "Installing collected packages: pyhyphen, textatistic\n",
      "Successfully installed pyhyphen-4.0.3 textatistic-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a6de54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary had a little lamb.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c3ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.24000000000002\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Create a Textatistic Object\n",
    "readability_scores = Textatistic(text).scores\n",
    "\n",
    "# Generate scores\n",
    "print(readability_scores['flesch_score'])\n",
    "print(readability_scores['gunningfog_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce859615",
   "metadata": {},
   "source": [
    "#### Exercises: Readability of 'The Myth of Sisyphus'\n",
    "\n",
    "```\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "```\n",
    "\n",
    "#### Exercises: Readability of various publications\n",
    "\n",
    "```\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a510b4f",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Text preprocessing, POS tagging and NER\n",
    "In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370713bd",
   "metadata": {},
   "source": [
    "### Tokenization and Lemmatization\n",
    "#### Text sources (all very different styles, grammars, and vocab)\n",
    "* News articles \n",
    "* Tweets\n",
    "* Social media comments\n",
    "\n",
    "#### Making text machine friendly\n",
    "* It is important to standardize all of these (above) texts into a machine-friendly format\n",
    "* We want our models to treat similar words as the same\n",
    "\n",
    "### Text preprocessing techniques\n",
    "* The text processing techniques you use are dependent on the application you're working on\n",
    "* Some of the common ones we'll be covering include:\n",
    "    * Covnverting words into lowercase\n",
    "    * Removing leading and trailing whitespaces\n",
    "    * Removing punctuation\n",
    "    * Removing commonly occuring words (**stopwords**)\n",
    "    * Expanding contractions\n",
    "    * Removing special characters (numbers, emojis, etc)\n",
    "    \n",
    "### Tokenization\n",
    "* **Tokenization** is the process of splitting a string into its constituent tokens\n",
    "* These tokens may be sentences, words, or punctuations and *are specific to a particular language.*\n",
    "* In this course, we will be primarily focused with word and punctuation tokens\n",
    "* Tokenization also involves **expanding contracted words.**\n",
    "\n",
    "#### Tokenization using spaCy\n",
    "* We load a pre-trained English model, `en_core_web_sm` using `spacy.load()`\n",
    "    * This will return a language object that has the know-how to perform tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be25cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271abaa",
   "metadata": {},
   "source": [
    "* The `doc` object defined above contains the required tokens (and many other things, as we will soon find out).\n",
    "* We generate the list of tokens by using list comprehension as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11a27ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02704e6",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "* **Lemmatization** is the process of converting a word into its lowercased base form, or **lemma**.\n",
    "* This is an extremely powerful process of standardization\n",
    "* Examples:\n",
    "    * `reducing`, `reduces`, `reduced`, `reduction` $\\Rightarrow$ $\\Rightarrow$ **`reduce`**\n",
    "    * `am`, `are`, `is` $\\Rightarrow$ $\\Rightarrow$ **`be`**\n",
    "    * `n't` $\\Rightarrow$ $\\Rightarrow$ **`not`**\n",
    "    * `'ve` $\\Rightarrow$ $\\Rightarrow$ **`have`**\n",
    "* When you pass the string into `nlp`, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens, except that we extract `token.lemma_` in each iteration inside the list comprehension instead of `token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acc4913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693c4d6",
   "metadata": {},
   "source": [
    "* **Also note that spaCy converted `I`s into `-PRON-`**; this is standard behavior, where every pronoun is converted into the string `-PRON-`\n",
    "\n",
    "#### Exercises: Tokenizing the Gettysburg Address\n",
    "\n",
    "```\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "#### Exercises: Lemmatizing the Gettysburg address\n",
    "\n",
    "```\n",
    "print(gettysburg)\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822d379",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "\n",
    "#### Text cleaning techniques\n",
    "* Unnecessary whitespaces and escape sequences\n",
    "* Punctuations\n",
    "* Special characters (numbers, emojis, etc.)\n",
    "* Stopwords\n",
    "\n",
    "\n",
    "* In other words, it is very common to remove non-alphabetic tokens and words that occur so commonly that they are not very useful for analyis\n",
    "\n",
    "#### isalpha()\n",
    "* Every Python string has an **`isalpha()`** method that returns `True` if all the characters of the string are alphabetic\n",
    "* This is an extremely convenient method to remove all (lemmatized) tokens that are or that contain numbers, punctuation and emojis\n",
    "* **A word of caution:** `isalpha()` has a tendency of returning false on words we would not want to remove. Examples:\n",
    "    * Abbreviations: `U.S.A.`, `U.K.`, etc\n",
    "    * Proper Nounds with numbers in them: `word2vec` and `xto10x`\n",
    "    * For such nuanced cases, `isalpha()` may not be sufficient and it may be advisable to write your own custom functions.\n",
    "    * Write your own custome functions (typically using regex) for the more nuanced cases \n",
    "\n",
    "#### Removing non-alphabetic characters\n",
    "* First, we generate the lemmatized tokens like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11a086c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "\n",
    "# Generate list of tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb5879",
   "metadata": {},
   "source": [
    "* Next, we loop through the tokens again and choose only those words that are either `-PRON-` or contain only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0389b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG this be like the good thing ever wow such an amazing song -PRON- be hooked Top definitely\n"
     ]
    }
   ],
   "source": [
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma =='-PRON-']\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde918b",
   "metadata": {},
   "source": [
    "* Make lower case (in video this was done automatically with above code, not sure why it didn't here, so I'm lower-casing it in a separate call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcee52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_lemmas = []\n",
    "for lemma in a_lemmas:\n",
    "    al_lemmas.append(lemma.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24d99f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', 'this', 'be', 'like', 'the', 'good', 'thing', 'ever', 'wow', 'such', 'an', 'amazing', 'song', '-pron-', 'be', 'hooked', 'top', 'definitely']\n"
     ]
    }
   ],
   "source": [
    "print(al_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13668735",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "* There are some words in the English language that occur so commonly that it is often a good idea to just ignore them\n",
    "* Examples: \n",
    "    * articles: \n",
    "        * a\n",
    "        * the\n",
    "    * be verbs:\n",
    "        * is\n",
    "        * am\n",
    "    * pronouns:\n",
    "        * he\n",
    "        * she\n",
    "        * they\n",
    "* **`spaCy` has a built-in list of stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cd91e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fafbd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG like good thing wow amazing song hooked Top definitely\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6eff369",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_lemmas = []\n",
    "for lemma in a_lemmas:\n",
    "    al_lemmas.append(lemma.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7888aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg like good thing wow amazing song hooked top definitely\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(al_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085c939",
   "metadata": {},
   "source": [
    "* **Notice** that we have removed the `-PRON-` condition as pronouns are stopwords anyway and should be removed\n",
    "* Additionally, we have introduced a new condition to check if the word belongs to spacy's list of stopwords\n",
    "* **Notice also** how the string consists only of base form words\n",
    "* **Always** exercise caution whil using third party stopword lists\n",
    "    * It is common that an application find certain words useful that may be consideed a stopword by third party lists\n",
    "    * **It is often advisable to create your own custom stopword lists**\n",
    "    \n",
    "#### Other text preprocessing techniques\n",
    "* There are other preprocessing techniques that are used but have been omitted for the sake of brevity\n",
    "* Some of them include:\n",
    "    * **Removing HTML or XML tags**\n",
    "    * **Replacing accented characters**\n",
    "    * **Correcting spelling errors and shorthands**\n",
    "    \n",
    "    \n",
    "* **A word of caution:** the text preprocessing techniques you use are always dependent on the application\n",
    "* There are many applications which may find punctuations, numbers, and emojis useful, so in these cases it may not be wise to remove them\n",
    "* **Always use only those text preprocessing techniques that are relevant to your application.**\n",
    "\n",
    "#### Exercises: Cleaning a blog post\n",
    "\n",
    "```\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "```\n",
    "\n",
    "#### Exercises: Cleaning TED talks in a dataframe\n",
    "\n",
    "```\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "500556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a35016",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "* Part-of-speech tagging (or **POS tagging**) is one of the most popularly used feature engineering techniques in NLP\n",
    "\n",
    "#### Applications\n",
    "* **Word-sense disambiguation:**\n",
    "    * `\"The bear is a majestic animal\"`\n",
    "    * `\"Please bear with me\"`\n",
    "* **Sentiment analysis**\n",
    "* **Question answering systems**\n",
    "* **Fake news and opinion spam detection** (linguistic approaches)\n",
    "    * For example, one paper discovered that fake news headlines, on average, tend to use less common nouns and more proper nouns than mainstream headlines\n",
    "    * Generating the POS tags for these words proved extremely useful in detecting false or hyperpartisan news\n",
    "    \n",
    "#### POS tagging using spaCy\n",
    "* **POS tagging** is the process of assigning every word (or token) in a piece of text, its corresponding part of speech.\n",
    "* Performing POS tagging with spaCy is almost identical to generating tokens or lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e9798eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53467cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize string\n",
    "string = \"Jane is an amazing guitarist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "624e59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226a23e",
   "metadata": {},
   "source": [
    "Using list comprehension, the first element of the tuple is the token and is generated using `token.text` and `token.pos_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2a711cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jane', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('guitarist', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3b372",
   "metadata": {},
   "source": [
    "* SpaCy infers the POS tags of these words based on the predictions given by its pre-trained models.\n",
    "* In other words, **the accuracy of the POS tagging is dependent on the data that the model has been trained on and the data that it is being used on.**\n",
    "\n",
    "#### POS annotations in spaCy\n",
    "* spaCy is capable of identifying close to 20 parts-of-speech and it uses specific annotations to denote a particular part of speech\n",
    "* complete spaCy annotation list [HERE](https://spacy.io/api/annotation)\n",
    "\n",
    "<img src='data/POS_annot.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84721316",
   "metadata": {},
   "source": [
    "#### POS tagging in Lord of the Flies\n",
    "\n",
    "```\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "```\n",
    "\n",
    "#### Exercises: Counting nouns in a piece of text\n",
    "In this exercise, we will write two functions, `nouns()` and `proper_nouns()` that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news.\n",
    "\n",
    "```\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6081c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "      # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "\n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2984a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "      # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "\n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54d5be",
   "metadata": {},
   "source": [
    "#### Exercises: Noun usage in fake news\n",
    "\n",
    "```\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc0c2c",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "* **Named entity recognition** or **NER** has a host of extremely useful applications\n",
    "\n",
    "#### Applications\n",
    "* Efficient search algorithms \n",
    "* Question answering systems\n",
    "* News article classification\n",
    "* Customer service centers (to classify and record complaints efficiently)\n",
    "\n",
    "#### Named entity recognition\n",
    "* A **named entity** is anything that can be denoted with a proper name or a proper noun. \n",
    "* **NER** is the process of identifying such named entities in a piece of text and classifying them into predefined categories\n",
    "* Categories include person, organization, country, etc.\n",
    "\n",
    "#### NER using spaCy\n",
    "* Performing NER is extremely easy using spaCy's pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75496890",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"John Doe is a software engineer working at Google. He lives in France.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3527aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04244c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3684cc5",
   "metadata": {},
   "source": [
    "* Note that `GPE` is \"Geopolitical Entity\"\n",
    "* Currently spaCy's models are capable of identifyin more than 15 different types \n",
    "* Find [complete list here](https://spacy.io/api/annotation#named-entities)\n",
    "* Below is a small snapshot:\n",
    "\n",
    "<img src='data/NER_annote.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b14f0",
   "metadata": {},
   "source": [
    "* **Word of caution** if we are trying to extract named entities for texts from a heavily technical field (such as medicine), spaCy's pretrained models may not perform very well.\n",
    "* In such nuances cases, it is better to train your own models with your specialized data.\n",
    "* Also remember that spacy's models are **language specific**\n",
    "\n",
    "#### Exercises: Named entities in a sentence\n",
    "\n",
    "```\n",
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "#### Exercises: Identifying people mentioned in a news article\n",
    "\n",
    "```\n",
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78de6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41671098",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: N-Gram models\n",
    "Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00480561",
   "metadata": {},
   "source": [
    "### Building a bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a385e",
   "metadata": {},
   "source": [
    "* **Vectorization** is the process of converting text (or other data) into vectors\n",
    "* Recall that for any ML algorithm, data must be in tabular form and training features must all be numerical\n",
    "* **Bag-of-words** is a technique that converts text documents into vectors we can use in ML algorithms\n",
    "* The **bag-of-words model** is a procedure of extracting word tokens from a text document, computing the frequency of these word tokens and constructing a word vector based on these frequencies and the vocabulry of the entire corpus of documents\n",
    "\n",
    "#### Bag of words model\n",
    "* Extract word tokens\n",
    "* Compute frequency of word tokens\n",
    "* Construct a word vector out of these frequencies and vocabulary of corpus\n",
    "* With, for example, 15 words in our vocabulary, our word vectors will have 15 dimensions and each dimension's value will correspond to the frequency of the word token corresponding to that dimension\n",
    "    * For instance, the second dimension will correspond to the number of times the second word in the vocabulary occurs in the document \n",
    "    \n",
    "<img src='data/bow_ex1.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfac854",
   "metadata": {},
   "source": [
    "* **Note** that performing text preprocessing usually leads to smaller vocabularies (which is often a good thing).\n",
    "* While working with vectorization, it is routine to form word vectors running into thousands of dimensions and keeping this (vocabulary, and subsequent dimensions) to a minimum helps improve performance\n",
    "* **Reducing number of dimensions helps improve performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7d484e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.Series([\n",
    "        \"The lion is the king of the jungle\",\n",
    "        \"Lions have lifespans of a decade\",\n",
    "        \"The lion is an endangered species\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34aa2e0",
   "metadata": {},
   "source": [
    "(For now we will ignore text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec4115c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 1 0 1 0 1 0 3]\n",
      " [0 1 0 1 0 0 0 1 0 1 1 0 0]\n",
      " [1 0 1 0 1 0 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf698ac8",
   "metadata": {},
   "source": [
    "* **Note** that the `bow_matrix` is a sparse matrix that can be printed out in its 2D form using `bow_matrix.toarray()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c471fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t3\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecbbd4",
   "metadata": {},
   "source": [
    "* **Notice** that the output of `bow_matrix_toarray()` is different from the word vectors generated in the image above.\n",
    "    * This is because `CountVectorizer` automatically lowercases words and ignores single-character-tokens such as `'a'`.\n",
    "    * Also, it doesn't necessarily index the vocabulary in alphabetical order\n",
    "    * We can use this `bow_matrix` as our training features in ML models\n",
    "    \n",
    "#### Exercises: BoW model for movie taglines \n",
    "In this exercise, you have been provided with a `corpus` of more than 7000 movie tag lines. Your job is to generate the bag of words representation `bow_matrix` for these taglines. For this exercise, we will ignore the text preprocessing step and generate `bow_matrix` directly.\n",
    "\n",
    "We will also investigate the shape of the resultant `bow_matrix`.\n",
    "\n",
    "```\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)\n",
    "```\n",
    "\n",
    "#### Exercises: Analyzing dimensionality and preprocessing\n",
    "Your job is to generate the bag of words representation `bow_lem_matrix` for these lemmatized taglines and compare its shape with that of `bow_matrix` obtained in the previous exercise. \n",
    "\n",
    "```\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)\n",
    "```\n",
    "\n",
    "#### Exercises: Mapping feature indices with feature names\n",
    "We had seen that `CountVectorizer` doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary.\n",
    "\n",
    "```\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f646514",
   "metadata": {},
   "source": [
    "### Building a BoW Naive Bayes classifier\n",
    "#### The spam filtering problem\n",
    "* **Steps:**\n",
    "    * 1) Text preprocessing\n",
    "    * 2) Building a bag-of-words model (or representation)\n",
    "    * 3) Machine learning (predictive modeling)\n",
    "    \n",
    "#### Text preprocessing using CountVectorizer\n",
    "* `CountVectorizer` arguments:\n",
    "    * **`lowercase`:** `False`, `True`\n",
    "    * **`strip_accents`:** `'unicode'`, `'ascii'`, `None`\n",
    "    * **`stop_words`:** `'english'`, `list`, `None`\n",
    "    * **`token_pattern`:** `regex`\n",
    "        * specify tokenization using a regular expression as the value of the `token_pattern` argument\n",
    "    * **`tokenizer`:** `function`\n",
    "        * tokenization can also be specified using a `tokenizer` argument\n",
    "        * here, you can pass a function that takes a string as an argument and returns a list of tokens\n",
    "\n",
    "* In these (2) ways, `CountVectorizer` allows usage of `spaCy`'s tokenization techniques\n",
    "* `CountVectorizer` cannot perform certain steps such as lemmatization automatically.\n",
    "    * This is where `spaCy` is useful\n",
    "* Although it performs tokenization and preprocessing, `CountVectorizer`'s main job is to convert a corpus into a matrix of numerical vectors\n",
    "* When building the spam-detecting BoW model below, we set `lowercase` to `False` because spam messages tend to abuse all-capital words and we might want to preserve this information for the ML step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f744345",
   "metadata": {},
   "source": [
    "```\n",
    "# Import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
    "\n",
    "# Import train_test_split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.25)\n",
    "\n",
    "# Generate training BoW vectors\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Generate test BoW vectors\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "```\n",
    "* **Note** that it is possible that there may be some words in the test data that are not in the vocabulary of the vectorizer (which was trained only with the vocabulary contained in the training set). **In such cases, `CountVectorizer` simply ignores these words.**\n",
    "\n",
    "```\n",
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train clf\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Compute accuracy on test set\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(accuracy)\n",
    "```\n",
    "\n",
    "#### Exercises: BoW vectors for movie reviews\n",
    "\n",
    "```\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "```\n",
    "\n",
    "#### Exercises: Predicting the sentiment of a movie review\n",
    "\n",
    "```\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "```\n",
    "\n",
    "### Building n-gram models\n",
    "\n",
    "<img src='data/bow_shortcomings.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268a499",
   "metadata": {},
   "source": [
    "* If we were to construct BoW vectors for these reviews, we would get identical vectors.\n",
    "* **The biggest shortcoming of the bag of words model: the *context* of words is lost.**\n",
    "\n",
    "### n-grams\n",
    "* An **n-gram** is a contiguous sequence of n elements (or words) in a given document\n",
    "* n = 1 $\\Rightarrow$ bag-of-words\n",
    "\n",
    "#### Applications\n",
    "* sentence completion\n",
    "* spelling correction\n",
    "* machine translation correction\n",
    "\n",
    "#### Building n-gram models using scikit-learn\n",
    "* `CountVectorizer` takes in an argument **`ngram_range`**, which is a tuple containing the lower and upper bound for the range of n-values\n",
    "* For example, the following only generates bigrams:\n",
    "    * `bigrams = CountVectorizer(ngram_range(2,2))`\n",
    "* The following generates unigrams, bigrams, and trigrms:\n",
    "    * `ngrams = CountVectorizer(ngam_range=(1,3))`\n",
    "    \n",
    "#### Shortcomings\n",
    "* Curse of dimensionality\n",
    "* Higher order n-grams are rare\n",
    "* Keep $n$ small\n",
    "\n",
    "#### Exercises: n-gram models for movie tag lines\n",
    "\n",
    "```\n",
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
    "```\n",
    "\n",
    "#### Exercises: Higher order n-grams for sentiment analysis\n",
    "\n",
    "```\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "```\n",
    "#### Exercises: Comparing performance of n-gram models\n",
    "\n",
    "```\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6537272",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 4: TF-IDF and similarity scores\n",
    "Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs.\n",
    "\n",
    "### Building tf-idf document vectors\n",
    "\n",
    "#### n-gram modeling\n",
    "* Weight of dimension dependent on the frequency of the word corresponding to the dimension\n",
    "\n",
    "#### Application\n",
    "* Autoatically detect stopwords\n",
    "* Search algorithms\n",
    "* Recommender systems\n",
    "* Better performance in predictive modeling for some cases\n",
    "\n",
    "#### Term frequency-inverse document frequency\n",
    "* Tfidf is the weighting mechanism for the importance of commonly occuring words\n",
    "* **It is based on the idea that the weight of a term in a document should be proportional to its frequency and an inverse function of the number of documents in which it occurs.**\n",
    "\n",
    "<img src='data/tfidf_formula2.png' width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62c34a",
   "metadata": {},
   "source": [
    "* **In general, the higher the tf-idf weight, the more important the word is in characterizing the document.**\n",
    "* **A high tf-idf weight for a word in a document may imply that the word is relatively exclusive to that particular document, or that the words occurs extremely commonly in the document, or both.**\n",
    "* The parameters and methods available within `TfidfVectorizer` is almost identical to `CountVectorizer`\n",
    "* The only difference is that `TfidfVectorizer` assigns weights using the tf-idf formula in the image above and has extra parameters related to inverse document frequency that CountVectorizer does not have.\n",
    "* Weights are non-integer (floats)\n",
    "\n",
    "```\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.toarray())\n",
    "```\n",
    "\n",
    "#### Exercises: tf-idf vectors for TED talks\n",
    "\n",
    "```\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f60d8a",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "* We'll now explore techniques that allow us to determine how similar two vectors and, consequentially two documents, are to each other\n",
    "* **Cosine similarity score** is one of the most popular metrics in NLP\n",
    "\n",
    "<img src='data/cos_sim.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Mathematically, cosine similarity is the ratio of the dot product of the vectors and the product of the madnitude of the two vectors\n",
    "\n",
    "<img src='data/vector_dot_products.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fa29f",
   "metadata": {},
   "source": [
    "#### Magnitude of a vector\n",
    "* The **magnitude** of a vector is essentially the length of the vector\n",
    "    * Mathematically it is defined as the square root of the sum of the squares of values across all the dimensions of a vector\n",
    "    \n",
    "<img src='data/vector_magnitude.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae614ea",
   "metadata": {},
   "source": [
    "#### The cosine score\n",
    "\n",
    "<img src='data/cos_score.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6e797",
   "metadata": {},
   "source": [
    "#### Cosine Score: points to remember:\n",
    "* Value between -1 and 1\n",
    "* In NLP, value between 0 and 1 ; (0 = no similarity, 1 = identical)\n",
    "* Robust to document length\n",
    "\n",
    "```\n",
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two 3-dimensional vectors A and B\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score \n",
    "print(score)\n",
    "```\n",
    "* **Remember** that `cosine_similarity` only accepts 2D arrays as inputs. Passing 1D arrays will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e989ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73881883]]\n"
     ]
    }
   ],
   "source": [
    "# Import the cosine_similarity\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two 3-dimensional vectors A and B\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac531f",
   "metadata": {},
   "source": [
    "* Note that we got the same answer in the calculations performed in the illustration above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a63ce",
   "metadata": {},
   "source": [
    "#### Exercises: Computing dot product\n",
    "\n",
    "```\n",
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)\n",
    "```\n",
    "\n",
    "#### Exercises: Cosine similarity matrix of a corpus\n",
    "\n",
    "```\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c067d1e",
   "metadata": {},
   "source": [
    "### Building a plot line based recommender\n",
    "\n",
    "#### Steps\n",
    "* 1) Text preprocessing\n",
    "* 2) Generate tf-idf vectors\n",
    "* 3) Generate cosine similarity matrix (containing the pairwise similarity scores of every movie with every other movie)\n",
    "\n",
    "#### The recommender function\n",
    "* 1) Take a movie title, cosine similarity matrix and indices series as arguments\n",
    "    * The **indices series** is a reverse mapping of movie titles with their indices in the original dataframe)\n",
    "* 2) Extract pairwise cosine similarity scores for the movie\n",
    "* 3) Sort the scores in descending order\n",
    "* 4) Output titles corresponding to the highest scores\n",
    "* 5) Ignore the highest similarity score (of 1)\n",
    "    * **This is because the movie most similar to a given movie is the movie itself!**\n",
    "    \n",
    "#### Generating tf-idf vectors\n",
    "\n",
    "```\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of tf-idf vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(movie_plots)\n",
    "```\n",
    "\n",
    "#### Generating cosine similarity matrix\n",
    "\n",
    "```\n",
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = cosine_simiarity(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "* This generates a matrix that contains the pairwise similarity score of every movie with every other movie.\n",
    "* The value corresponding to the $i$th row and the $j$th column is the cosine similarity score of movie $i$ with movie $j$\n",
    "* The diagonal elements of the matrix will be 1 (movie with itself)\n",
    "\n",
    "#### The linear_kernel function\n",
    "* Magnitude of a tf-idf vector is 1\n",
    "* Cosine score between two tf-idf vectors is their dot product\n",
    "* Can significantly improve computation time\n",
    "* Use `linear_kernel` instead of `cosine_similarity`\n",
    "\n",
    "#### Generating cosine similarity matrix\n",
    "\n",
    "```\n",
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "* Using the `linear_kernel` outputs the same result, but takes significantly less time to compute.\n",
    "\n",
    "\n",
    "#### The get_recommendations function\n",
    "* `get_recommendations('The Lion King', cosine_sim, indices)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab3d1e",
   "metadata": {},
   "source": [
    "#### Exercises: Comparing linear_kernel and cosine_similarity\n",
    "\n",
    "```\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "```\n",
    "\n",
    "#### Exercises: Plot recommendation engine\n",
    "\n",
    "```\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\n",
    "```\n",
    "\n",
    "#### Exercises: The recommender function\n",
    "\n",
    "```\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bdbb4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc8819",
   "metadata": {},
   "source": [
    "#### Exercises: TED talk recommender\n",
    "\n",
    "```\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced8bde",
   "metadata": {},
   "source": [
    "### Beyond n-grams: word embeddings\n",
    "\n",
    "#### The problem with BoW and tf-idf\n",
    "* Consider the following sentences:\n",
    "    * `'I am happy'`\n",
    "    * `'I am joyous'`\n",
    "    * `'I am sad'`\n",
    "* If we were to compute the similarities, `'I am happy'` and `'I am joyous'` would have the same score as `'I am happy'` and `'I am sad'`... regardless of how we vectorize it.\n",
    "    * This is because `happy`, `joyous`, and `sad` are considered to be completely different words. \n",
    "* The *meaning* of the words is something that the vectorization techniques that we've covered so far simply cannot capture\n",
    "\n",
    "### Word embeddings\n",
    "* Mapping words into an n-dimensional vector space\n",
    "* Produced using deep learning and huge amounts of data\n",
    "* Once generated, these vectors can be used to discern how similar two words are to each other\n",
    "* Used to detect synonyms and antonyms\n",
    "* Captures complex relationships\n",
    "    * `King` : `Queen` $\\Rightarrow$ `Man` : `Woman`\n",
    "    * `France` : `Paris` $\\Rightarrow$ `Russia` : `Moscow`\n",
    "* **Note** that word embeddings are not trained on user data; they are trained on the pre-trained spacy model you're using and are independent on the size (and contents) of your dataset\n",
    "\n",
    "#### Word embeddings using spaCy\n",
    "* **Note** that it is advisable to load larger spacy models while working with word vectors\n",
    "* This is because the `en_core_web_sm` model does not technically ship with word vector but context specific tensors, which tend to give relatively poorer results\n",
    "\n",
    "```\n",
    "import spacy\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp('I am happy')\n",
    "\n",
    "# Generate word vectors for each token\n",
    "for token in doc:\n",
    "    print(token.vector)\n",
    "```\n",
    "\n",
    "#### Word similarities\n",
    "\n",
    "```\n",
    "doc = nlp(\"happy joyous sad\")\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "```\n",
    "\n",
    "#### Document similarities\n",
    "\n",
    "```\n",
    "# Generate doc objects\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")\n",
    "\n",
    "# Compute similarity between sent1 and sent2\n",
    "sent1.similarity(sent2)\n",
    "\n",
    "# Compute similarity between sent1 and sent3\n",
    "sent1.similarity(sent3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de192371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy 1.0\n",
      "happy joyous 0.5097088\n",
      "happy sad 0.20651372\n",
      "joyous happy 0.5097088\n",
      "joyous joyous 1.0\n",
      "joyous sad 0.39810386\n",
      "sad happy 0.20651372\n",
      "sad joyous 0.39810386\n",
      "sad sad 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-a368ac3982e1>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(token1.text, token2.text, token1.similarity(token2))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"happy joyous sad\")\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bb28216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8965221275043582\n",
      "0.9127636276495428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-0b499feff541>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(sent1.similarity(sent2))\n",
      "<ipython-input-59-0b499feff541>:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(sent1.similarity(sent3))\n"
     ]
    }
   ],
   "source": [
    "# Generate doc objects\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")\n",
    "\n",
    "# Compute similarity between sent1 and sent2\n",
    "print(sent1.similarity(sent2))\n",
    "\n",
    "# Compute similarity between sent1 and sent3\n",
    "print(sent1.similarity(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21bf7c",
   "metadata": {},
   "source": [
    "#### Exercises: Generating word vectors\n",
    "\n",
    "```\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))\n",
    "```\n",
    "***\n",
    "\n",
    "#### Exercises: Computing similarity of Pink Floyd songs\n",
    "\n",
    "```\n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
