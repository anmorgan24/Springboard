{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7598caa0",
   "metadata": {},
   "source": [
    "Note: This notebook was completed as part of DataCamp's course of the same name.\n",
    "\n",
    "# Feature Engineering for NLP in Python\n",
    "In this course, you will learn techniques that will allow you to extract useful information from text and process them into a format suitable for applying ML models. More specifically, you will learn about POS tagging, named entity recognition, readability scores, the n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. You will also learn to compute how similar two documents are to each other. In the process, you will predict the sentiment of movie reviews and build movie and Ted Talk recommenders. Following the course, you will be able to engineer critical features out of any text and solve some of the most challenging problems in data science!\n",
    "\n",
    "**Instructor:** Rounak Banik, Data Scientist at Fractal Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb9f8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textatistic import Textatistic\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51a2b5",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Basic features and readability scores\n",
    "Learn to compute basic features such as number of words, number of characters, average word length and number of special characters (such as Twitter hashtags and mentions). You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62716fc7",
   "metadata": {},
   "source": [
    "### Introduction to NLP feature engineering\n",
    "* Learn to extract useful features out of text and convert them into formats that are suitable for machine learning algorithms\n",
    "* Recall that for any ML algorithm, data fed into it must be in tabular form and all the training features must be numerical\n",
    "* ML algorithms can also work with categorical data provided the categories are converted into numerical form through one-hot-encoding.\n",
    "\n",
    "#### One-hot encoding with pandas\n",
    "\n",
    "```\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "df = pd.get_dummies(df, columns=['sex'])\n",
    "```\n",
    "* **Note** that *not* mentioning columns will lead pandas to automatically encde all non-numerical features\n",
    "* Consider the following movie reviews dataset:\n",
    "\n",
    "<img src='data/mov_rev_data.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09163b2",
   "metadata": {},
   "source": [
    "* The above data cannot be utilized by any ML algorithm\n",
    "* The training feature `review` is not numerical\n",
    "    * Neither is it categorical to perform one-hot encoding on\n",
    "    \n",
    "#### Text pre-processing\n",
    "* We need to perform two steps to make this dataset suitable for ML:\n",
    "    * 1) **Standardize the text:**\n",
    "        * converting words to lowercase\n",
    "        * lemmatization/ converting to root form\n",
    "        * example: `Reduction` gets converted to `reduce`\n",
    "    * 2) **Vectorization:**\n",
    "        * After standardization, the reviews are converted into a set of numerical training features through a process known as **vectorization**.\n",
    "        * After vectoriztion, our original review dataset gets converted into something like this:\n",
    "        \n",
    "<img src='data/vect_ex.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682bf10",
   "metadata": {},
   "source": [
    "* We will learn techniques to achieve this in later lessons\n",
    "\n",
    "#### Basic features\n",
    "* We can alo extract certain basic features from text like:\n",
    "    * **word count**\n",
    "    * **character count**\n",
    "    * **average word length**\n",
    "* When working with niche data, such as tweets, it also may be useful to know how many hashtags have been used in a given tweet\n",
    "\n",
    "#### POS tagging\n",
    "* Some NLP applications may require you to extract features for individual words\n",
    "* For instance, you may want to do **parts-of-speech** or **POS** tagging to know the different parts-of-speech present in your text as shown:\n",
    "\n",
    "<img src='data/POS_tagging.png' width=\"150\" height=\"75\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b91e3",
   "metadata": {},
   "source": [
    "* Consider the example above; POS tagging will label each word with its corresponding part-of-speech\n",
    "\n",
    "#### Named Entity Recognition (NER)\n",
    "* You may also want to perform named entity recognition to find out if a particular noun is referring to a person, organization or country\n",
    "* Does noun refer to person, orginazion, or country (or other)?\n",
    "\n",
    "#### Concepts covered\n",
    "* Text preprocessing\n",
    "* Basic features \n",
    "* Word features\n",
    "* Vectorization\n",
    "\n",
    "#### Exercises: One-hot-encoding\n",
    "\n",
    "```\n",
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fdaba",
   "metadata": {},
   "source": [
    "## Basic feature extraction\n",
    "* While not very powerful, basic features can give us a good idea of the text we are dealing with\n",
    "* The most basic feature we can extract from text is **number of characters** (including whitespaces)\n",
    "\n",
    "### Number of characters\n",
    "* The most basic feature we can extract from text\n",
    "* **Includes whitespaces**\n",
    "* For exapmle, the string `I don't know.` has **13 characters**.\n",
    "* The number of characters is the length of the string, or: `len(string)`\n",
    "* If our dataframe `df` has a textual feature (say `review`), we can compute the number of characters for each review and store it as a new feature `num_chars` by using the pandas dataframe `apply()` method:\n",
    "    * **`df['num_chars'] = df['review'].apply(len)`**\n",
    "\n",
    "### Number of words\n",
    "* Assuming that every word is separated by a space, we can use a string's `split()` method to convert it into a list where every element is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6b386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'had', 'a', 'little', 'lamb.']\n"
     ]
    }
   ],
   "source": [
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "\n",
    "# Print the list containing words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7942e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Print number of words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876bf36",
   "metadata": {},
   "source": [
    "* To do this for a textual feature in a dataframe, we first define a function that takes in a string as an argument and returns the number of words in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2509e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in string\n",
    "def word_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return length of words list\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880dbdd",
   "metadata": {},
   "source": [
    "* We can now pass this function, `word_count()` to `apply()` and create `df['num_words']`:\n",
    "\n",
    "```\n",
    "# Create num_words feature in df\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "```\n",
    "\n",
    "### Average word length\n",
    "* Let's define a function `avg_word_length()` which takes in a string and returns the average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399f7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns average word length\n",
    "def avg_word_length(x):\n",
    "    # Split the string into words\n",
    "    words = x.split()\n",
    "    # Compute length of each word and store in a separate list\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    # Compute average word length\n",
    "    avg_word_length = sum(word_lengths)/len(words)\n",
    "    # Return average word length\n",
    "    return(avg_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2753fc",
   "metadata": {},
   "source": [
    "* We can now pass this function (`avg_word_length()`) into `apply()` to generate an average word length feature in the df\n",
    "\n",
    "```\n",
    "# Create a new feature avg_word_length\n",
    "df['avg_word_length'] = df['review'].apply(doc_density)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0a569",
   "metadata": {},
   "source": [
    "### Special features\n",
    "* When working with data such as tweets, it may be useful to compute the number of hashtags or mentions used.\n",
    "\n",
    "### Hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe4cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of hashtags\n",
    "def hashtag_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Create a list of hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    # Return number of hashtags\n",
    "    return len(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c6ea6",
   "metadata": {},
   "source": [
    "* The procedure to compute number or mentions is identical except that we check if a word starts with `@` instead of `#`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6434552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of mentions\n",
    "def mention_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Create a list of mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    # Return number of mentions\n",
    "    return len(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65427a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "289087f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81094b1",
   "metadata": {},
   "source": [
    "#### Other features\n",
    "* There are other basic features we can compute such as:\n",
    "    * Number of sentences \n",
    "    * Number of paragraphs\n",
    "    * Number of words starting with an uppercase\n",
    "    * All-capital words\n",
    "    * Numeric quantities\n",
    "    * etc. ...\n",
    "* The procedure to extract the above features is extremely similar to the ones we've already covered\n",
    "\n",
    "#### Exercises: Character count of Russian tweets\n",
    "\n",
    "```\n",
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())\n",
    "```\n",
    "\n",
    "#### Exercises: Word count of TED talks\n",
    "\n",
    "```\n",
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())\n",
    "```\n",
    "\n",
    "#### Hashtags and mentions in Russian tweets\n",
    "\n",
    "```\n",
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12a315",
   "metadata": {},
   "source": [
    "### Readability tests\n",
    "* Here we will look at a set of interesting features known as **readability tests**, which are used to determine the readability of a particular passage (in English)\n",
    "* In other words, it indicates at what educational level a person needs to be, in order to comprehend a particular piece of text\n",
    "* The scale usually ranges from **primary school** up to **college graduate level** and is in context of the American education system\n",
    "* **Readability tests** are done using a mathematical formula that utilizes the word, syllable, and sentence count of the passage.\n",
    "* Readability tests are routinely used by organizations to determine how difficult their publications are to understand (or not).\n",
    "* Readability tests have also found applications in domains such as **fake news**, and **opinion spam detection**.\n",
    "* There are a variety of readability tests in use\n",
    "\n",
    "#### Readability text examples\n",
    "* Some common examples:\n",
    "    * **Flesch reading ease**\n",
    "    * **Gunning fog index**\n",
    "    * **Simple Measure of Gobbledygook (SMOG)**\n",
    "    * **Dale-Chall score**\n",
    "* $\\star$ **Note** that all of these tests are used for texts in **English**\n",
    "* Tests for other languages also exist that take into consideration the nuances of that particular language\n",
    "* In this lesson, we will cover the first two scores (Flesch reading ease and Gunning fog index) in detail\n",
    "    * However, once you understand these two, you will be in a good position to understand and use the other scores as well.\n",
    "    \n",
    "### Flesch reading ease\n",
    "* The Flesch Reading Ease is one of the **oldest** and **most widely used** readability tests\n",
    "* Dependent on two factors:\n",
    "    * **1) The greater the average sentence length, the harder a text is to read.**\n",
    "    * **2) The greater the average number of syllables in a word, the harder a text is to read.**\n",
    "* The higher the Flesch Reading Ease score, the greater is the readability \n",
    "    * A higher score indicates that the text is easier to understand\n",
    "* **Higher the score, greater the readability**\n",
    "    \n",
    "<img src='data/flesch_scores.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "### Gunning fog index\n",
    "* Developed in 1954\n",
    "* Dependent on:\n",
    "    * **1) Average sentence length**\n",
    "    * **2) The greater the percentage of complex words, the harder the text is to read.**\n",
    "            * Here, \"complex words\" refer to all words that have three or more syllables\n",
    "* Unlike Flesch, the formula for Gunning fog index is such that the higher the score, the more difficult the passage is to understand\n",
    "* **Higher the index, lesser the readability.**\n",
    "\n",
    "<img src='data/gunning_scores.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94817f21",
   "metadata": {},
   "source": [
    "### The textatistic library \n",
    "* We can conduct these readability tests in Python using the Textatistic library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4907a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textatistic\n",
      "  Downloading textatistic-0.0.1.tar.gz (29 kB)\n",
      "Collecting pyhyphen>=2.0.5\n",
      "  Downloading PyHyphen-4.0.3-cp37-abi3-macosx_10_14_x86_64.whl (37 kB)\n",
      "Requirement already satisfied: setuptools>=52.0 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests>=2.25 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (2.25.1)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (1.4.4)\n",
      "Requirement already satisfied: wheel>=0.36.0 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from pyhyphen>=2.0.5->textatistic) (0.36.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (4.0.0)\n",
      "Building wheels for collected packages: textatistic\n",
      "  Building wheel for textatistic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for textatistic: filename=textatistic-0.0.1-py3-none-any.whl size=29056 sha256=48e2605433b68a36e2057c6e5bb2e18dd29014aa948312c62909c461ee3a9758\n",
      "  Stored in directory: /Users/abigailmorgan/Library/Caches/pip/wheels/82/24/c4/de7882083c3530984f6eda43ae9e94875c84d906063ef10bcb\n",
      "Successfully built textatistic\n",
      "Installing collected packages: pyhyphen, textatistic\n",
      "Successfully installed pyhyphen-4.0.3 textatistic-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6c3a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary had a little lamb.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bfc484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.24000000000002\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Create a Textatistic Object\n",
    "readability_scores = Textatistic(text).scores\n",
    "\n",
    "# Generate scores\n",
    "print(readability_scores['flesch_score'])\n",
    "print(readability_scores['gunningfog_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4838e",
   "metadata": {},
   "source": [
    "#### Exercises: Readability of 'The Myth of Sisyphus'\n",
    "\n",
    "```\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "```\n",
    "\n",
    "#### Exercises: Readability of various publications\n",
    "\n",
    "```\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2711e6",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Text preprocessing, POS tagging and NER\n",
    "In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8f933",
   "metadata": {},
   "source": [
    "### Tokenization and Lemmatization\n",
    "#### Text sources (all very different styles, grammars, and vocab)\n",
    "* News articles \n",
    "* Tweets\n",
    "* Social media comments\n",
    "\n",
    "#### Making text machine friendly\n",
    "* It is important to standardize all of these (above) texts into a machine-friendly format\n",
    "* We want our models to treat similar words as the same\n",
    "\n",
    "### Text preprocessing techniques\n",
    "* The text processing techniques you use are dependent on the application you're working on\n",
    "* Some of the common ones we'll be covering include:\n",
    "    * Covnverting words into lowercase\n",
    "    * Removing leading and trailing whitespaces\n",
    "    * Removing punctuation\n",
    "    * Removing commonly occuring words (**stopwords**)\n",
    "    * Expanding contractions\n",
    "    * Removing special characters (numbers, emojis, etc)\n",
    "    \n",
    "### Tokenization\n",
    "* **Tokenization** is the process of splitting a string into its constituent tokens\n",
    "* These tokens may be sentences, words, or punctuations and *are specific to a particular language.*\n",
    "* In this course, we will be primarily focused with word and punctuation tokens\n",
    "* Tokenization also involves **expanding contracted words.**\n",
    "\n",
    "#### Tokenization using spaCy\n",
    "* We load a pre-trained English model, `en_core_web_sm` using `spacy.load()`\n",
    "    * This will return a language object that has the know-how to perform tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27d0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4005a",
   "metadata": {},
   "source": [
    "* The `doc` object defined above contains the required tokens (and many other things, as we will soon find out).\n",
    "* We generate the list of tokens by using list comprehension as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d6b64fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee92926",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "* **Lemmatization** is the process of converting a word into its lowercased base form, or **lemma**.\n",
    "* This is an extremely powerful process of standardization\n",
    "* Examples:\n",
    "    * `reducing`, `reduces`, `reduced`, `reduction` $\\Rightarrow$ $\\Rightarrow$ **`reduce`**\n",
    "    * `am`, `are`, `is` $\\Rightarrow$ $\\Rightarrow$ **`be`**\n",
    "    * `n't` $\\Rightarrow$ $\\Rightarrow$ **`not`**\n",
    "    * `'ve` $\\Rightarrow$ $\\Rightarrow$ **`have`**\n",
    "* When you pass the string into `nlp`, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens, except that we extract `token.lemma_` in each iteration inside the list comprehension instead of `token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24149f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a91d0",
   "metadata": {},
   "source": [
    "* **Also note that spaCy converted `I`s into `-PRON-`**; this is standard behavior, where every pronoun is converted into the string `-PRON-`\n",
    "\n",
    "#### Exercises: Tokenizing the Gettysburg Address\n",
    "\n",
    "```\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "#### Exercises: Lemmatizing the Gettysburg address\n",
    "\n",
    "```\n",
    "print(gettysburg)\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad033a",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "\n",
    "#### Text cleaning techniques\n",
    "* Unnecessary whitespaces and escape sequences\n",
    "* Punctuations\n",
    "* Special characters (numbers, emojis, etc.)\n",
    "* Stopwords\n",
    "\n",
    "\n",
    "* In other words, it is very common to remove non-alphabetic tokens and words that occur so commonly that they are not very useful for analyis\n",
    "\n",
    "#### isalpha()\n",
    "* Every Python string has an **`isalpha()`** method that returns `True` if all the characters of the string are alphabetic\n",
    "* This is an extremely convenient method to remove all (lemmatized) tokens that are or that contain numbers, punctuation and emojis\n",
    "* **A word of caution:** `isalpha()` has a tendency of returning false on words we would not want to remove. Examples:\n",
    "    * Abbreviations: `U.S.A.`, `U.K.`, etc\n",
    "    * Proper Nounds with numbers in them: `word2vec` and `xto10x`\n",
    "    * For such nuanced cases, `isalpha()` may not be sufficient and it may be advisable to write your own custom functions.\n",
    "    * Write your own custome functions (typically using regex) for the more nuanced cases \n",
    "\n",
    "#### Removing non-alphabetic characters\n",
    "* First, we generate the lemmatized tokens like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "358dc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "\n",
    "# Generate list of tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698927e",
   "metadata": {},
   "source": [
    "* Next, we loop through the tokens again and choose only those words that are either `-PRON-` or contain only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b674a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG this be like the good thing ever wow such an amazing song -PRON- be hooked Top definitely\n"
     ]
    }
   ],
   "source": [
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma =='-PRON-']\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dfc069",
   "metadata": {},
   "source": [
    "* Make lower case (in video this was done automatically with above code, not sure why it didn't here, so I'm lower-casing it in a separate call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3e9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_lemmas = []\n",
    "for lemma in a_lemmas:\n",
    "    al_lemmas.append(lemma.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aa6e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', 'this', 'be', 'like', 'the', 'good', 'thing', 'ever', 'wow', 'such', 'an', 'amazing', 'song', '-pron-', 'be', 'hooked', 'top', 'definitely']\n"
     ]
    }
   ],
   "source": [
    "print(al_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab24c9",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "* There are some words in the English language that occur so commonly that it is often a good idea to just ignore them\n",
    "* Examples: \n",
    "    * articles: \n",
    "        * a\n",
    "        * the\n",
    "    * be verbs:\n",
    "        * is\n",
    "        * am\n",
    "    * pronouns:\n",
    "        * he\n",
    "        * she\n",
    "        * they\n",
    "* **`spaCy` has a built-in list of stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad52028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88505ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG like good thing wow amazing song hooked Top definitely\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a106650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_lemmas = []\n",
    "for lemma in a_lemmas:\n",
    "    al_lemmas.append(lemma.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b351e1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg like good thing wow amazing song hooked top definitely\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(al_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba864b",
   "metadata": {},
   "source": [
    "* **Notice** that we have removed the `-PRON-` condition as pronouns are stopwords anyway and should be removed\n",
    "* Additionally, we have introduced a new condition to check if the word belongs to spacy's list of stopwords\n",
    "* **Notice also** how the string consists only of base form words\n",
    "* **Always** exercise caution whil using third party stopword lists\n",
    "    * It is common that an application find certain words useful that may be consideed a stopword by third party lists\n",
    "    * **It is often advisable to create your own custom stopword lists**\n",
    "    \n",
    "#### Other text preprocessing techniques\n",
    "* There are other preprocessing techniques that are used but have been omitted for the sake of brevity\n",
    "* Some of them include:\n",
    "    * **Removing HTML or XML tags**\n",
    "    * **Replacing accented characters**\n",
    "    * **Correcting spelling errors and shorthands**\n",
    "    \n",
    "    \n",
    "* **A word of caution:** the text preprocessing techniques you use are always dependent on the application\n",
    "* There are many applications which may find punctuations, numbers, and emojis useful, so in these cases it may not be wise to remove them\n",
    "* **Always use only those text preprocessing techniques that are relevant to your application.**\n",
    "\n",
    "#### Exercises: Cleaning a blog post\n",
    "\n",
    "```\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "```\n",
    "\n",
    "#### Exercises: Cleaning TED talks in a dataframe\n",
    "\n",
    "```\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eed2761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107a263",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "* Part-of-speech tagging (or **POS tagging**) is one of the most popularly used feature engineering techniques in NLP\n",
    "\n",
    "#### Applications\n",
    "* **Word-sense disambiguation:**\n",
    "    * `\"The bear is a majestic animal\"`\n",
    "    * `\"Please bear with me\"`\n",
    "* **Sentiment analysis**\n",
    "* **Question answering systems**\n",
    "* **Fake news and opinion spam detection** (linguistic approaches)\n",
    "    * For example, one paper discovered that fake news headlines, on average, tend to use less common nouns and more proper nouns than mainstream headlines\n",
    "    * Generating the POS tags for these words proved extremely useful in detecting false or hyperpartisan news\n",
    "    \n",
    "#### POS tagging using spaCy\n",
    "* **POS tagging** is the process of assigning every word (or token) in a piece of text, its corresponding part of speech.\n",
    "* Performing POS tagging with spaCy is almost identical to generating tokens or lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09e21a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4056b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize string\n",
    "string = \"Jane is an amazing guitarist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "912d9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3211b3c",
   "metadata": {},
   "source": [
    "Using list comprehension, the first element of the tuple is the token and is generated using `token.text` and `token.pos_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "306ceb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jane', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('guitarist', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fc2e9",
   "metadata": {},
   "source": [
    "* SpaCy infers the POS tags of these words based on the predictions given by its pre-trained models.\n",
    "* In other words, **the accuracy of the POS tagging is dependent on the data that the model has been trained on and the data that it is being used on.**\n",
    "\n",
    "#### POS annotations in spaCy\n",
    "* spaCy is capable of identifying close to 20 parts-of-speech and it uses specific annotations to denote a particular part of speech\n",
    "* complete spaCy annotation list [HERE](https://spacy.io/api/annotation)\n",
    "\n",
    "<img src='data/POS_annot.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada7484",
   "metadata": {},
   "source": [
    "#### POS tagging in Lord of the Flies\n",
    "\n",
    "```\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "```\n",
    "\n",
    "#### Exercises: Counting nouns in a piece of text\n",
    "In this exercise, we will write two functions, `nouns()` and `proper_nouns()` that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news.\n",
    "\n",
    "```\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1aa9f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "      # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "\n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e92a39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "      # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "\n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a06d3",
   "metadata": {},
   "source": [
    "#### Exercises: Noun usage in fake news\n",
    "\n",
    "```\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6f09d",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "* **Named entity recognition** or **NER** has a host of extremely useful applications\n",
    "\n",
    "#### Applications\n",
    "* Efficient search algorithms \n",
    "* Question answering systems\n",
    "* News article classification\n",
    "* Customer service centers (to classify and record complaints efficiently)\n",
    "\n",
    "#### Named entity recognition\n",
    "* A **named entity** is anything that can be denoted with a proper name or a proper noun. \n",
    "* **NER** is the process of identifying such named entities in a piece of text and classifying them into predefined categories\n",
    "* Categories include person, organization, country, etc.\n",
    "\n",
    "#### NER using spaCy\n",
    "* Performing NER is extremely easy using spaCy's pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51ce30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"John Doe is a software engineer working at Google. He lives in France.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8349051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "475707a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10057e05",
   "metadata": {},
   "source": [
    "* Note that `GPE` is \"Geopolitical Entity\"\n",
    "* Currently spaCy's models are capable of identifyin more than 15 different types \n",
    "* Find [complete list here](https://spacy.io/api/annotation#named-entities)\n",
    "* Below is a small snapshot:\n",
    "\n",
    "<img src='data/NER_annote.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb229a53",
   "metadata": {},
   "source": [
    "* **Word of caution** if we are trying to extract named entities for texts from a heavily technical field (such as medicine), spaCy's pretrained models may not perform very well.\n",
    "* In such nuances cases, it is better to train your own models with your specialized data.\n",
    "* Also remember that spacy's models are **language specific**\n",
    "\n",
    "#### Exercises: Named entities in a sentence\n",
    "\n",
    "```\n",
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "#### Exercises: Identifying people mentioned in a news article\n",
    "\n",
    "```\n",
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc08da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e8e23",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: N-Gram models\n",
    "Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d44b7d",
   "metadata": {},
   "source": [
    "### Building a bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355ab1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e26f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8838065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f54e39",
   "metadata": {},
   "source": [
    "<img src='data/NER_example.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
