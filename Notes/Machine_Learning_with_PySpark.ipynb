{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17222b62",
   "metadata": {},
   "source": [
    "Note: The notes contained in this notebook were taken from DataCamp's \"Machine Learning with PySpark\"\n",
    "# Machine Learning with PySpark\n",
    "Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!\n",
    "\n",
    "**Instructor:** Andrew Collier, Data Scientist @ Exegetic Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61c488",
   "metadata": {},
   "source": [
    "#### First, some recap:\n",
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
    "\n",
    "However, with greater computing power comes greater complexity.\n",
    "\n",
    "Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n",
    "    * Is my data too big to work with on a single machine?\n",
    "    * Can my calculations be easily parallelized?\n",
    "    \n",
    "The first step in using Spark is connecting to a cluster.\n",
    "\n",
    "In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n",
    "\n",
    "When you're just getting started with Spark it's simpler to just run a cluster locally. Thus, for this course, instead of connecting to another computer, all computations will be run on DataCamp's servers in a simulated cluster.\n",
    "\n",
    "Creating the connection is as simple as creating an instance of the `SparkContext` class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you're connecting to.\n",
    "\n",
    "An object holding all these attributes can be created with the `SparkConf()` constructor. Take a look at the [documentation](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html) for all the details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187d558",
   "metadata": {},
   "source": [
    "#### Using DataFrames\n",
    "Spark's core data structure is the **Resilient Distributed Dataset (RDD)**. This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface with that connection.\n",
    "\n",
    "#### Put some Spark in your data\n",
    "In the last exercise, you saw how to move data from Spark to `pandas`. However, maybe you want to go the other direction, and put a `pandas` DataFrame into a Spark cluster! The `SparkSession` class has a method for this as well.\n",
    "\n",
    "The `.createDataFrame()` method takes a `pandas` DataFrame and returns a Spark DataFrame.\n",
    "\n",
    "The output of this method is stored locally, not in the `SparkSession` catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the `.sql()` method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table.\n",
    "\n",
    "You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific `SparkSession` used to create the Spark DataFrame.\n",
    "\n",
    "There is also the method `.createOrReplaceTempView()`. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables.\n",
    "\n",
    "Check out the diagram to see all the different ways your Spark data structures interact with each other.\n",
    "\n",
    "<img src='data/spark_createTempView.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370ffd9",
   "metadata": {},
   "source": [
    "* Similar to `.withColumn()`, you can do column-wise computations within a `SELECT` statement. \n",
    "* `SELECT origin, dest, air_time / 60 FROM flights;`\n",
    "* the following two expressions will produce the same output:\n",
    "* `flights.filter(\"air_time > 120\").show()`\n",
    "* `flights.filter(flights.air_time > 120).show()`\n",
    "* The difference between `.select()` and `.withColumn()` methods is that `.select()` returns only the columns you specify, while .`withColumn()` returns all the columns of the DataFrame in addition to the one you defined. \n",
    "* At the core of the `pyspark.ml` module are the `Transformer` and `Estimator` classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
    "* `Transformer` classes have a `.transform()` method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class `Bucketizer` to create discrete bins from a continuous feature or the class `PCA` to reduce the dimensionality of your dataset using principal component analysis.\n",
    "* `Estimator` classes all implement a `.fit()` method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a `StringIndexerModel` for including categorical data saved as strings in your models, or a `RandomForestModel` that uses the random forest algorithm for classification or regression.\n",
    "* Before you get started modeling, it's important to know that Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals (called 'doubles' in Spark).\n",
    "* To remedy this, you can use the `.cast()` method in combination with the `.withColumn()` method. It's important to note that `.cast()` works on columns, while `.withColumn()` works on DataFrames.\n",
    "* In Spark it's important to make sure you split the data **after** all the transformations. This is because operations like `StringIndexer` don't always produce the same index even when given the same list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e55649",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Introduction\n",
    "\n",
    "## Machine Learning and Spark\n",
    "Spark is a framework for working with Big Data. In this chapter you'll cover some background about Spark and Machine Learning. You'll then find out how to connect to Spark using Python and load CSV data.\n",
    "\n",
    "Here, we'll learn how to build Machine Learning models on large data sets using distributed computing techniques\n",
    "\n",
    "* The performance of an ML depends on data; in general, more data is a good thing\n",
    "* If the data can fit entirely in RAM then the algorithm can operate efficiently\n",
    "* When the data no longer fit into memory, the computer will start to use **virutal memory** and the data will be **paged** back and forth between RAM and disk\n",
    "    * Relative to **RAM** access, retrieving data from disk is slow\n",
    "    * And as the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets.\n",
    "\n",
    "<img src='data/datasize_RAM.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* One option is to **distribute the problem across multiple computers in a cluster;** rather than trying to handle a large dataset on a single machine, it's divided up into partitions which are processed separately.\n",
    "    * Ideally each data partition can fit into RAM on a single computer in the cluster.\n",
    "    * This is the approach used by **Spark**\n",
    "    \n",
    "#### What is Spark?\n",
    "* Compute across a distributed cluster\n",
    "* Data processing in memory\n",
    "* Well-documented high-level API\n",
    "* **It is generally much faster than other Big Data technologies like Hadoop, because it does most processing in memory .**\n",
    "* **It has a developer-friendly interface which hides much of the complexity of distributed computing.**\n",
    "\n",
    "#### Cluster components\n",
    "* A cluster consists of one or more **nodes**\n",
    "* Each **node** is a computer with CPU, RAM, and physical storage\n",
    "* A **cluster manager** allocates resources and coordinates activity across the cluster\n",
    "* Every application running on the Spark cluster has a **driver** program\n",
    "* Using the **Spark API**, the driver communicates with the cluster manager, which in turn distributes work to the nodes.\n",
    "* On each node, Spark launches an **executor** process which persists for the duration of the application\n",
    "* Work is divided up into **tasks**, which are simply units of computation\n",
    "* The executors run tasks in multiple **threads** across the **cores** in a node\n",
    "\n",
    "<img src='data/cluster_structure.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b986ee1",
   "metadata": {},
   "source": [
    "#### Interacting with Spark\n",
    "* Languages for interacting with Spark:\n",
    "    * Java: low-level, compiled\n",
    "    * Scala, Python and R: high-level and interactive REPL (Read-Eval-Print-Loop, which is crucial for **interactive development**)\n",
    "    \n",
    "#### Importing pyspark\n",
    "* Python doesn't \"speak\" natively with spark\n",
    "* From Python import the `pyspark` module first, which makes the Spark functionality available in the Python interpreter\n",
    "* Spark is under vigorous development and because it is constantly evolving, it is import to check your version before getting started \n",
    "* In this course we'll be using version `2.4.1` (released March 2019)\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "pyspark.__version__\n",
    "```\n",
    "\n",
    "#### Sub-modules\n",
    "* In addition to `pyspark`, there are:\n",
    "    * Structured Data -- `pyspark.sql`\n",
    "    * Streaming Data -- `pyspark.streaming`\n",
    "    * Machine Learning -- `pyspark.mllib` (deprecated) and **`pyspark.ml`**\n",
    "* With the `pyspark` module loaded, you're able to connect to Spark. The next thing you need to do is **tell Spark where the cluster is located.** Two options:\n",
    "\n",
    "#### Remote Cluster\n",
    "   * Connect to a **Remote Cluster** using Spark URL:\n",
    "        * `spark://<IP address | DNS name>:<port>`\n",
    "        * *Example with IP:* `spark://13.59.151.161:7077`\n",
    "        * *Example with DNS:* `spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077`\n",
    "        * The Spark URL gives the location of the cluster's master node\n",
    "        * The URL is composed of an **IP address or DNS name** and a **port number**\n",
    "        * The **default port** for Spark is 7077 (but this must still be explicitly specified\n",
    "        \n",
    "#### Local Cluster\n",
    "* When you're figuring out how Spark works, the infrastructure of a distributed network can get in the way\n",
    "* For this reason it may be helpful to create a **local cluster**, where everything happens on a single computer\n",
    "    * This is the setup that you're going to use throughout this course\n",
    "* For a local cluster, you need only specify \"local\" and, optionally, the number of cores to use.\n",
    "    * *Examples:*\n",
    "        * `local` -- only 1 core;\n",
    "        * `local[4]` -- 4 cores; or\n",
    "        * `local[*]` -- all available cores.\n",
    "* By **default** a local cluster will run on a single core.\n",
    "\n",
    "### Creating a SparkSession\n",
    "* You connect to Spark by creating a `SparkSession` object\n",
    "* You then specify the **location** of the cluster using the **`master()`** method:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]')\n",
    "                    .appName('first_spark_application') \\\n",
    "                    .getOrCreate()\n",
    "```\n",
    "* Or: `spark = SparkSession.builder.master('local[*]').appName('first_spark_application').getOrCreate()`\n",
    "\n",
    "\n",
    "* Optionally, you can also assign a name to the application using the `appName()` method\n",
    "* Finally, we call the `getOrCreate()` method, which will either create a new session object or return an existing object.\n",
    "* *Once the session has been created, you are able to interact with Spark.*\n",
    "* Although it's possible for multiple SparkSessions to co-exist, it's good practice to stop the SparkSession when you're done. \n",
    "* `# Close connection to Spark`\n",
    "* **`>>> spark.stop()`**\n",
    "* For more info on: [SparkSession](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbe23e",
   "metadata": {},
   "source": [
    "#### Creating a SparkSession\n",
    "* For more info on: [SparkSession](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n",
    "* The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "    * Specify the location of the master node;\n",
    "    * Name the application (optional); and\n",
    "    * Retrieve an existing SparkSession or, if there is none, create a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5b658",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "* Selected methods:\n",
    "    * `count()` : returns number of rows \n",
    "    * `show()` : displays a subset of rows\n",
    "    * `printSchema()` : column types\n",
    "* Selected attributes:\n",
    "    * `dtypes` : column types\n",
    "    \n",
    "#### Reading data from CSV\n",
    "* The `.csv()` method reads a CSV file and returns a `DataFrame`\n",
    "* `cars = spark.read.csv('cars.csv', header=True)`\n",
    "* **Optional arguments:**\n",
    "    * **`header`:** is first row a header? (default: `False`)\n",
    "    * **`sep`:** field separator (default: a comma `','`)\n",
    "    * **`schema`:** explicit column data types\n",
    "    * **`inferSchema`:** deduce column data types from data?\n",
    "    * **`nullValue`:** placeholder for missing data; *case-sensitive*\n",
    "    \n",
    "#### Check column types\n",
    "* `cars.printSchema()`\n",
    "* **The `.csv()` method treats all columns as strings by default.** Or, \n",
    "    * **(1)** Infer the columns from the data:\n",
    "        * `cars = spark.read.csv('cars.csv', header=True, inferSchema=True)`\n",
    "        * In this scenario, Spark needs to make an extra pass over the data to figure out the column types before reading the data\n",
    "        * Con: if the data file is big, this will notably increase the load time\n",
    "        * Con: While usually accurate, there may also misidentified types\n",
    "        * Con: interprets NA as a string (and therefore columns with NA as string types)\n",
    "    * **(2)** Manually specify the types\n",
    "    \n",
    "#### Manually specifying column types\n",
    "* Manually specify the type of each column in an explicit schema\n",
    "* During this process, it is also possible to choose alternative column names\n",
    "\n",
    "```\n",
    "schema = StructType([\n",
    "                StructField(\"maker\", StringType()),\n",
    "                StructField(\"model\", StringType()),\n",
    "                StructField(\"origin\", StringType()),\n",
    "                StructField(\"type\", StringType()),\n",
    "                StructField(\"cyl\", StringType()),\n",
    "                StructField(\"size\", StringType()),\n",
    "                StructField(\"weight\", StringType()),\n",
    "                StructField(\"length\", StringType()),\n",
    "                StructField(\"rpm\", StringType()),\n",
    "                StructField(\"consumption\", StringType())\n",
    "])\n",
    "cars = spark.read.csv(\"cars.csv\", header = True, schema = schema, nullValue = 'NA')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca95ac",
   "metadata": {},
   "source": [
    "#### Exercises: Loading flights data\n",
    "\n",
    "```\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)\n",
    "```\n",
    "\n",
    "#### Exercises: Loading SMS spam data\n",
    "\n",
    "```\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e719d",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Classification\n",
    "Now that you are familiar with getting data into Spark, you'll move onto building two types of classification model: Decision Trees and Logistic Regression. You'll also find out about a few approaches to data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656bc59",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "#### Dropping columns\n",
    "* There are two approaches:\n",
    "    * Drop the columns you don't want; or,\n",
    "    * Select the fields you do want\n",
    "\n",
    "```\n",
    "# Drop the columns you don't want\n",
    "cars = cars.drop('maker', 'model')\n",
    " \n",
    " \n",
    "# Select the columns you do want\n",
    "cars = cars.select(\"origin\", \"type\", \"cyl\", \"size\", \"weight\", \"length\", \"rpm\", \"consumption\")\n",
    "```\n",
    "\n",
    "#### Filtering out missing data\n",
    "* Use the `.filter()` method and provide a **logical predicate** using **SQL syntax** that identifies NULL values:\n",
    "\n",
    "```\n",
    "# How many missing values?\n",
    "cars = filter('cyl IS NULL').count()\n",
    "\n",
    "# Drop records with missing values in the `cylinders` column\n",
    "cars = cars.filter('cyl IS NOT NULL')\n",
    "\n",
    "# Drop records with missing values in any column\n",
    "cars = cars.dropna()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6722b",
   "metadata": {},
   "source": [
    "#### Mutating Columns \n",
    "* Use the `.withColumn()` method to create a new mass column in units of kilograms\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create a new \"mass\" column\n",
    "cars = cars.withColumn(\"mass\", round(cars.weight / 2.205, 0))\n",
    "\n",
    "# Convert length to meters\n",
    "cars = cars.withColumn('length', round(cars.length * 0.0254, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611756e4",
   "metadata": {},
   "source": [
    "### Indexing categorical data\n",
    "* Use `StringIndexer` class\n",
    "* Within constructor, provide string input column and a name for the new output column to be created\n",
    "* The indexer is first fit to the data, creating a `StringIndexerModel`\n",
    "* During the fitting process the distinct string values are identified and an index is assigned to each value\n",
    "* The model is then used to transform the data, creating a new column with the index values\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='type',\n",
    "                        outputCol='type_idx')\n",
    "                        \n",
    "# Assign index values to strings\n",
    "indexer = indexer.fit(cars)\n",
    "\n",
    "# Create column with index values\n",
    "cars = indexer.transform(cars)\n",
    "```\n",
    "* **By default, the index values are assigned according to the descending relative frequency of each of the string values.**\n",
    "\n",
    "<img src='data/index_cat_data.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2a3c",
   "metadata": {},
   "source": [
    "* Note, too, that indexing starts at zero (most common)\n",
    "* It is also possible to choose different strategies for assigning index values, by apecifying the **`stringOrderType`** argument.\n",
    "\n",
    "* **Indexing country of origin:**\n",
    "\n",
    "```\n",
    "# Index country of origin:\n",
    "#\n",
    "# USA          -> 0\n",
    "# non-USA      -> 1\n",
    "#\n",
    "cars = StringIndexer(\n",
    "    inputCol = \"origin\",\n",
    "    outputCol = \"label\"\n",
    ").fit(cars).transform(cars)\n",
    "```\n",
    "\n",
    "#### Assembling columns\n",
    "* The final step in preparing a dataset is to consolidate the various input columns into a single column\n",
    "* This is necessary because **the Machine Learning algorithms in Spark operate on a single vector of predictors**(although each element in that vector may consist of multiple values).\n",
    "* First you create an instance of the `VectorAssembler` class, providing it with the names of the columns that you want to consolidate and the name of the new output column:\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['cyl', 'size'], outputCol='features')\n",
    "assembler.transform(cars)\n",
    "```\n",
    "\n",
    "<img src='data/column_vectors.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4107d30",
   "metadata": {},
   "source": [
    "**Note** above the new `features` column, which consists of values from the `cylinders` and `size` columns consolidated into a vector.\n",
    "\n",
    "```\n",
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d502e70",
   "metadata": {},
   "source": [
    "#### Exercises: Categorical columns \n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(5)\n",
    "```\n",
    "\n",
    "#### Exercises: Assembling Columns \n",
    "\n",
    "```\n",
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf9d5d",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "* Decision trees are perhaps the most intuitive models\n",
    "* Decision trees are constructed using an algorithm called **Recursive Partitioning**.\n",
    "* Ideally, you want the groups to be as **homogeneous** (or \"pure\") as possible (without overfitting)\n",
    "* There are a variety of stopping criteria which can cause splitting to stop along a branch\n",
    "\n",
    "<img src='data/classifying_cars.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* **Note** the updated `features` and `label` columns above\n",
    "* Here, we want to build a decision tree which will use \"features\" to predict \"label\"\n",
    "\n",
    "#### Split train/test\n",
    "* Split data into training and testing sets\n",
    "\n",
    "```\n",
    "# Specify a seed for reproducability\n",
    "cars_train, cars_test = cars.randomSplit([0.8, 0.2], seed=23)\n",
    "```\n",
    "* Two DataFrames: `cars_train` and `cars_test`\n",
    "\n",
    "#### Build a Decision Tree model\n",
    "\n",
    "```\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree_model = tree.fit(cars_train)\n",
    "\n",
    "prediction = tree_model.transform(cars_test)\n",
    "```\n",
    "\n",
    "<img src='data/pred_cars.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace3bf1",
   "metadata": {},
   "source": [
    "#### Evaluating\n",
    "* Make predictions on the testing data and compare to known values\n",
    "* The `transform()` method added new columns to the DataFrame\n",
    "* The `prediction` column gives the class assigned by the model; you can compare this directly to the known labels in the testing data\n",
    "* Althought the model gets the first example wrong (above), it's correct for the following four examples\n",
    "* There's also a `probability` column which gives the probabilities assigned to each of the outcome classes\n",
    "\n",
    "#### Confusion matrix\n",
    "* A confusion matrix is a table which describes performance of a model on testing data\n",
    "* `prediction.groupBy(\"label\", \"prediction\").count().show()`\n",
    "\n",
    "<img src='data/cars_conf_mat.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f2e75",
   "metadata": {},
   "source": [
    "* **Accuracy = (TN + TP) / (TN + TP + FN + FP) -- proportion of correct predictions.**\n",
    "* A good way to understand the performance of a model is to create a confusion matrix, which gives a breakdown of the model predictions versus the known labels\n",
    "* The counts above can be used to calculate the accuracy, or any other classification metric (like precision, recall, f1, etc...)\n",
    "\n",
    "#### Exercises: Train/test split\n",
    "\n",
    "```\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print(training_ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ad817",
   "metadata": {},
   "source": [
    "#### Exercises: Build a Decision Tree\n",
    "\n",
    "```\n",
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)\n",
    "```\n",
    "\n",
    "#### Exercises: Evaluate the Decision Tree\n",
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "\n",
    "   * *True Negatives* (TN) — model predicts negative outcome & known outcome is negative\n",
    "   * *True Positives* (TP) — model predicts positive outcome & known outcome is positive\n",
    "   * *False Negatives* (FN) — model predicts negative outcome but known outcome is positive\n",
    "   * *False Positives* (FP) — model predicts positive outcome but known outcome is negative.\n",
    "   \n",
    "```\n",
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5e7cd",
   "metadata": {},
   "source": [
    "### Logisitic Regression\n",
    "* Logistic regression is another commonly used classification model.\n",
    "* It uses a logistic function to model a binary target, where the target states are usually denoted by 1 and 0, or `True` and `False`.\n",
    "* **For a Logistic Regression model, the x-axis is a linear combination of predictor variables and the y-axis is the output of the model.**\n",
    "* Since the value of the logistic function is a number between zero and one, it's often thought of as a probability.\n",
    "* In order to translate this number into one or another of the target states, it's compared to a threshold, which is normally (but not always) set at one half.\n",
    "* To prepare for modeling, once again:\n",
    "    * assemble (consolidate) the predictors into a single column (called `features`) and\n",
    "    * split data into training and testing sets\n",
    "    \n",
    "#### Build a Logistic Regression model\n",
    "\n",
    "```\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic = logistic.fit(cars_train)\n",
    "prediction = logistic.transform(cars_test)\n",
    "```\n",
    "\n",
    "### Precision and recall\n",
    "* **Precision** is the proportion of positive predictions which are correct\n",
    "\n",
    "```\n",
    "# Precision (positive)\n",
    "TP / (TP + FP)\n",
    "```\n",
    "* **Recall** is the proportion of positive targers which are correctly predicted\n",
    "\n",
    "```\n",
    "# Recall (positive)\n",
    "TP / (TP + FN)\n",
    "```\n",
    "\n",
    "#### Weighted metrics\n",
    "* Another way of looking at these ratios is to weight them across the positive and negative predictions\n",
    "* You can do this by creating an evaluator object and then calling the `evaluate()` method\n",
    "\n",
    "```\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(prediction, {evaluator.metricName: 'weightedPrecision'})\n",
    "```\n",
    "* The `evaluate()` method (called on the `evaluator` object) accepts an argument which specifies the required metric\n",
    "* It's possible to request the following metrics:\n",
    "    * `weightedPrecision`\n",
    "    * `weightedRecall`\n",
    "    * `accuracy`\n",
    "    * `f1` : the harmonic mean of precision and recall, which is generally more robust than the accuracy\n",
    "* All of the above metrics have an assumed threshold of 0.5 but can be changed\n",
    "    * Choosing a larger or smaller value for the threshold will affect the performance of the model\n",
    "* A **threshold** is used to decide whether the number returned by the Logistic Regression model translates into either the positive or the negative class\n",
    "\n",
    "### ROC and AUC \n",
    "* The **ROC curve** plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left)\n",
    "* **ROC** = **Receiver Operating Characteristic**\n",
    "\n",
    "<img src='data/ROC_AUC.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "* TP versus FP\n",
    "* threshold = 0 (top right)\n",
    "* threshold = 1 (bottom left)\n",
    "#### AUC\n",
    "* The **AUC** summarizes the ROC curve in a single number.\n",
    "* **AUC** = **Area under the curve**\n",
    "* Ideally AUC = 1\n",
    "* An ideal model that performs perfectly regardless of the threshold, would have an AUC of 1\n",
    "* AUC indicates how well a model performs across all values of the threshold\n",
    "\n",
    "#### Exercises: Build a Logistic Regression Model\n",
    "\n",
    "```\n",
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "```\n",
    "\n",
    "#### Exercises: Evaluate the Logistic Regression model \n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target class.\n",
    "\n",
    "There are two other useful metrics, however:\n",
    "    * Precision\n",
    "    * Recall\n",
    "    \n",
    "Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "\n",
    "Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\n",
    "\n",
    "The precision and recall are generally formulated in terms of the positive target class. But it's also possible to calculate *weighted* versions of these metrics which look at both target classes.\n",
    "\n",
    "\n",
    "```\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906100fd",
   "metadata": {},
   "source": [
    "### Turning Text into Tables\n",
    "* It is said that 80% of Machine Learning is data preparation (this is particularly true for text data)\n",
    "* Before you can use ML algorithms, you need to take unstructured text data and create structure, ultimately transforming the data into a table.\n",
    "* **Term-document matrix:**\n",
    "\n",
    "<img src='data/term_document_matrix.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Removing punctuation\n",
    "* Use **REGEX** (regular expressions), a mini-language for pattern-matching, to remove the punctuation symbols\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Regular expression (REGEX) to match commas and hyphens\n",
    "REGEX = '[,\\\\-]`\n",
    "\n",
    "books = books.withColumn('text', regexp_replace(books.text, REGEX, ' '))\n",
    "```\n",
    "* Above, **the hypen is escaped by the backslashes** because hyphens have another meaning in the context of regular expressions. **By escaping it, you tell Spark to interpret the hyphen literally.**\n",
    "* You need to specify a column name (`books.text`), a pattern to be matched (`REGEX`), and the replacement text, which is simply a space `' '`.\n",
    "* The results have some double spaces, but we can use REGEX to clean those up too.\n",
    "\n",
    "<img src='data/regex1.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3850fe",
   "metadata": {},
   "source": [
    "#### Text to tokens\n",
    "* Next, we split the text into words or **tokens.**\n",
    "* First, create a tokenizer object, giving it the name of the input column containing the text and the output column which will contain the tokens\n",
    "* The tokenizer is then applied to the text using the `transform()` method\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "books = Tokenizer(inputCol='text', outputCol='tokens').transform(books)\n",
    "```\n",
    "\n",
    "<img src='data/tokens1.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae7597",
   "metadata": {},
   "source": [
    "* In the results, you can see a new column in which each document has been transformed into a list of words\n",
    "* As a side effect, all the words have been reduced to lower case and are comma-delimited\n",
    "\n",
    "#### What are stop words?\n",
    "* Common \"stop\" words convey very little information (like \"the\", \"at\", \"a\", etc), so you will typically also remove them\n",
    "* Remove stop words using an instance of the `StopWordsRemover` class\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopwords = StopWordsRemover()\n",
    "\n",
    "# Take a look at the list of stop words\n",
    "stopwords.getStopWords()\n",
    "```\n",
    "* This contains a list of stop words, which **can be customized if necessary.**\n",
    "* First, specify input and output column names and apply the transform method (we could also have given the input and output names when we created the remover)\n",
    "\n",
    "```\n",
    "# Specify the input and output column names\n",
    "stopwords = stopwords.setInputCol('tokens').setOutputCol('words')\n",
    "\n",
    "books = stopwords.transform(books)\n",
    "```\n",
    "\n",
    "#### Feature hashing\n",
    "* Documents could contain a large variety of words, so in principle our table could end up with an enormous number of columns, many of which would be only sparsely populated\n",
    "* It would also be handy to convert the words into numbers\n",
    "* Enter the **hashing trick**, which in simple terms converts words into numbers\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hasher = HashingTF(inputCol='words', outputCol='hash', numFeatures=32)\n",
    "books = hasher.transform(books)\n",
    "```\n",
    "* Create an instance of the HashingTF class, providing the names of the input and output columns \n",
    "* Also give the number of features, which is effectively the largest number that will be produced by the hashing trick\n",
    "    * This needs to be sufficiently big to capture the diversity in the words\n",
    "* The output in the hash column is presented in sparse format \n",
    "\n",
    "<img src='data/feature_hashing.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a1a70",
   "metadata": {},
   "source": [
    "* **Note that there are two lists:**\n",
    "    * The first list contains the hashed values\n",
    "    * The second list indicates how many times each of those values occurs\n",
    "* For example, in the first document, the word `long` has a hash of `8` and occurs twice (`2.0`).\n",
    "\n",
    "#### Dealing with common words\n",
    "* The final step is to account for some words occurring frequently across many documents\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "books = IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)\n",
    "```\n",
    "* If a word appears in many documents, then it's probably going to be less useful for building a classifier\n",
    "* We want to weight the number of counts for a word in particular document against how frequently that word occurs across all documents. To do this you reduce the effective count for more common words, giving what is known as the **inverse document frequency**.\n",
    "* **Inverse document frequency** is generated by the IDF class, which is first fit to the hashed data and then used to generate weighted counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3d327",
   "metadata": {},
   "source": [
    "#### Exercises: Punctuation, numbers, and tokens\n",
    "\n",
    "```\n",
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)\n",
    "```\n",
    "#### Exercises: Stop words and hashing\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
    "      .transform(sms)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol = 'terms', outputCol='hash', numFeatures=1024)\\\n",
    "      .transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
    "      .fit(wrangled).transform(wrangled)\n",
    "      \n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)\n",
    "```\n",
    "\n",
    "#### Training a spam classifier\n",
    "\n",
    "```\n",
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed = 13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2b3ab",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: Regression\n",
    "Next you'll learn to create Linear Regression models. You'll also find out how to augment your data by engineering new predictors as well as a robust approach to selecting only the most relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759464e",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "* **The problem with indexed data:**\n",
    "    * The index numbers don't have any objective meaning\n",
    "    * It doesn't make sense to do arithmetic on that index\n",
    "    * Nor would it make sense to compare those numerical indexes\n",
    "    * Numerical indexes also imply an order or chronology\n",
    "    * **A regression model works predciesly by performing arithmetic on predictor variables.**\n",
    "* **We need to convert the index values into a format in which we can perform meaningful mathematical operations.\n",
    "\n",
    "#### Dummy variables\n",
    "* The first step is to create a column for *each* of the levels or categories\n",
    "* These new columns are known as **dummy variables**\n",
    "* Use **binary encoding**, where `1` indicates the presence of the corresponding level (or cateogry type) and `0` indicates the absense of a corresponding level (or category type)\n",
    "* Note that while this will create many more columns, the majority of the new columns contain zeros\n",
    "* We can exploit this by converting the data into a **sparse format**:\n",
    "    * Rather than recording the individual values, the sparse format representation simply records the column numbers and value for the non-zero values\n",
    "    * Since the categorical levels are mutually exclusive, you can drop one of the columns\n",
    "    * This process is called **One-hot encoding** (because only one of the columns created is ever \"active\" or \"hot\"\n",
    "    \n",
    "#### One-Hot Encoding in Spark \n",
    "* Note that these arguments are given as lists, so it's possible to specify mulitple columns if necessary.\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "onehot = OneHotEncoderEstimator(inputCols=['type_idx'], outputCols=['type_dummy'])\n",
    "onehot = onehot.fit(cars)\n",
    "\n",
    "# How many category levels?\n",
    "onehot.categorySizes\n",
    "\n",
    "cars = onehot.transform(cars)\n",
    "cars.select('type', 'type_idx', 'type_dummy').distinct().sort('type_idx').show()\n",
    "```\n",
    "\n",
    "<img src='data/ohe_spark.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb739486",
   "metadata": {},
   "source": [
    "#### Dense vs sparse formats\n",
    "* The sparse format used to represent dummy variables looks a little complicated\n",
    "* Suppose you wanqt to store a vector which consists mostly of zeros\n",
    "    * `[1, 0, 0, 0, 0, 7, 0, 0]`\n",
    "* You could store it as a Dense vector, in which each of the element of the vector is store explicitly.\n",
    "    * This is wasteful though because most of those elements are zeros.\n",
    "    \n",
    "<img src='data/dense_sparse.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baacba",
   "metadata": {},
   "source": [
    "* To create a sparse vector you need to specify the size of the vector, the positions which are non-zero, and the values of those positions\n",
    "* Sparse representation is essential for effective one-how encoding on large data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca6609",
   "metadata": {},
   "source": [
    "#### Exercises: Encoding Flight Origin\n",
    "\n",
    "```\n",
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()\n",
    "```\n",
    "\n",
    "#### Exercises: Encoding Shirt Sizes\n",
    "\n",
    "You have data for a consignment of t-shirts. The data includes the size of the shirt, which is given as either S, M, L or XL.\n",
    "\n",
    "Here are the counts for the different sizes:\n",
    "\n",
    "<img src='data/ohe_shirt2.png' width=\"150\" height=\"75\" align=\"center\"/>\n",
    "\n",
    "The sizes are first converted to an index using StringIndexer and then one-hot encoded using OneHotEncoderEstimator.\n",
    "\n",
    "Which of the following is not true:\n",
    "\n",
    "* S shirts get index `2.0` and are one-hot encoded as `(3,[2],[1.0])`\n",
    "* M shirts get index `1.0` and are one-hot encoded as `(3,[1],[1.0])`\n",
    "* L shirts get index `0.0` and are one-hot encoded as `(3,[0],[1.0])`\n",
    "* **XL shirts get index `3.0` and are one-hot encoded as `(3,[3],[1.0])`** $\\Leftarrow$ $\\Leftarrow$ $\\Leftarrow$\n",
    "\n",
    "**Why?** This statement is false: XL is the least frequent size, so it receives an index of 3. However, it is one-hot encoded to `(3,[],[])` because it does not get it's own dummy variable. If none of the other dummy variables are true, then this one *must* be true. So to make a separate dummy variable would be redundant!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27358940",
   "metadata": {},
   "source": [
    "### Regression\n",
    "* Out of all possible models, the best model is found by minimizing a loss function, which is an equation that describes how well the model fits the data\n",
    "\n",
    "#### Build Regression Model\n",
    "* **Note:** By default the `LinearRegression` class expects to find the target data in a column named **`label`**.\n",
    "* As, in the example below, we are lookin to predict the `consumption` column, we need to explicitly specify the name of the label column when creating the regression object, `regression`.\n",
    "\n",
    "```\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "regression = LinearRegression(labelCol = 'consumption')\n",
    "regression = regression.fit(cars_train)\n",
    "predictions = regression.transform(cars_test)\n",
    "```\n",
    "\n",
    "#### Calculate RMSE\n",
    "* It's useful to have a single number which summarizes the performance of a model.\n",
    "* The root mean squared error is often used for regression models, which corresponds to the standard deviation of the residuals.\n",
    "* Values of RMSE are relative to the scale of the value that you're aiming to predict, so interpretation is a little more challenging\n",
    "* A smaller RMSE, however, always indicates better predictions.\n",
    "\n",
    "```\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Find RMSE (Root Mean Squared Error)\n",
    "RegressionEvaluator(labelCol='consumption').evaluate(predictions)\n",
    "```\n",
    "\n",
    "* **A `RegressionEvaluator()` can also calculate the following metrics:\n",
    "   * `mae` (Mean Absolute Error)\n",
    "   * `r2` ($R^2$)\n",
    "   * `mse` (Mean Squared Error)\n",
    "   \n",
    "#### Consumption versus mass\n",
    "* The **intercept** is the value predicted by the model when all predictors are 0\n",
    "    * On a plot, this is the point where the model line inersects the y-axis\n",
    "* You can find this value for the model using the intercept attribute:\n",
    "    * **`regression.intercept`**\n",
    "    * This will return the predcted fuel consumption when both mass and number of cylinders are zero and the vehicle type is `Van` in the `cars` dataset example. \n",
    "    * However, this is a hypothetical scenario, as no vehicle could have zero mass obviously \n",
    "    \n",
    "#### Slope\n",
    "* The slope associated with each of the predictors represents how rapidly the model changes when that predictor changes \n",
    "* **`regression.coefficients`**\n",
    "    * The `coefficients` attribute gives you access to the coefficient for each of the predictors\n",
    "    * These `coefficients` also represent the rate of change for the corresponding predictor\n",
    "    * Remember that there's no dummy variable for `Van`?\n",
    "        * The coefficients for type dummy variables are relative to Vans\n",
    "        * These coefficients should also be interpreted with care: if you are going to compare the values for different vehicle types then this needs to be done for fixed mass and number of cylinders\n",
    "        \n",
    "        \n",
    "#### Exercises Flight duration model: Just distance\n",
    "\n",
    "```\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "```\n",
    "\n",
    "#### Exercises: Interpreting the coefficients\n",
    "\n",
    "```\n",
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)\n",
    "```\n",
    "\n",
    "#### Exercises: Flight duration model: Adding origin airport\n",
    "\n",
    "Some airports are busier than others. Some airports are bigger than others too. Flights departing from large or busy airports are likely to spend more time taxiing or waiting for their takeoff slot. So it stands to reason that the duration of a flight might depend not only on the distance being covered but also the airport from which the flight departs.\n",
    "\n",
    "You are going to make the regression model a little more sophisticated by including the departure airport as a predictor.\n",
    "\n",
    "These data have been split into training and testing sets and are available as `flights_train` and `flights_test`. The origin airport, stored in the `org` column, has been indexed into `org_idx`, which in turn has been one-hot encoded into `org_dummy`. The first few records are displayed in the terminal.\n",
    "\n",
    "```\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "```\n",
    "\n",
    "#### Exercises: Interpreting coefficients\n",
    "Remember that origin airport, `org`, has eight possible values (ORD, SFO, JFK, LGA, SMF, SJC, TUS and OGG) which have been one-hot encoded to seven dummy variables in `org_dummy`.\n",
    "\n",
    "The values for `km` and `org_dummy` have been assembled into `features`, which has eight columns with sparse representation. Column indices in `features` are as follows:\n",
    "\n",
    "* 0 — `km`\n",
    "* 1 — `ORD`\n",
    "* 2 — `SFO`\n",
    "* 3 — `JFK`\n",
    "* 4 — `LGA`\n",
    "* 5 — `SMF`\n",
    "* 6 — `SJC` \n",
    "* 7 — `TUS`\n",
    "\n",
    "Note that `OGG` does not appear in this list because it is the reference level for the origin airport category.\n",
    "\n",
    "In this exercise you'll be using the `intercept` and `coefficients` attributes to interpret the model.\n",
    "\n",
    "The `coefficients` attribute is a list, where the first element indicates how flight duration changes with flight distance.\n",
    "\n",
    "```\n",
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60/regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)\n",
    "```\n",
    "\n",
    "### Bucketing and Engineering\n",
    "* The largest improvements in ML model performance are often achieved by carefully manipulating features \n",
    "\n",
    "#### Bucketing\n",
    "* **It's often convenient to convert a continuous variable, like age or height, into discrete values.**\n",
    "* Buckets might have uniform *or* variable width (?)\n",
    "* A categorical variable (as a result of bucketing or binning) is often a more powerful predictor than the original continuous variable.\n",
    "\n",
    "#### RPM Buckets\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketizer = Bucketizer(splits = [3500, 4500, 6000, 6500],\n",
    "                        inputCol='rpm',\n",
    "                        outputCol='rpm_bin')\n",
    "cars = bucketizer.transform(cars)                        \n",
    "\n",
    "bucketed.select('rpm', 'rpm_bin').show(5)\n",
    "\n",
    "cars.groupBy('rpm_bin').count().show()\n",
    "```\n",
    "\n",
    "<img src='data/rpm_buckets.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='data/rpm_buckets_ohe.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5a223",
   "metadata": {},
   "source": [
    "* As we saw earlier, before we can use the index values in a regression model, they first need to be one-hot-encoded\n",
    "* The low and medium RPM ranges are mapped to distinct dummy variables, while the high range is the reference level and does not get a separate dummy variable.\n",
    "* Let's look at the intercept and coefficients for a model which predicts fuel consumption based on bucketed RPM data:\n",
    "\n",
    "<img src='data/bucketed_rpm3.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42717d7",
   "metadata": {},
   "source": [
    "* The intercept above tells us what the fuel consumption is for the reference level, which is the high RPM bucket.\n",
    "* To get the consumption for the low RPM bucket, you add the first coefficient to the intercept\n",
    "* Similarly, to find the consuption for the medium RPM bucket, you add the second coefficient to the intercept\n",
    "\n",
    "#### More feature engineering\n",
    "* There are many other approaches to engineering new features\n",
    "* **Operations on a single column:**\n",
    "    * `log()`\n",
    "    * `sqrt()`\n",
    "    * `pow()`\n",
    "* **Operations on two columns:**\n",
    "    * product\n",
    "    * ratio\n",
    "    \n",
    "#### Engineering density\n",
    "* You can create different forms of density by dividing the mass through by the first three powers of length\n",
    "* Since you only have the length of the vehicles but not their width or height, the length is being used as a proxy for these missing dimensions.\n",
    "\n",
    "```\n",
    "cars = cars.withColumn('density_line', cars.mass / cars,length)          # Linear density\n",
    "cars = cars.withColumn('density_quad', cars.mass / cars,length**2)       # Area density\n",
    "cars = cars.withColumn('density_cube', cars.mass / cars,length**3)       # Volume density \n",
    "```\n",
    "* In the above code, the first density represents how mass changes with vehicle length\n",
    "* The second and third densities approximate how mass varies with the area and volume of the vehicle \n",
    "* Powerful new features are often discovered through trial and error\n",
    "\n",
    "#### Exercises: Bucketing departure time \n",
    "\n",
    "Time of day data are a challenge with regression models. They are also a great candidate for bucketing.\n",
    "\n",
    "In this lesson you will convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You'll then take those binned values and one-hot encode them.\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[3 * x for x in range(9)], \n",
    "                     inputCol = 'depart', \n",
    "                     outputCol = 'depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)\n",
    "```\n",
    "\n",
    "#### Exercises: Flight duration model: Adding departure time \n",
    "\n",
    "```\n",
    "# Find the RMSE on testing data\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8]\n",
    "print(avg_night_jfk)\n",
    "```\n",
    "\n",
    "### Regularization\n",
    "\n",
    "```\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit Lasso model (α = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a025544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f752df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625e9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3266d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114a918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dbd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2ff05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f8504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584b4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf51953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a664ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b2cc01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b671bf",
   "metadata": {},
   "source": [
    "<img src='data/course_datasets.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
