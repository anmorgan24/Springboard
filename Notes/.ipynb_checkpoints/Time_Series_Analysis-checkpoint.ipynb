{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e78d09",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae4f77",
   "metadata": {},
   "source": [
    "* Time series data is one of the most common data types and understanding how to work with it is a critical data science skill if you want to make predictions and report on trends. \n",
    "* Any column of a DataFrame can have datetime object, but the most important is the DateTimeIndex, because this converts the entire DataFrame into a Time Series.\n",
    "* Many Series/DF methods rely on time information in the index to provide time-series functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157e222",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a70ec",
   "metadata": {},
   "source": [
    "## **NOTE TO SELF: ideas to fill missing values Cov_weather project:** \n",
    "\n",
    "* Use shift to fill in missing weekend values of COVID19 cases?\n",
    "* use forward fill, back fill, or fill_value?\n",
    "* or: interpolate missing weekend values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19236b80",
   "metadata": {},
   "source": [
    "#### Timestamps \n",
    "   * `pd.Timestamp()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html)\n",
    "   * `time_stamp = pd.Timestamp(datetime(2017, 1, 1))` -OR- `pd.Timestamp('2017-01-01') == time_stamp`\n",
    "   * use date string or datetime object (above)\n",
    "   * timestamps created with date only will automatically set time to midnight (\"00:00:00\")\n",
    "   * Timestamp object has many attributes to store time-specific information\n",
    "      * `.year`, `.weekday_name`, etc\n",
    "   * Timestamps can also have frequency information\n",
    "   * __Common timestamp attributes:__ \n",
    "       * .second, .minute, .hour\n",
    "       * .day, .month, .quarter, .year\n",
    "       * .weekday\n",
    "       * .dayofweek\n",
    "       * .weekofyear\n",
    "       * .dayofyear\n",
    "   * to create a sequence of timestamps: \n",
    "        * `pd.date_range()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html)\n",
    "        * `index = pd.date_range(start= '2017-1-1', periods=12, freq='M')` OR\n",
    "        * `index = pd.date_range(start= '2017-1-1', end= '2017-12-1', freq='M')`\n",
    "\n",
    "#### Periods and frequencies\n",
    "   * `pd.Period()`([doc here](https://pandas.pydata.org/docs/reference/api/pandas.Period.html)) & `freq` \n",
    "   * Period object is pandas specific\n",
    "   * the period object always has a frequency, with month (`M`) as default.\n",
    "   * `period = pd.Period('2017-01-01', 'D')`\n",
    "   * `period = pd.Period('2017-01')` #default period is month\n",
    "   * convert period to new frequencies:\n",
    "   * `period.asfreq('D')` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.asfreq.html)\n",
    "   * You can convert a period to a timestamp object and a timestamp object to a period object:\n",
    "   * `period.to_timestamp().to_period('M')` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_timestamp.html)\n",
    "   * perform basic date arithmetic with periods:\n",
    "       * if: `period = pd.Period('2017-01', 'M')`\n",
    "       * then: `period + 2` = `Period('2017-03', 'M')` #plus two units (frequency) M\n",
    "   * Timestamps can also have frequency information\n",
    "   * Sequences of dates & times:    \n",
    "   * to create a TimeSeries, you need a sequence of dates\n",
    "   * to create a sequence of Timestamps, use the pandas function `date_range()`\n",
    "       * `pd.date_range(start, end, periods, freq)`\n",
    "       * `index = pd.date_range(start = '2017-01-01, periods =12, freq = 'M')` # default freq = 'D'\n",
    "   * Convert Timestamp index to a Period index with: `index.to_period()`\n",
    "    \n",
    "#### Creating a time series\n",
    "   * `pd.DateTimeIndex()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html)\n",
    "   * Create 12 rows x 2 columns of random data to match 12 datetime indices created above:\n",
    "       * `data = np.random.random((size = 12,2))\n",
    "       * pd.DataFrame(data=data, index=index)`\n",
    "   * Common frequency aliases:\n",
    "       * Hour : 'H'\n",
    "       * Day : 'D' (\"calendar day\"; midnight to midnight)\n",
    "       * Week : 'W'\n",
    "       * Month : 'M'\n",
    "       * Quarter : 'Q'\n",
    "       * Year : 'A'\n",
    "       * May also be specified to beginning vs end of period, or business-specific defintion \n",
    "   * Convert to **business day** frequency: `.asfreq('B')`\n",
    "\n",
    "#### Indexing & resampling time series\n",
    "   * **Upsampling:** Increasing the time frequency (requires generating new data)\n",
    "   * **Downsampling:** Decreasing the time frequency (requires aggregating data)\n",
    "   * parse date string to datetime64: \n",
    "       * `pd.to_datetime()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)\n",
    "       * `google.date = pd.to_datetime(google.date)` #convert whole column(s)\n",
    "   * date to index: `.set_index(inplace=True)`\n",
    "   * to select subsets of time series data, use strings that represent a complete date or relevant parts of a date\n",
    "       * `google['2015'].info()` #returns data from 2015\n",
    "       * `google['2015-3': '2016-2'].info()` #data from 02-03 of 2016\n",
    "       * *Note that the date range is inclusive of the end date.*\n",
    "       * Also: use `.loc` with complete date and column label: `google.loc['2016-6-1', 'price']`\n",
    "   * set frequency information with `.asfreq('D')`\n",
    "   * Convert to business day frequency: `.asfreq('B')`\n",
    "\n",
    "#### Lags, changes, and returns for stock price series\n",
    "   * typical time series manipulations include:\n",
    "       * Shift or lag values backwards or forwards in time\n",
    "       * difference in value for a give time period\n",
    "       * compute percent change over any number of periods (aka rate of growth)\n",
    "       * instead of manually `pd.to_datetime()`, let `pd.read_csv()` do the parsing work:\n",
    "           * `google.pd.read_csv('google.csv', parse_dates= ['date'], index_col = 'date')`\n",
    "           * this converts original date column's column name to index name/header (also 'date')\n",
    "       * `.shift()`:\n",
    "           * defaults to periods = 1\n",
    "           * 1 period = 1 period in the future (first value with be missing); -1 = 1 period in the past (last value will be missing)\n",
    "           * `google['shifted'] = google.price.shift()` # defaults to period = 1\n",
    "           * shifting data is useful for comparing data at different points in time \n",
    "       * calculate **rate of change** from period to period (aka **financial return**):\n",
    "           * all of the following methods have a have a `period` keyword that defaults to value 1.\n",
    "           * `google['change'] = google.price.div(google.shifted)`\n",
    "           * `.div()` allows you to divide a series by a value OR series of values, for instance by another column in the same dataframe\n",
    "           * calculate one-period percent change: \n",
    "               * `google['return'] = google.change.sub(1).mul(100)`\n",
    "               * create column `return` by subtracting 1 from change and dividing by 100 to calculate percent difference\n",
    "           * `.diff()` : calculates the difference in value for two adjacent periods [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html)\n",
    "           * `.pct_change()`: `google['pct_change'] = google.price.pct_change().mul(100)` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html)\n",
    "           * to change value of `period` and calculate pct_change between values several periods apart:\n",
    "               * `google['return_3d'] = google.price.pct_change(periods=3).mul(100)`\n",
    "\n",
    "#### Compare time series growth rates\n",
    "   * We often want to compare time series' trends, but because they start at different levels, this isn't always easy\n",
    "   * Simple solution: normalize price series to start at 100:\n",
    "       * Divide all prices by first in series, multiply by 100\n",
    "       * Causes all prices to reflect relative difference in prices from starting price\n",
    "       * Multiple by 100 and you have the difference to starting point, in percentage points\n",
    "   * Normalizing a single series:\n",
    "       * `normalized = google.price.div(first_price).mul(100)`\n",
    "   * Normalizing multiple series:\n",
    "       * `normalized = prices.div(prices.iloc[0])`\n",
    "           * `.div`: automatic alignment of Series index and DataFrame columns [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.div.html)\n",
    "   * add a __benchmark__:\n",
    "\n",
    "```\n",
    "index = pd.read_csv('benchmark.csv', parse_dates= ['date'], index_col = 'date')\n",
    "prices = pd.concat([prices, index], axis = 1).dropna()\n",
    "```\n",
    "   * plotting performance difference: `diff = normalized[tickers].sub(normalized['SP500'], axis=0)\n",
    "   * `.sub(..., axis=0)`: Subtract a series from each DataFrame column by aligning indices\n",
    "   * Normalize and plot data in one step: \n",
    "       * `data.div(data.iloc[0]).mul(100).plot()`\n",
    "       * `plt.show()`\n",
    "\n",
    "#### Changing the time series frequency resampling\n",
    "   * frequency is an attribute\n",
    "   * DateTimeIndex: set & change freq using `.asfreq()`\n",
    "   * changing frequency affects the values in the dataframe\n",
    "       * **Upsampling:** converting to higher frequency: fill or interpolate missing data\n",
    "       * **Downsampling:** convering to a lower frequency: aggregate existing data\n",
    "       * pandas API: \n",
    "           * `.asfreq()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.asfreq.html)\n",
    "           * `.reindex()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html)\n",
    "           * `.resample()` + transformation [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)\n",
    "       * when you choose quarterly frequency, pandas defaults to December for the end of the 4th quarter\n",
    "           * this is modifiable\n",
    "       * Umpsampling fill methods:\n",
    "           * Forward fill: `monthly['ffill'] = quarterly.asfreq('M', method='ffill')` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html) \n",
    "           * Back fill: `monthly['bfill'] = quarterly.asfreq('M', method = 'bfill')` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html)\n",
    "           * provide fill value: `monthly['value'] = quarterly.asfreq('M', fill_value = 0)` [doc here](https://numpy.org/doc/stable/reference/generated/numpy.ma.masked_array.fill_value.html)\n",
    "           * add missing index: `.reindex()`: conform DataFrame to new index, same filling logic as `.asfreq()`\n",
    "               * `quarterly.reindex(dates)` : default fill value is NaN\n",
    "       * Creating new date series info\n",
    "```\n",
    "dates = pd.date_range(start='2016', periods=4, freq='Q')\n",
    "data= range(1,5)\n",
    "quarterly = pd.Series(data=data, index=dates)\n",
    "```        \n",
    "\n",
    "#### Upsampling and interpolation with .resample()\n",
    "   * `.resample()` ([doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)) follows a logic similar to `.groupby()`:\n",
    "       * `resample()`groups data within resampling period and applies one or several methods to each group\n",
    "           * new date is determined by offset - start, end, etc\n",
    "           * Upsampling: fill from existing or interpolate values\n",
    "           * Downsampling: apply aggregation to existing data\n",
    "       * **Resampling Period & Frequency Offsets**\n",
    "           * Several alternatives to calendar month end\n",
    "               * Calendar month end: 'M' | Sample date: 2017-04-30\n",
    "               * Calendar month start: 'MS' | Sample date: 2017-04-01\n",
    "               * Business month end: 'BM' | Sample date: 2017-04-28\n",
    "               * Business month start: 'BMS' | Sample date: 2107-04-03\n",
    "       * .resample() will only produce df after secondary call:\n",
    "               * `unrate.resample('MS')` #outputs `DatetimeIndexResampler`\n",
    "               * `unrate.asfreq('MS').equals(unrate.resample('MS').asfreq())` #outputs data\n",
    "       * `gdp_1 = gdp.resample('MS').ffill().add_suffix('_ffill')`\n",
    "       * Interpolate: finds points on straight line between existing data.\n",
    "           * `.interpolate()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html)\n",
    "           * `gdp_2 = gdp.resample('MS').interpolate().add_suffix('_inter')`\n",
    "       * `pd.concat()` defaults to axis =0; resetting to axis=1 allows you to concatenate horizontally [doc here](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "       * Downsampling and aggregating:\n",
    "           * **Downsampling:** reduce the frequency of your data | How to represent existing data with new aggregated values?\n",
    "               * mean, median, last value\n",
    "           * first assign calendar day frequency using resample:\n",
    "           * `ozone = ozone.resample('D').asfreq()`\n",
    "           * convert daily to monthly\n",
    "           * `ozone.resample('M').mean()`\n",
    "           * `resample().mean()`: monthly average, assigned to end of calendar month\n",
    "           * similar to groupby, you can also apply multiple aggregations at once:\n",
    "           * `.agg()` [doc here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html)\n",
    "           * `ozone.resample('M').agg(['mean', 'std']).head()`\n",
    "           * `resample().agg()` : list of aggregation functions like groupby\n",
    "           * `.squeeze()` [doc here](https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c626e8",
   "metadata": {},
   "source": [
    "```\n",
    "# Create the range of dates here\n",
    "seven_days = pd.date_range(start= '2017-01-01', periods = 7)\n",
    "\n",
    "# Iterate over the dates and print the number and name of the weekday\n",
    "for day in seven_days:\n",
    "    print(day.dayofweek, day.weekday_name)\n",
    "```\n",
    "***\n",
    "```\n",
    "data = pd.read_csv('nyc.csv')\n",
    "\n",
    "# Inspect data\n",
    "print(data.info())\n",
    "\n",
    "# Convert the date column to datetime64\n",
    "data.date=pd.to_datetime(data.date)\n",
    "\n",
    "# Set date column as index\n",
    "data.set_index('date', inplace=True)\n",
    "\n",
    "# Inspect data \n",
    "print(data.info())\n",
    "\n",
    "# Plot data\n",
    "data.plot(subplots=True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16118e",
   "metadata": {},
   "source": [
    "```\n",
    "# Create dataframe prices here\n",
    "prices = pd.DataFrame()\n",
    "\n",
    "# Select data for each year and concatenate with prices here \n",
    "for year in ['2013', '2014', '2015']:\n",
    "    price_per_year = yahoo.loc[year, ['price']].reset_index(drop=True)\n",
    "    price_per_year.rename(columns={'price': year}, inplace=True)\n",
    "    prices = pd.concat([prices, price_per_year], axis=1)\n",
    "\n",
    "# Plot prices\n",
    "prices.plot()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f4a07",
   "metadata": {},
   "source": [
    "* From stock prices to climate data, time series data are found in a wide variety of domains, and being able to effectively work with such data is an increasingly important skill for data scientists.\n",
    "* Time series analysis deals with data that is ordered in time\n",
    "* Some useful pandas methods:\n",
    "    * `df['col'].pct_change()` : percent change\n",
    "    * `df['col'].diff()` : difference\n",
    "    * `df['ABC'].corr(df['XYZ'])`: correlation (for pd Series)\n",
    "* __Google Trends__ allows users to see how often a term is searched for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f6374",
   "metadata": {},
   "source": [
    "* A first step when analyzing a time series is to visualize the data with a plot. \n",
    "* Stock and bond markets in the U.S. are closed on different days. For example, although the bond market is closed on Columbus Day (around Oct 12) and Veterans Day (around Nov 11), the stock market is open on those days. One way to see the dates that the stock market is open and the bond market is closed is to convert both indexes of dates into sets and take the difference in sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa46808",
   "metadata": {},
   "source": [
    "### Two Time Series\n",
    "* Often, two time series vary together\n",
    "* __correlation coefficient:__ a measure of how much two series vary together; a correlation of 1 means two series have a perfect linear correlation with no deviations. A correlation of 0 means no correlation whatsoever.\n",
    "#### Common Mistake: Correlation of two trending series \n",
    "* Just because two time series seem to be trending together in the same direction(s) does not mean that they are correlated.\n",
    "* Example: Dow Jones and UFO sightings\n",
    "* If you're looking at the correlation of two stocks, you should look at the comparison of their returns, not their levels\n",
    "* Compute percent changes in each, then compute correlation\n",
    "* Scatter plots are also useful for visualizing the correlation between the two variables.\n",
    "\n",
    "```\n",
    "#Compute percent change using pct_change()\n",
    "returns = stocks_and_bonds.pct_change()\n",
    "#Compute correlation using corr()\n",
    "correlation = returns['SP500'].corr(returns['US10Y'])\n",
    "print(\"Correlation of stocks and interest rates: \", correlation)\n",
    "#Make scatter plot\n",
    "plt.scatter(returns['SP500'], returns['US10Y'])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f24b9b",
   "metadata": {},
   "source": [
    "* Two trending series may show a strong correlation even if they are completely unrelated. This is referred to as \"spurious correlation\". That's why when you look at the correlation of say, two stocks, you should look at the correlation of their returns and not their levels.\n",
    "```\n",
    "#Compute correlation of levels\n",
    "correlation1 = levels['DJI'].corr(levels['UFO'])\n",
    "print(\"Correlation of levels: \", correlation1)\n",
    "#Compute correlation of percent changes\n",
    "changes = levels.pct_change()\n",
    "correlation2 = changes['DJI'].corr(changes['UFO'])\n",
    "print(\"Correlation of changes: \", correlation2)\n",
    "```\n",
    "### Simple Linear Regression\n",
    "\n",
    "* Linear Regression aka OLS\n",
    "* Python packages to perform regressions:\n",
    "    * In statsmodels:\n",
    "        * `sm.OLS(y, x).fit()`\n",
    "    * In numpy:\n",
    "        * `np.polyfit(x, y, deg=1)`\n",
    "    * In pandas:\n",
    "        * `pd.ols(y, x)`\n",
    "    * In scipy:\n",
    "        * `stats.lingress(x, y)`\n",
    "* __Beware:__ The order of x and y is not consistent across all packages\n",
    "* R-squared measures how closely the data fit the regression line.\n",
    "    * the R-squared in a simple regression is related to the correlation between the two variables. \n",
    "    * the magnitude of the correlation is the square root of the R-squared and the sign of the correlation is the sign of the regression coefficient.\n",
    "\n",
    "```\n",
    "#Import the statsmodels module\n",
    "import statsmodels.api as sm\n",
    "#Compute correlation of x and y\n",
    "correlation = x.corr(y)\n",
    "print(\"The correlation between x and y is %4.2f\" %(correlation))\n",
    "#Convert the Series x to a DataFrame and name the column x\n",
    "dfx = pd.DataFrame(x, columns=['x'])\n",
    "#Add a constant to the DataFrame dfx\n",
    "dfx1 = sm.add_constant(dfx)\n",
    "#Regress y on dfx1\n",
    "result = sm.OLS(y, dfx1).fit()\n",
    "#Print out the results and look at the relationship between R-squared and the correlation above\n",
    "print(result.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa901ec",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "* Autocorrelation is the correlation of a single Time Series with a lagged copy of itself. \n",
    "* Aka 'Serial Correlation'\n",
    "* Often, when we refer to Autocorrelation, we mean __'Lag-one autocorrelation'__\n",
    "#### What does it mean when a Series has a positive or a negative autocorrelation?\n",
    "* __Mean Reversion:__ Negative autocorrelation\n",
    "* __Momentum__ or __Trend-Following:__ Positive autocorrelation\n",
    "* Example: Traders on Wall Street use autocorrelation to make money\n",
    "* Individual stocks have historically negative autocorrelation\n",
    "\n",
    "* `df.resample(rule= 'M', how= 'last')`\n",
    "    * 'M' = monthly \n",
    "    * how = how to do the resampling; you can use the first date, the last date, or even the average\n",
    "* __Mean reversion__ in stock prices: prices tend to bounce back, or revert, towards previous levels after large moves, which are observed over time horizons of about a week. \n",
    "* A more mathematical way to describe mean reversion is to say that stock returns are negatively autocorrelated.\n",
    "```\n",
    "#Convert the daily data to weekly data\n",
    "MSFT = MSFT.resample(rule='W', how='last')\n",
    "#Compute the percentage change of prices\n",
    "returns = MSFT.pct_change()\n",
    "#Compute and print the autocorrelation of returns\n",
    "autocorrelation = returns['Adj Close'].autocorr()\n",
    "print(\"The autocorrelation of weekly returns is %4.2f\" %(autocorrelation))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683f326",
   "metadata": {},
   "source": [
    "* When you look at daily changes in interest rates, the autocorrelation is close to zero. However, if you resample the data and look at annual changes, the autocorrelation is negative. This implies that while short term changes in interest rates may be uncorrelated, long term changes in interest rates are negatively autocorrelated. \n",
    "```\n",
    "#Compute the daily change in interest rates \n",
    "daily_diff = daily_rates.diff()\n",
    "# Compute and print the autocorrelation of daily changes\n",
    "autocorrelation_daily = daily_diff['US10Y'].autocorr()\n",
    "print(\"The autocorrelation of daily interest rate changes is %4.2f\" %(autocorrelation_daily))\n",
    "#Convert the daily data to annual data\n",
    "yearly_rates = daily_rates.resample(rule='A').last()\n",
    "#Repeat above for annual data\n",
    "yearly_diff = yearly_rates.diff()\n",
    "autocorrelation_yearly = yearly_diff['US10Y'].autocorr()\n",
    "print(\"The autocorrelation of annual interest rate changes is %4.2f\" %(autocorrelation_yearly))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa66ab1",
   "metadata": {},
   "source": [
    "## Autocorrelation Function\n",
    "* __ACF:__ AutoCorrelation Function; shows not only the lag 1 autocorrelation, but the entire autocorrelation function for different lags\n",
    "* Any significant non-zero correlations implies that the series can be forecast from the past \n",
    "* ACF useful for model selection\n",
    "* __Plot ACF in Python:__\n",
    "    * `from statsmodels.graphics.tsaplots import plot_acf`\n",
    "    * input x is series or array \n",
    "    * `plot_acf(x, lags=20, alpha=0.05)`\n",
    "    * the argument `lags` indicates how many lags of the autocorellation function will be plotted.\n",
    "    * the `alpha` argument sets the width of the confidence interval.\n",
    "* __Confidence Interval of ACF:__\n",
    "    * In `plot_acf` the argument `alpha` determines the width of the confidence interval.\n",
    "    * `alpha` = chance the if true autocorrelation is zero, it will fall outside blue band\n",
    "    * Example: `alpha` = 0.05 = 5% chance\n",
    "    * Confidence bands are wider if:\n",
    "        * alpha is lower\n",
    "        * fewer observations\n",
    "    * Under some simplifying observations, 95% confidence bands are $\\pm$2/ $\\sqrt{N}$\n",
    "    * If you don't want to see confidence intervals in your plot, set `alpha=1`\n",
    "* __Numerical Values of ACF:__\n",
    "    * Instead of plotting ACF, you can also print numerical values of ACF\n",
    "    * `from statsmodels.tsa.stattools import acf`\n",
    "    * `print(acf(x))`\n",
    "\n",
    "```\n",
    "#Import the acf module and the plot_acf module from statsmodels\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#Compute the acf array of HRB\n",
    "acf_array = acf(HRB)\n",
    "print(acf_array)\n",
    "#Plot the acf function\n",
    "plot_acf(HRB, alpha=1)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616277fb",
   "metadata": {},
   "source": [
    "* the standard deviation of the sample autocorrelation is 1/$\\sqrt{N}$ where _N_ is the number of observations.\n",
    "* if _N_ = 100, for example, the standard deviation of the ACF is 0.1\n",
    "* since 95% of a normal curve is between +1.96 and -1.96 standard deviations from the mean, the 95% confidence interval is $\\pm$1.96/$\\sqrt{N}$.\n",
    "* This approximation only holds when the true autocorrelations are all zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee064",
   "metadata": {},
   "source": [
    "```\n",
    "#Import the plot_acf module from statsmodels and sqrt from math\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from math import sqrt\n",
    "#Compute and print the autocorrelation of MSFT weekly returns\n",
    "autocorrelation = returns['Adj Close'].autocorr()\n",
    "print(\"The autocorrelation of weekly MSFT returns is %4.2f\" %(autocorrelation))\n",
    "#Find the number of observations by taking the length of the returns DataFrame\n",
    "nobs = len(returns)\n",
    "#Compute the approximate confidence interval\n",
    "conf = 1.96/sqrt(nobs)\n",
    "print(\"The approximate confidence interval is +/- %4.2f\" %(conf))\n",
    "#Plot the autocorrelation function with 95% confidence intervals and 20 lags using plot_acf\n",
    "plot_acf(returns, alpha=0.05, lags=20)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc50c88",
   "metadata": {},
   "source": [
    "## White Noise\n",
    "* White noise is a series with:\n",
    "    * constant mean\n",
    "    * constant variance\n",
    "    * zero autocorrelations at all lags\n",
    "* Special Case: if data has normal distribution, then: _Gaussian White Noise._\n",
    "* __Simulating White Noise:__\n",
    "    * `import numpy as np`\n",
    "    * `noise = np.random.normal(loc=0, scale=1, size=500)`\n",
    "    * all autocorrelations of white noise are zero:\n",
    "    * ` plot_acf(noise, lags=50)`\n",
    "* White noise cannot be forecasted. \n",
    "* Stock returns are often modeled as white noise\n",
    "* A white noise time series is simply a sequence of uncorrelated random variables that are identically distributed. \n",
    "\n",
    "```\n",
    "#Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#Simulate white noise returns\n",
    "returns = np.random.normal(loc=0.02, scale=0.05, size=1000)\n",
    "#Print out the mean and standard deviation of returns\n",
    "mean = np.mean(returns)\n",
    "std = np.std(returns)\n",
    "print(\"The mean is %5.3f and the standard deviation is %5.3f\" %(mean,std))\n",
    "#Plot returns series\n",
    "plt.plot(returns)\n",
    "plt.show()\n",
    "#Plot autocorrelation function of white noise returns\n",
    "plot_acf(returns, lags=20)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef18be",
   "metadata": {},
   "source": [
    "### Random Walk\n",
    "* Whereas stock returns are often modeled as white noise, stock prices closely follow a random walk. In other words, today's price is yesterday's price plus some random noise.\n",
    "* Today's price = yesterday's price + noise\n",
    "* $P_t$ = $P_t-1$ + $\\epsilon_t$\n",
    "* Change in price of a random walk is just white noise:\n",
    "    * $P_t$ - $P_t-1$ = $\\epsilon_t$\n",
    "* If stock prices follow a random walk, then stock returns are white noise. \n",
    "* However, when adding noise, you could theoretically get negative prices\n",
    "* Can't forecast a random walk; the best forecast for tomorrow's price is today's price. \n",
    "* In __Random Walk with drift:__ prices drift by $\\mu$ every period \n",
    "    * Many time series, like stock prices, are random walks but tend to drift up over time.\n",
    "    * Change in price for a random walk with drift is still white noise, but with a mean of $\\mu$\n",
    "    * If we view stock prices as random walk with drift, then the returns are still white noise, but with an average return of $\\mu$ instead of zero.\n",
    "* __Statistical test for random walk:__\n",
    "    * Regress current prices on lag prices \n",
    "        * if the slope coefficient $\\beta$ is not significantly different from one, then we can not reject the null hypothesis that the series is a random walk.\n",
    "        * If $\\beta$ is significantly less than 1, then we _can_ reject the null hyposthesis that the series is a random walk\n",
    "        * An identical way to perform statistical test for random walk is to regress the difference in prices o the lag price. And instead of testing is the slope coefficient is one, now we test whether it is zero.: __Dickey Fuller Test__\n",
    "\n",
    "```\n",
    "#Generate 500 random steps with mean=0 and standard deviation=1\n",
    "steps = np.random.normal(loc=0, scale=1, size=500)\n",
    "#Set first element to 0 so that the first price will be the starting stock price\n",
    "steps[0]=0\n",
    "#Simulate stock prices, P with a starting price of 100\n",
    "P = 100 + np.cumsum(steps)\n",
    "#Plot the simulated stock prices\n",
    "plt.plot(P)\n",
    "plt.title(\"Simulated Random Walk\")\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "#Generate 500 random steps\n",
    "steps = np.random.normal(loc=0.001, scale=0.01, size=500) + 1\n",
    "#Set first element to 1\n",
    "steps[0]=1\n",
    "#Simulate the stock price, P, by taking the cumulative product\n",
    "P = 100 * np.cumprod(steps)\n",
    "#Plot the simulated stock prices\n",
    "plt.plot(P)\n",
    "plt.title(\"Simulated Random Walk with Drift\")\n",
    "plt.show()\n",
    "```\n",
    "### ADF Test in Python:\n",
    "* `from statsmodels.tsa.stattools import adfuller`\n",
    "* Run augmented Dicket Test:\n",
    "* `adfuller(x)`\n",
    "* With the ADF test, the \"null hypothesis\" (the hypothesis that we either reject or fail to reject) is that the series follows a random walk. Therefore, a low p-value (say less than 5%) means we can reject the null hypothesis that the series is a random walk.\n",
    "* (results[0] is the test statistic, and results[1] is the p-value).\n",
    "``` \n",
    "#Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "#Run the ADF test on the price series and print out the results\n",
    "results = adfuller(AMZN['Adj Close'])\n",
    "print(results)\n",
    "#Just print out the p-value\n",
    "print('The p-value of the test on prices is: ' + str(results[1]))\n",
    "```\n",
    "```\n",
    "#Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "#Create a DataFrame of AMZN returns\n",
    "AMZN_ret = AMZN.pct_change()\n",
    "#Eliminate the NaN in the first row of returns\n",
    "AMZN_ret = AMZN_ret.dropna()\n",
    "#Run the ADF test on the return series and print out the p-value\n",
    "results = adfuller(AMZN_ret['Adj Close'])\n",
    "print('The p-value of the test on returns is: ' + str(results[1]))\n",
    "```\n",
    "* Most stock prices follow a random walk (perhaps with a drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15925c",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "* __Strong Stationarity:__ entire distribution of data is time-invariant.\n",
    "* __Weak Stationarity:__ A less restrictive type of stationarity; mean, variance, and auto correlation are time-invariant; easier to test\n",
    "* If a process is not stationary, then it becomes difficult to model.\n",
    "* Modeling involves estimating a set of parameters, and if a process is not stationary and the parameters are different at each point in time, then there are too many parameters to estimate. You may end up having more parameters than actual data. \n",
    "* Stationarity is necessary for a parsimonious model (with a smaller set of parameters to estimate). \n",
    "* A random walk is a common type of non-stationary series; the variance grows with time\n",
    "* Seasonal series are also non-stationary\n",
    "* Many non-stationary series can be made stationary through a simple transformation or set of transformations. \n",
    "    * First difference\n",
    "    * Seasonal difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c107e",
   "metadata": {},
   "source": [
    "### Seasonality\n",
    "* Many time series exhibit strong seasonal behavior. \n",
    "* The procedure for removing the seasonal component of a time series is called seasonal adjustment.\n",
    "* For example, most economic data published by the government is seasonally adjusted.\n",
    "* \"Fourth difference\" = periodicity of series = every 4 periods\n",
    "```\n",
    "#Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#Seasonally adjust quarterly earnings\n",
    "HRBsa = HRB.diff(4)\n",
    "#Print the first 10 rows of the seasonally adjusted series\n",
    "print(HRBsa.head(10))\n",
    "#Drop the NaN data in the first four rows\n",
    "HRBsa = HRBsa.dropna()\n",
    "#Plot the autocorrelation function of the seasonally adjusted series\n",
    "plot_acf(HRBsa)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f884be4",
   "metadata": {},
   "source": [
    "## Introducing an AR model\n",
    "* AR model = AutoRegressive Model\n",
    "* Since there is only one lagged value on the right hand side, this is called a:\n",
    "    * AR model of order 1 or;\n",
    "    * AR(1) model \n",
    "* (AR models of other orders also exist)\n",
    "* If the AR parameter $\\phi$ is 1, then the process is a random walk.\n",
    "* If the AR parameter $\\phi$ is 0, then the process is white noise. \n",
    "* In order for the process to be stable and stationary: -1 < $\\phi$ < 1\n",
    "* __Positive $\\phi$:__ Mean reversion\n",
    "* __Negative $\\phi$:__ Momentum\n",
    "* Autocorrelation decays exponentially at a rate of $\\phi$\n",
    "* Therefore, if $\\phi$ = 0.9:\n",
    "    * lag(1) Autocorrelation = 0.9\n",
    "    * lag(2) Autocorrelation = $0.9^2$\n",
    "    * lag(3) Autocorrelation = $0.9^3$\n",
    "    * lag(4) Autocorrelation = $0.9^4$\n",
    "    * etc...\n",
    "* Whe $\\phi$ is negative, the autocorrelation function still decays exponentially, but the signs $\\pm$ of the autocorrelation reverse at each lag. \n",
    "### Simulating an AR Process:\n",
    "* `statsmodels` provides models for simulating AR processes\n",
    "* There are a few conventions when using the arima_process module that require some explanation. \n",
    "* First, these routines were made very generally to handle both AR and MA models. \n",
    "* Second, when inputting the coefficients, you must include the zero-lag coefficient of 1, and the sign of the other coefficients is opposite what we have been using (to be consistent with the time series literature in signal processing). \n",
    "* For example, for an AR(1) process with , the array representing the AR parameters would be `ar = np.array([1, -0.9])`\n",
    "```\n",
    "#import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "#Plot 1: AR parameter = +0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1, -0.9])\n",
    "ma1 = np.array([1])\n",
    "AR_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = AR_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "#Plot 2: AR parameter = -0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1, 0.9])\n",
    "ma2 = np.array([1])\n",
    "AR_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = AR_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "plt.show()\n",
    "```\n",
    "* The autocorrelation function decays exponentially for an AR time series at a rate of the AR parameter. \n",
    "* A smaller AR parameter will have a steeper decay, and for a negative AR parameter, say -0.9, the decay will flip signs\n",
    "\n",
    "```\n",
    "#Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#Plot 1: AR parameter = +0.9\n",
    "plot_acf(simulated_data_1, alpha=1, lags=20)\n",
    "plt.show()\n",
    "#Plot 2: AR parameter = -0.9\n",
    "plot_acf(simulated_data_2, alpha=1, lags=20)\n",
    "plt.show()\n",
    "#Plot 3: AR parameter = +0.3\n",
    "plot_acf(simulated_data_3, alpha=1, lags=20)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2156f4",
   "metadata": {},
   "source": [
    "## Estimating and Forecasting an AR Model:\n",
    "* To estimate parameters \n",
    "```\n",
    "#Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "#Fit an AR(1) model to the first simulated data\n",
    "mod = ARMA(simulated_data_1, order=(1,0))\n",
    "res = mod.fit()\n",
    "#Print out summary information on the fit\n",
    "print(res.summary())\n",
    "#Print out the estimate for the constant and for phi\n",
    "print(\"When the true phi=0.9, the estimate of phi (and the constant) are:\")\n",
    "print(res.params)\n",
    "```\n",
    "\n",
    "```\n",
    "#Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "#Forecast the first AR(1) model\n",
    "mod = ARMA(simulated_data_1, order=(1,0))\n",
    "res = mod.fit()\n",
    "res.plot_predict(start=990, end=1010)\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "#Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "#Forecast interest rates using an AR(1) model\n",
    "mod = ARMA(interest_rate_data, order=(1,0))\n",
    "res = mod.fit()\n",
    "#Plot the original series and the forecasted series\n",
    "res.plot_predict(start=0,end=\"2022\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "#Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#Plot the interest rate series and the simulated random walk series side-by-side\n",
    "fig, axes = plt.subplots(2,1)\n",
    "#Plot the autocorrelation of the interest rate series in the top plot\n",
    "fig = plot_acf(interest_rate_data, alpha=1, lags=12, ax=axes[0])\n",
    "#Plot the autocorrelation of the simulated random walk series in the bottom plot\n",
    "fig = plot_acf(simulated_data, alpha=1, lags=12, ax=axes[1])\n",
    "#Label axes\n",
    "axes[0].set_title(\"Interest Rate Data\")\n",
    "axes[1].set_title(\"Simulated Random Walk Data\")\n",
    "plt.show()\n",
    "```\n",
    "## Choosing the right model:\n",
    "* You will ordinarily not be the told the order of the model you're trying to estimate\n",
    "* There are two techniques that can help determine the order of the AR model:\n",
    "    * __Partial Autocorrelation Function (PACF):__ measures the incremental benefit of adding another lag\n",
    "    * __Information Criteria:__ The more parameters in a model, the better the model will fit the data, but this can lead to overfitting of the data. The information criteria adjusts the goodness of fit for number of paramenters by implementing a penalty of number of parameters used.\n",
    "        * Two popular adjusted goodness-of-fit measures:\n",
    "            * __AIC (Akaike Information Criterion):__ `result.aic`\n",
    "            * __BIC (Bayesian Information Criterion):__ `result.bic`\n",
    "            * Try a few different models and choose the one with the lowest information criterion\n",
    "            \n",
    "```\n",
    "#Import the module for estimating an ARMA model\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "#Fit the data to an AR(p) for p = 0,...,6 , and save the BIC\n",
    "BIC = np.zeros(7)\n",
    "for p in range(7):\n",
    "    mod = ARMA(simulated_data_2, order=(p,0))\n",
    "    res = mod.fit()\n",
    "#Save BIC for AR(p)    \n",
    "    BIC[p] = res.bic  \n",
    "#Plot the BIC as a function of p\n",
    "plt.plot(range(1,7), BIC[1:7], marker='o')\n",
    "plt.xlabel('Order of AR Model')\n",
    "plt.ylabel('Bayesian Information Criterion')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c193660",
   "metadata": {},
   "source": [
    "* MA model = moving average\n",
    "* lag one autocorrelation: only last period's data affects current period's data (nothing older than that)\n",
    "* Note: one-period auto-correlation is not $\\theta$, but: $\\theta$ / (1 + $\\theta^2$)\n",
    "* 'the bid-ask bounce'\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.array([1])\n",
    "ma = np.array([1, 0.5])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)\n",
    "```\n",
    "\n",
    "```\n",
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "# Plot 1: MA parameter = -0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1])\n",
    "ma1 = np.array([1, -0.9])\n",
    "MA_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = MA_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: MA parameter = +0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1])\n",
    "ma2 = np.array([1, 0.9])\n",
    "MA_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = MA_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot 1: MA parameter = -0.9\n",
    "plot_acf(simulated_data_1, lags=20)\n",
    "plt.show()\n",
    "# Plot 2: MA parameter = 0.9\n",
    "plot_acf(simulated_data_2, lags=20)\n",
    "plt.show()\n",
    "# Plot 3: MA parameter = -0.3\n",
    "plot_acf(simulated_data_3, lags=20)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873a21b",
   "metadata": {},
   "source": [
    "## Estimation and forecasting an MA model\n",
    "* Estimating an MA model (same as estimating an AR model, except MA order 1 is order = (0,1), not AR order 1, which is order = (1,0))\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order= (0,1))\n",
    "result = mod.fit()\n",
    "res.plot_predict(start='2016-07-01', end = '2017-06-01')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Fit an MA(1) model to the first simulated data\n",
    "mod = ARMA(simulated_data_1, order=(0,1))\n",
    "res = mod.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res.summary())\n",
    "\n",
    "# Print out the estimate for the constant and for theta\n",
    "print(\"When the true theta=-0.9, the estimate of theta (and the constant) are:\")\n",
    "print(res.params)\n",
    "\n",
    "# Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Forecast the first MA(1) model\n",
    "mod = ARMA(simulated_data_1, order=(0,1))\n",
    "res = mod.fit()\n",
    "res.plot_predict(start=990, end=1010)\n",
    "plt.show()\n",
    "```\n",
    "## ARMA models\n",
    "* Combination of an AR model and an MA model \n",
    "* Can be converted to pure AR or pure MA models\n",
    "\n",
    "```\n",
    "# import datetime module\n",
    "import datetime\n",
    "\n",
    "# Change the first date to zero\n",
    "intraday.iloc[0,0] = 0\n",
    "\n",
    "# Change the column headers to 'DATE' and 'CLOSE'\n",
    "intraday.columns = ['DATE', 'CLOSE']\n",
    "\n",
    "# Examine the data types for each column\n",
    "print(intraday.dtypes)\n",
    "\n",
    "# Convert DATE column to numeric\n",
    "intraday['DATE'] = pd.to_numeric(intraday['DATE'])\n",
    "\n",
    "# Make the `DATE` column the new index\n",
    "intraday = intraday.set_index('DATE')\n",
    "\n",
    "# Notice that some rows are missing\n",
    "print(\"If there were no missing rows, there would be 391 rows of minute data\")\n",
    "print(\"The actual length of the DataFrame is:\", len(intraday))\n",
    "# Everything\n",
    "set_everything = set(range(391))\n",
    "\n",
    "# The intraday index as a set\n",
    "set_intraday = set(intraday.index)\n",
    "\n",
    "# Calculate the difference\n",
    "set_missing = set_everything - set_intraday\n",
    "\n",
    "# Print the difference\n",
    "print(\"Missing rows: \", set_missing)\n",
    "# Fill in the missing rows\n",
    "intraday = intraday.reindex(range(391), method='ffill')\n",
    "\n",
    "# From previous step\n",
    "intraday = intraday.reindex(range(391), method='ffill')\n",
    "\n",
    "# Change the index to the intraday times\n",
    "intraday.index = pd.date_range(start='2017-09-01 9:30', end='2017-09-01 16:00', freq='1min')\n",
    "\n",
    "# Plot the intraday time series\n",
    "intraday.plot(grid=True)\n",
    "plt.show()\n",
    "```\n",
    "* an AR(1) model is equivalent to an MA() model with the appropriate parameters\n",
    "\n",
    "```\n",
    "# import the modules for simulating data and plotting the ACF\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Build a list MA parameters\n",
    "ma = [.8**i for i in range(30)]\n",
    "\n",
    "# Simulate the MA(30) model\n",
    "ar = np.array([1])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(simulated_data, lags=30)\n",
    "plt.show()\n",
    "```\n",
    "## Cointegration models\n",
    "* What is cointegration?\n",
    "* Two series can be random walks, but the linear combination of the two series may not be a random walk (!?)\n",
    "* If that is true: each series on its own is not forecastable, but combined they are forecastable. In these cases we say the two series are **cointegrated.**\n",
    "* Analogy: Dog on a leash. The owner may follow a random walk and the dog may follow a random walk, but the difference between their positions may very well be mean reverting. \n",
    "* What type of series are cointegrated?\n",
    "    * Oil and gas\n",
    "    * With certain commodities, there may be economic forces that link the two prices (like in the instance of gas and oil)\n",
    "    * Platinum and Palladium\n",
    "    * Corn and wheat\n",
    "    * Corn and sugar\n",
    "    * ... etc...\n",
    "    * Bitcoin and Ethereum?\n",
    "* For stocks, a natural starting point for identifying cointgrated pairs are stocks in the same industry. However, competitors are not necessarily economic substitutes. (Coke vs Pepsi?)\n",
    "### Two steps to test for cointegration\n",
    "* First, regress the level of one series onto the level of the other series and get slope *c* \n",
    "* Run augmented Dickey-Fuller test for random walk on the linear combination of the two series\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.stattools import coint\n",
    "coint(P,Q)\n",
    "```\n",
    "***\n",
    "```\n",
    "# Plot the prices separately\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(7.25*HO, label='Heating Oil')\n",
    "plt.plot(NG, label='Natural Gas')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "\n",
    "# Plot the spread\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(7.25*HO-NG, label='Spread')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.axhline(y=0, linestyle='--', color='k')\n",
    "plt.show()\n",
    "\n",
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Compute the ADF for HO and NG\n",
    "result_HO = adfuller(HO['Close'])\n",
    "print(\"The p-value for the ADF test on HO is \", result_HO[1])\n",
    "result_NG = adfuller(NG['Close'])\n",
    "print(\"The p-value for the ADF test on NG is \", result_NG[1])\n",
    "\n",
    "# Compute the ADF of the spread\n",
    "result_spread = adfuller(7.25 * HO['Close'] - NG['Close'])\n",
    "print(\"The p-value for the ADF test on the spread is \", result_spread[1])\n",
    "\n",
    "# Import the statsmodels module for regression and the adfuller function\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regress BTC on ETH\n",
    "ETH = sm.add_constant(ETH)\n",
    "result = sm.OLS(BTC,ETH).fit()\n",
    "\n",
    "# Compute ADF\n",
    "b = result.params[1]\n",
    "adf_stats = adfuller(BTC['Price'] - b*ETH['Price'])\n",
    "print(\"The p-value for the ADF test is \", adf_stats[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121381ea",
   "metadata": {},
   "source": [
    "# Analyzing Temperature Data\n",
    "* Temperature data:\n",
    "    * New York City from 1870-2016\n",
    "    * Downloaded from NOAA\n",
    "    \n",
    "$\\times$ 1) Convert index to datetime object \\\n",
    "$\\times$ 2) Plot the data \\\n",
    "$\\times$ 3) Run Augmented Dickey Fuller Test to see whether the data is a random walk \\\n",
    "4) Take first differences of the data to transform it into a stationary series \\\n",
    "5) Compute ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Funcion) \\\n",
    "6) Using that as a guide, fit a few AR, MA, and ARMA models to the data \\\n",
    "7) Use information criterion to choose the best model \\\n",
    "8) Forecast temperature over next 30 years \\\n",
    "\n",
    "* __`.corr()`__\n",
    "* __`.autocorr()`__\n",
    "* __`.plot(grid=True)`__\n",
    "* __White Noise:__ a series with\n",
    "    * constant mean\n",
    "    * constant variance\n",
    "    * zero autocorrelations at all lags\n",
    "    \n",
    "* **Random Walk: First Differences**\n",
    "\n",
    "**Is Temperature a Random Walk (with Drift)?**\n",
    "* An ARMA model is a simplistic approach to forecasting climate changes, but it illustrates many points.\n",
    "\n",
    "1)\n",
    "\n",
    "```\n",
    "# Import the adfuller function from the statsmodels module\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Convert the index to a datetime object\n",
    "temp_NY.index = pd.to_datetime(temp_NY.index, format='%Y')\n",
    "\n",
    "# Plot average temperatures\n",
    "temp_NY.plot()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print ADF p-value\n",
    "result = adfuller(temp_NY['TAVG'])\n",
    "print(\"The p-value for the ADF test is \", result[1])\n",
    "```\n",
    "*** \n",
    "2)\n",
    "```\n",
    "# Import the modules for plotting the sample ACF and PACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Take first difference of the temperature Series\n",
    "chg_temp = temp_NY.diff()\n",
    "chg_temp = chg_temp.dropna()\n",
    "\n",
    "# Plot the ACF and PACF on the same page\n",
    "fig, axes = plt.subplots(2,1)\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(chg_temp, lags=20, ax=axes[0])\n",
    "\n",
    "# Plot the PACF\n",
    "plot_pacf(chg_temp, lags=20, ax=axes[1])\n",
    "plt.show()\n",
    "```\n",
    "***\n",
    "3)\n",
    "```\n",
    "# Import the module for estimating an ARMA model\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Fit the data to an AR(1) model and print AIC:\n",
    "mod_ar1 = ARMA(chg_temp, order=(1,0))\n",
    "res_ar1 = mod_ar1.fit()\n",
    "print(\"The AIC for an AR(1) is: \", res_ar1.aic)\n",
    "\n",
    "# Fit the data to an AR(2) model and print AIC:\n",
    "mod_ar2 = ARMA(chg_temp, order=(2,0))\n",
    "res_ar2 = mod_ar2.fit()\n",
    "print(\"The AIC for an AR(2) is: \", res_ar2.aic)\n",
    "\n",
    "# Fit the data to an ARMA(1,1) model and print AIC:\n",
    "mod_arma11 = ARMA(chg_temp, order=(1,1))\n",
    "res_arma11 = mod_arma11.fit()\n",
    "print(\"The AIC for an ARMA(1,1) is: \", res_arma11.aic)\n",
    "```\n",
    "***\n",
    "4)\n",
    "\n",
    "```\n",
    "# Import the ARIMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Forecast temperatures using an ARIMA(1,1,1) model\n",
    "mod = ARIMA(temp_NY, order=(1,1,1))\n",
    "res = mod.fit()\n",
    "\n",
    "# Plot the original series and the forecasted series\n",
    "res.plot_predict(start='1872-01-01', end='2046-01-01')\n",
    "plt.show()\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4664a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d18d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b783d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
