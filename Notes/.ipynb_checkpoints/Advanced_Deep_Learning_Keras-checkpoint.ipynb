{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2686dffc",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning with Keras\n",
    "This course shows you how to solve a variety of problems using the versatile Keras functional API. You will start with simple, multi-layer dense networks (also known as multi-layer perceptrons), and continue on to more complicated architectures. The course will cover how to build models with multiple inputs and a single output, as well as how to share weights between layers in a model. We will also cover advanced topics such as category embeddings and multiple-output networks. If you've ever wanted to train a network that does both classification and regression, then this course is for you!\n",
    "\n",
    "**Instructor:** Zachary Deane-Mayer, VP, Data Science at DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f71434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9b900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e5bca",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter 1: The Keras Functional API\n",
    "n this chapter, you'll become familiar with the basics of the Keras functional API. You'll build a simple functional network using functional building blocks, fit it to data, and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2824c",
   "metadata": {},
   "source": [
    "### Keras input and dense layers\n",
    "* Learn how to build complex models using Keras Functional API, including using advanced topics such as **shared layers**, **categorical embeddings**, **multiple inputs**, and **multiple outputs**.\n",
    "* The **Keras Functional API** is extremely simple, yet immensely powerful.\n",
    "* By the end of this course, **you will build a model capable of solving a regression and a classification problem at the same time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fba9d",
   "metadata": {},
   "source": [
    "## Course Datasets\n",
    "\n",
    "<img src='data/course_datasets.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55cb05",
   "metadata": {},
   "source": [
    "* The first dataset is from the regular season and has the above data.\n",
    "* For the tournament dataset, you also have the tournament \"seed\", which is a pre-tournament ranking for each team.\n",
    "    * These seeds range from 1 to 16, where the best 4 teams get a seed of 1, and the worst 4 teams get a seed of 16\n",
    "    * You will use the difference between the two teams' seeds as an input to your model.\n",
    "* The team variables are encoded as integers\n",
    "* The tournament dataset has one additional column: the difference between the tournament seeds for both teams\n",
    "* Other than the seed difference, the two datasets have identical columns \n",
    "* Within a given year, a team's roster stays relatively constant, but between years it can change a lot, as seniors graduate and freshmen start\n",
    "    * Therefore, for every year, each school is given a unique integer ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e9ff9",
   "metadata": {},
   "source": [
    "### Inputs and outputs\n",
    "* Keras models, at their simplest, are fundamentally composed of 2 parts: an **input layer** and an **output layer**\n",
    "\n",
    "#### Inputs\n",
    "* Note that the `shape` argument expects a tuple\n",
    "* The `Input()` function returns a `tensor`\n",
    "\n",
    "```\n",
    "from keras.layers import Input\n",
    "input_tensor = Input(shape=(1,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a448b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c564b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(1,))\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4794f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b47b5",
   "metadata": {},
   "source": [
    "* When we print the tensor, we see that it is a `tf.Tensor` object, which indicates it is ready to be used by our model as input\n",
    "\n",
    "#### Outputs\n",
    "* Outputs in Keras are most commonly a single `Dense` layer which specifies the shape of the expected input\n",
    "* In the case below, we are expecting our model to predict a single value, so we pass one unit to the dense layer \n",
    "* If you print the output layer, the result is NOT a tensorflow tensor, **it is a function, which takes a tensor as input and produces a tensor as output.**\n",
    "\n",
    "```\n",
    "from keras.layers import Dense\n",
    "output_layer = Dense(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d93e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbef19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x7fa6001b3d30>\n"
     ]
    }
   ],
   "source": [
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b19d067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fa6001b3d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b393c",
   "metadata": {},
   "source": [
    "* **The difference between *layers* and *tensors* is key to understanding the Keras Function API.**\n",
    "* **Layers** are used to construct a deep learning model\n",
    "* **Tensors** are used to define the data flow through the model\n",
    "* In the case above (and same case below), the `input_layers` defines a tensor, which we pass to the `output_layer()` function\n",
    "\n",
    "```\n",
    "from keras.layers import Input, Dense\n",
    "input_tensor = Input(shape=(1,))\n",
    "output_layer = Dense(1)\n",
    "output_tensor = output_layer(input_tensor)\n",
    "```\n",
    "* The final output of our model is a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb03b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = output_layer(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc968d",
   "metadata": {},
   "source": [
    "### Keras models\n",
    "* In this lesson, we'll learn how to turn the collection of layers we assembled above into an actual model that we can fit to the data and then use to predict on new data.\n",
    "\n",
    "#### Shortcut to above layer construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225d8eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 15:54:54.299531: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-19 15:54:54.299818: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create a dense layer and connect the dense layer to the input_tensor in one step\n",
    "# Note that we did this in 2 steps in the previous exercise, but are doing it in one step now\n",
    "output_tensor = Dense(1)(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401db52",
   "metadata": {},
   "source": [
    "* **Note** that there are 2 sets of parentheses in the line above, because we both create the function and then call it in the same line\n",
    "* To *build* a model, simply import the `Model()` class from keras and pass your **input and output** to this class.\n",
    "* In our case, we only have a single input and a single output, which we pass directly to the model.\n",
    "* Later in the course, we will work with multiple inputs and multiple outputs (in which case you will pass lists of input or lists of outputs to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8701f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb207e",
   "metadata": {},
   "source": [
    "#### Compile\n",
    "* Finally, you must compile the model before fitting it to the data\n",
    "* The **compilation** step finalizes the model and gets it completely ready for use in fitting and predicting\n",
    "* During compilation, select an `optimizer` and `loss` function\n",
    "* **`mae` tends to be a good general-purpose error function for keras models, as it is a little bit less sensitive to outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381f5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418a23a",
   "metadata": {},
   "source": [
    "#### Summarize the model\n",
    "* Before fitting a model, it is good practice to summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e8dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f865e",
   "metadata": {},
   "source": [
    "* The model we have defined here is a **standard linear regression model**, equivalent to $y = m*x + b$\n",
    "* $m$ and $b$ are the 2 parameters (referred to in `Param #` of `Dense` layer above) of the model\n",
    "\n",
    "\n",
    "* $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$\n",
    "* In the terminology of *linear regression*, $m$ is the slope, and $b$ is the intercept.\n",
    "* In the terminology of *Keras*, $m$ is the weight of the dense layer, and $b$ is the bias of the dense layer\n",
    "* $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$ $\\star$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1a7c5",
   "metadata": {},
   "source": [
    "### Plot model using keras\n",
    "* It is also useful to plot the model before fitting it \n",
    "* A plot gives you a little more information than a summary\n",
    "* It shows you how the layers connect together, visually\n",
    "* **Note** that in the example below, we've named the dense layer; **names are useful when you're looking at model plots, to help keep track of which layer in the plot is which layer in the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76803d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install graphviz pip install pydot pydotplus graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a91ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = Input(shape=(1,))\n",
    "output_layer = Dense(1, name = 'Predicted-Score-Diff')\n",
    "output_tensor = output_layer(input_tensor)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "plot_model(model, to_file = 'model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d00b7e",
   "metadata": {},
   "source": [
    "* ** `plot_model`** saves the image to a file, which we can then display using matplotlib's `imread()` and `imshow()` functions as shown above.\n",
    "\n",
    "### Fit and evaluate a model \n",
    "* In this lesson, we'll take the model compiled in the previous lesson and fit it to college basketball data\n",
    "* Goal: predict which team will win a tournament game\n",
    "* Data available: team ratings from the tournament organizers\n",
    "* input: `seed_diff`\n",
    "* output: `score_diff`\n",
    "* Because this model has one input and one output, it is exactly the simple model we created in the last lesson\n",
    "* Seed differences will range from -15 to 15\n",
    "* A positive seed difference is usually predictive of a negative score\n",
    "* A negative seed difference is usually predictive of a positive score difference\n",
    "* Our target variable is the game's score difference and ranges from about -50 to positive 50\n",
    "    * This means there are games where team 1 lost by 50 points, as well as games where team 1 won by 50 points\n",
    "* **Note** that both the regular season and the tournament datasets have 2 rows per game, where the second row has the opposite signs of the first row (team 1 of row 1 = team 2 of row 2 and team 2 of row 1 = team 1 of row 2).\n",
    "* You could use the simple model built above for **any regression problem with a single predictor and a single outcome**\n",
    "\n",
    "#### Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce9e0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:32:13.078141: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 17.9076 - val_loss: 16.5687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa620d74c10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games = pd.read_csv('data/games_tourney.csv')\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.fit(games['seed_diff'],\n",
    "          games['score_diff'],\n",
    "          batch_size=64,\n",
    "          validation_split = 0.2,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e863b5",
   "metadata": {},
   "source": [
    "* Once you've fit a model, it's useful to evaluate it on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8aecb",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Two Input Networks Using Categorical Embeddings, Shared Layers, and Merge Layers\n",
    "In this chapter, you will build two-input networks that use categorical embeddings to represent high-cardinality data, shared layers to specify re-usable building blocks, and merge layers to join multiple inputs to a single output. By the end of this chapter, you will have the foundational building blocks for designing neural networks with complex data flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94369da",
   "metadata": {},
   "source": [
    "### Category embeddings\n",
    "* The dataset of the regular season is *much* larger than the tournament dataset.\n",
    "* In the 2 basketball datasets we use in this course there are just under 11,000 teams.\n",
    "* Each team is encoded as an integer starting with 1 and ending with 10,887.\n",
    "* In this lesson, we'll learn how to use the team IDs as inputs to a model that learns the strength of each team.\n",
    "* **Categorical embeddings** are an advanced type of layer only available in deep learning libraries.\n",
    "    * Input: integers\n",
    "    * Output: floats\n",
    "    * **Note: Increased dimensionality: output layer (lookup table) flattens back to 2D**\n",
    "    * They are extremely useful for dealing with high cardinality categorical data\n",
    "    * **Embedding layers** are also very useful for dealing with text data (such as in Word2vec models)\n",
    "    \n",
    "    \n",
    "```\n",
    "input_tensor = Input(shape=(1,))\n",
    "n_teams = 10887\n",
    "embed_layer = Embedding(input_dim=n_teams,\n",
    "                        input_length=1\n",
    "                        output_dim=1,\n",
    "                        name='Team-Strength-Lookup')\n",
    "embed_tensor = embed_layer(input_tensor)                        \n",
    "```\n",
    "\n",
    "* Since there are 10,887 unique teams in the dataset, the input dimension of the embedding layer is 10,887\n",
    "* As you are representing each team as a single integer, use an input length of 1\n",
    "* Likewise, because we want to produce a single team strength rating, the output dimension (`output_dim` will be `1`).\n",
    "* **To use the embedding layer, connect it to the tensor produce by the input layer**\n",
    "    * This will produce an embedding output tensor\n",
    "* **Embedding layers increase the dimensionality of your data**\n",
    "    * The input CSV has two dimensions: rows and columns\n",
    "    * Embedding adds a third dimension\n",
    "    * This third dimension can be useful when dealing with images and text, but is not relevant in this course\n",
    "    * Therefore, we use the `Flatten` layer to flatten the embeddings from 3D to 2D\n",
    "    * The `Flatten` layer is also the output layer for the embedding process\n",
    "    \n",
    "#### Flattening    \n",
    "* We use the `Flatten` layer to flatten the embeddings from 3D to 2D\n",
    "* The `Flatten` layer is also the output layer for the embedding process   \n",
    "* `Flatten` layers are an advanced layer for deep learning models and can be used to transform data from multiple dimensions back down to two dimensions\n",
    "* They are useful for dealing with time series data, text data, and images. \n",
    "   \n",
    "```\n",
    "from keras.layers import Flatten\n",
    "flatten_tensor = Flatten()(embed_tensor)\n",
    "```\n",
    "\n",
    "### Putting it all together\n",
    "* You can wrap your `Embedding` layer in a model; this will allow you to reuse the model for multiple inputs in the dataset\n",
    "* You do this by defining an input later, then a flatten layer for the output\n",
    "* Finally, wrap the input tensor and flatten tensor in a model.\n",
    "* **This model can be treated exactly the same as a layer, and can be re-used inside of another model.**\n",
    "\n",
    "```\n",
    "input_tensor = Input(shape=(1,))\n",
    "n_teams = 10887\n",
    "embed_layer = Embedding(input_dim=n_teams,\n",
    "                        input_length=1,\n",
    "                        output_dim=1,\n",
    "                        name='Team-Strength-Lookup')\n",
    "embed_tensor = embed_layer(input_tensor)\n",
    "flatten_tensor = Flatten()(embed_tensor)\n",
    "model = Model(input_tensor, flatten_tensor)\n",
    "```\n",
    "\n",
    "#### Exercises: Define team lookup \n",
    "\n",
    "```\n",
    "# Imports\n",
    "from keras.layers import Embedding\n",
    "from numpy import unique\n",
    "\n",
    "# Count the unique number of teams\n",
    "n_teams = unique(games_season['team_1']).shape[0]\n",
    "\n",
    "# Create an embedding layer\n",
    "team_lookup = Embedding(input_dim=n_teams,\n",
    "                        output_dim=1,\n",
    "                        input_length=1,\n",
    "                        name='Team-Strength')\n",
    "                        \n",
    "# Imports\n",
    "from keras.layers import Input, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "# Create an input layer for the team ID\n",
    "teamid_in = Input(shape=(1,))\n",
    "\n",
    "# Lookup the input in the team strength embedding layer\n",
    "strength_lookup = team_lookup(teamid_in)\n",
    "\n",
    "# Flatten the output\n",
    "strength_lookup_flat = Flatten()(strength_lookup)\n",
    "\n",
    "# Combine the operations into a single, re-usable model\n",
    "team_strength_model = Model(teamid_in, strength_lookup_flat, name='Team-Strength-Model')                   \n",
    "```                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773b0d8",
   "metadata": {},
   "source": [
    "### Shared Layers\n",
    "* In this chapter we will create a model with two inputs: one for each team in the basketball dataset\n",
    "* However, you want these two teams to each use the same embedding layer defined in the previous lesson\n",
    "* Accomplishing this requires a **shared layer**\n",
    "* A **shared layer:**\n",
    "    * Requires the funcitonal API\n",
    "    * Very flexible \n",
    "    \n",
    "<img src='data/shared_layer.png' width=\"600\" height=\"300\" align=\"center\"/>    \n",
    "\n",
    "* **Shared layers** are an advanced deep learning concept, and are only possible with the Keras functional API\n",
    "* **They allow you to define an operation and then apply the exact same operation (with the exact same weights) on different inputs.**\n",
    "* In this model, we will shared team rating for both inputs \n",
    "* The learned rating will be the same, whether it applies to team 1 or team 2\n",
    "* To **create a shared layer**, you must first create two (or more) inputs, each of which will be passed to the shared layer.\n",
    "\n",
    "```\n",
    "input_tensor_1 = Input((1,))\n",
    "input_tensor_2 = Input((1,))\n",
    "```\n",
    "\n",
    "<img src='data/shared_layer2.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efb8ed",
   "metadata": {},
   "source": [
    "* Once you have two inputs, the magic of the Keras function API becomes apparent\n",
    "* Recall that the `Dense()` function returns a function as its output\n",
    "* This function (which `Dense` outputs) **takes a tensor as input** and **produces a tensor as output**\n",
    "* This same `Dense` function can be used to create a shared layer\n",
    "* Doing so is as simple as calling the function twice, with a different input tensor each time.\n",
    "\n",
    "\n",
    "```\n",
    "shared_layer = Dense(1)\n",
    "output_tensor_1 = shared_layer(input_tensor_1)\n",
    "output_tensor_2 = shared_layer(input_tensor_2)\n",
    "```\n",
    "\n",
    "<img src='data/shared_layer3.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c259595",
   "metadata": {},
   "source": [
    "### Sharing multiple layers as a model\n",
    "* Recall the category embedding model we made in the previous lesson which first embed an input and then flattens it.\n",
    "* **You can also share models, not just layers.**\n",
    "    * This is part of what makes the functional API so usefu\n",
    "    * $\\star$ **You can define modular components of models and then reuse them!** $\\star$\n",
    "\n",
    "<img src='data/shared_layers_code.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48f57",
   "metadata": {},
   "source": [
    "* After defining an Embedding layer and wrapping it in a model, we define 2 input tensors and pass each to the same model, producing 2 output tensors\n",
    "* This will use the same model, with the same layers and the same weights, for mapping each input to its corresponding output\n",
    "* In other words, you can take an arbitrary sequence of keras layers, and wrap them up in a model\n",
    "\n",
    "<img src='data/shared_layer4.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746de87",
   "metadata": {},
   "source": [
    "* Once you have a model, you can re-use thT model to share that sequece of steps for different input layers\n",
    "\n",
    "### Merge layers\n",
    "* Now that we've got multiple inputs and a shared layer, **we'll need to combine inputs into a single layer that we can use to predict a single output**; this requires a **merge layer.**\n",
    "* **Merge layers** allow you to define advanced, *non-sequential* network topologies.\n",
    "    * This can give you a lot of flexibility to creatively design networks to solve very specific problems\n",
    "* There are many kinds of merge layers available in Keras\n",
    "* Merge layers for simple arithmetic operations:\n",
    "    * Add\n",
    "    * Subtract\n",
    "    * Multiply\n",
    "    * Concatenate\n",
    "* **These merge layers do simple arithmetic operations *by element* on the input layers** and require them to be the same shape\n",
    "* Concatenate similar to `np.hstack()` or `pd.concat()`\n",
    "    * Unlike the other merge layers, the `Concatenate` layer can operate on layers with different numbers of columns (and rows)\n",
    "    \n",
    "    \n",
    "    \n",
    "* Let's build a simple Keras model that takes in two numbers and adds them together:\n",
    "\n",
    "```\n",
    "from keras.layers import Input, Add\n",
    "in_tensor_1 = Input((1,))\n",
    "in_tensor_2 = Input((1,))\n",
    "out_tensor = Add()([in_tensor_1, in_tensor_2])\n",
    "```\n",
    "\n",
    "<img src='data/add_layer.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42afc1",
   "metadata": {},
   "source": [
    "* If you'd like to add together many inputs, you can pass a list with more than two elements to an `Add` layer:\n",
    "\n",
    "```\n",
    "in_tensor_3 = Input((1,))\n",
    "out_tensor = Add()([in_tensor_1, in_tensor_2, in_tensor_3])\n",
    "```\n",
    "* **Note** that all of the inputs are required to have the same shape, so they can be combined element-wise.\n",
    "\n",
    "#### Create the model\n",
    "* Now you can wrap the output from your `Add` layer inside a `Model`, which will then allow you to fit it to data\n",
    "\n",
    "```\n",
    "model = Model([in_tensor_1, in_tensor_2], out_tensor)\n",
    "```\n",
    "* **Note** that above the model takes in a list of inputs because it has more than one input\n",
    "\n",
    "### Fitting and Predicting with multiple inputs \n",
    "* Keras models with multiple inputs work just like Keras model with a single input; they use the same `fit`, `predict`, and `evaluate` methods\n",
    "* The only difference is that all of these methods take a list of inputs, rather than a single input.\n",
    "\n",
    "```\n",
    "model.fit([data_1, data_2], target)\n",
    "```\n",
    "* **While this network is very simple, the concept it illustrates is quite advanced.**\n",
    "* Later in the course you will process different inputs to the network in different ways.\n",
    "* In other words, multiple inputs let you data pre-processing as part of the model you learn.\n",
    "\n",
    "#### Predict with multiple inputs\n",
    "* **To make predictions from a model with two inputs, you also need to provide two inputs to the model's `predict()` method, once again as a list.**\n",
    "\n",
    "```\n",
    "model.predict([np.array([[1]]), np.array([[2]])])\n",
    "array([[3.]], dtype=float32)\n",
    "\n",
    "model.predict([np.array([[42]]), np.array([[199]])])\n",
    "array([[161.]], dtype=float32)\n",
    "```\n",
    "\n",
    "#### Evaluate with multiple inputs\n",
    "* To evaluate a model with multiple inputs, simply give it a list of inputs, along with a single output, and the model will return its loss on the new data\n",
    "\n",
    "```\n",
    "model.evaluate([np.array(-1]]), np.array([[-2]])], np.array([[-3]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78755d90",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: Multiple Inputs: 3 Inputs (and Beyond!)\n",
    "In this chapter, you will extend your 2-input model to 3 inputs, and learn how to use Keras' summary and plot functions to understand the parameters and topology of your neural networks. By the end of the chapter, you will understand how to extend a 2-input model to 3 inputs and beyond.\n",
    "\n",
    "### Three-input models\n",
    "* This demonstrates the power of the Keras functional API\n",
    "* Once you have learned how to work with two input networks, it is trivial to extend that knowledge to 3 or more input networks \n",
    "* Making a keras model with 3 inputs is almost exactly the same as making a Keras model with two inputs. \n",
    "\n",
    "<img src='data/3_inputs.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "from keras.layers import Input, Concatenate, Dense\n",
    "in_tensor_1 = Input(shape=(1,))\n",
    "in_tensor_2 = Input(shape=(1,))\n",
    "in_tensor_3 = Input(shape=(1,))\n",
    "out_tensor = Concatenate()([in_tensor_1, in_tensor_2, in_tensor_3])\n",
    "output_tensor = Dense(1)(out_tensor)\n",
    "```\n",
    "* In this model, we use a `Concatenate()` layer to combine the inputs, but we could also have used a `Subtract()`, `Add()`, or `Multiply()` (etc) layer.\n",
    "* Finally, add a Dense layer to reduce the three inputs to a single output\n",
    "* In the exercises for this chapter, we will also practice using a shared layer in a model with more than 2 inputs\n",
    "\n",
    "```\n",
    "shared_layer = Dense(1)\n",
    "shared_tensor_1 = shared_layer()(in_tensor_1)\n",
    "shared_tensor_2 = shared_layer()(in_tensor_1)\n",
    "out_tensor = Concatenate()([shared_tensor_1, shared_tensor_2, in_tensor_3])\n",
    "out_tensor = Dense(1)(out_tensor)\n",
    "model = Model([in_tensor_1, in_tensor_2, in_tensor_3], out_tensor)\n",
    "```\n",
    "* For example (as shown above), you can pass the first two inputs to a shared layer and then concatenate the result of that shared layer with the third input\n",
    "* Compile, fit and evaluate:\n",
    "\n",
    "```\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "model.fit([[train['col1'], train['col2'], train['col3']], train_data['target'])\n",
    "\n",
    "model.evaluate([[test['col1'], test['col2'], test['col3'], test['target'])\n",
    "\n",
    "# Make a Model\n",
    "model = Model([team_in_1, team_in_2, home_in], out)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "```\n",
    "\n",
    "### Summarizing and plotting models\n",
    "* **Importantly, Keras models can have non-trainable parameters that are fixed and do not change, as well as trainable parameters, that are learned from the data when the model is fit.**\n",
    "* Models with more trainable parameters are typically more flexible \n",
    "    * **This can also make them more prone to overfitting**\n",
    "* Models with fewer trainable parameters are less flexible, but therefore less likely to overfit. \n",
    "* **A model's trainable parameters are usually in its `Dense` layers.**\n",
    "\n",
    "#### Understanding a model summary\n",
    "* Here is the summary of a slightly more complicated model \n",
    "\n",
    "<img src='data/understanding_modsum.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* You can see that this model has an `Embedding` layer.\n",
    "* Even though the `Dense` layer still only has 4 parameters, the model has many more trainable parameters because of the embedding layer\n",
    "* **It's important to remember that embedding layers often add a very large number of trainable parameters to a model**\n",
    "    * Recall that **embedding layers map integers to floats**: *each unique value of the embedding input gets a parameter for its output.*\n",
    "* Here is a plot of a slightly more complicated model:\n",
    "    \n",
    "<img src='data/plot_mod2.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Note that output layers have arrows coming in, but no arrows going out \n",
    "* Intermediary layers: have arrows going in and coming out\n",
    "* This model has a shared model: the team strength model, which is applied to two of the inputs before they are combined in the concatenate layer with the third input \n",
    "* Input layers only have one arrow going out (none coming in)\n",
    "* Here's another way of looking at the same model:\n",
    "\n",
    "<img src='data/plot_mod3.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Shared models work exactly the same as shared layers\n",
    "* This is a useful abstraction because **you can put together a sequence of layers to define a custom model, and then share the entire model in exactly the same way you'd share a layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1f510",
   "metadata": {},
   "source": [
    "## Stacking models\n",
    "* **Model stacking:** using the predictions from one model as an input to another model\n",
    "* Model stacking is a very advanced data science concept; it is the most sophisticated way of combining models, and when done right can yielf some of the most accurate models in existence.\n",
    "* Model stacking is often employed to win popular predictice modeling competitions\n",
    "* In this course so far, we've been working with 2 datasets: the college basketball data from the regular season and the college basketball data from the post-season tournament\n",
    "\n",
    "#### Stacking models requires 2 datasets\n",
    "* In this course so far, we've been working with 2 datasets: the college basketball data from the regular season and the college basketball data from the post-season tournament\n",
    "* Both datasets contain the two teams playing, whether team 1 is home or away, and the score difference of the games\n",
    "* The tournament dataset additionally contains the difference in seeds of the two teams playing\n",
    "\n",
    "#### Enrivh the tournament data\n",
    "* There's a lot more data on regular season games than there is on tournament games\n",
    "* The regular season dataset has over 300,000 rows, but the tournament dataset only has about 4,000 rows (which is pretty small)\n",
    "* Recall also that our embedding layer has about 11,000 inputs\n",
    "* 4,000 rows of data is not enough to learn all 11,000 parameters in our embedding layer\n",
    "* In the previous lesson, we build a three-input model on the regular season data\n",
    "* Now let's reuse this model to add predictions from the regular season model to the tournament dataset\n",
    "* This diagram shows the process for stacking these two models\n",
    "\n",
    "<img src='data/stack_mod_flow2.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The prediction from the regular season model captures the effects of `team_1` and `team_2`, which that you now don't need to use those two variables in the tournament model and can avoid the use of an embedding layer; now you can focus your modeling efforts on the purely numeric data, which is a little easier to work with.\n",
    "* With purely numeric inputs, you can pass all of them to a single input layer\n",
    "* In other words, an input layer with a shape of 3 is another way of defining a 3 input model\n",
    "    * The only drawback of this approach is that **all the inputs must be numeric**\n",
    "* **A huge advantage of this approach is simplicity.**\n",
    "    * You can create a model with a single input tensor and an output tensor, and fir it using a single dataset\n",
    "    * Similarly, evaluating the model requires a single dataset, rather than a list\n",
    "    \n",
    "```\n",
    "in_tensor = Input(shape=(3,))\n",
    "out_tensor = Dense(1)(in_tensor)\n",
    "\n",
    "model = Model(in_tensor, out_tensor)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "train_X = train_data[['home','seed_diff','pred']]\n",
    "train_y = train_data['score_diff']\n",
    "model.fit(train_X, train_y, epochs=10, validation_split=0.10)\n",
    "\n",
    "test_X = test_data[['home', 'seed_diff', 'pred']]\n",
    "test_y = test_data['score_diff']\n",
    "model.evaluate(test_X, test_y)\n",
    "```\n",
    "* `9.1132`\n",
    "* The model is pretty accurate. It's off, on average, by about 9 points in a given game\n",
    "* To recap: **stacking keras models means using the predictions from one model as an input to a second model.**\n",
    "    * **When stacking, it's important to use different datasets for each model.**\n",
    "    * **If your input dataset is purely numeric, you can put multiple inputs in a single input layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2349a1",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 4: Multiple Outputs\n",
    "In this chapter, you will build neural networks with multiple outputs, which can be used to solve regression problems with multiple targets. You will also build a model that solves a regression problem and a classification problem simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1ca1d",
   "metadata": {},
   "source": [
    "### Two-output models\n",
    "* In this chapter we'll cover neural networks with 2 outputs\n",
    "    * These networks make predictions for 2 targets at once\n",
    "    * For example, you could use a single model to predict the scores of both teams in a basketball game or use a single model to predict both the score difference and the win/loss outcome of that game\n",
    "    \n",
    "### Simple model with 2 outputs \n",
    "* We start with an input layer\n",
    "* To make a 2-output model, we simply make a Dense layer with 2 units for the output. The model will now make 2 predictions.\n",
    "* The API for creating a 2-output model and compiling it is exactly the same as for a single model output\n",
    "    * Wrap the input and output tensors in your call to Model()\n",
    "    * Then compile with appropriate parameters\n",
    "* **To fit a model with 2 outputs, you use a dataset with 2 columns for the y variable.**\n",
    "* In this case, the training set has the seed difference for the two teams, as well as the teams scores for the game\n",
    "* The model's single input is seed difference, and the 2 outputs are the scores for each team\n",
    "\n",
    "```\n",
    "input_tensor = Input(shape=(1,))\n",
    "output_tensor = Dense(2)(input_tensor)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "games_tourney_train[['seed_diff', 'score_1', 'score_2']].head()\n",
    "\n",
    "X = games_tourney_train[['seed_diff']]\n",
    "y = games_tourney_train[['score_1', 'score_2']]\n",
    "\n",
    "model.fit(X, y, epochs=500)\n",
    "```\n",
    "* The `fit` call is exactly the same as a single input, single output model, the only difference is that the y variable has 2 columns.\n",
    "* This particular model takes awhile to converge, so we use 500 epochs in the fit.\n",
    "\n",
    "#### Inspecting a 2 output model\n",
    "* Now that the model is fit, you can take a look at what it learned \n",
    "* The dense layer has two weights and two biases\n",
    "\n",
    "```\n",
    "model.get_weights()\n",
    "```\n",
    "\n",
    "<img src='data/get_weights.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The weights indicate that each additional unit of seed difference for the input data equals about 60 additional points for team 1 (and 60 fewer points for team 2)\n",
    "* The bias, or intercept term for each team is about 70 points, indicating that we expect an average basketball team to score about 70 points in an average game\n",
    "* In other words, 2 teams with a 1 point seed difference would be expected to have a score of about 69-71, while 2 teams with a 10 point difference would be expected to have a score of about 64 to 76\n",
    "\n",
    "#### Evaluating a model with 2 outputs\n",
    "* Evaluating a model with two outputs is very similar to evaluating a model with 1 output, except you provide the evaluation function a dataset with 2 columns of data for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f108ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "input_tensor = Input((2,))\n",
    "\n",
    "# Define the output\n",
    "output_tensor = Dense(2)(input_tensor)\n",
    "\n",
    "# Create a model\n",
    "model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss = 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3a07e",
   "metadata": {},
   "source": [
    "```\n",
    "# Fit the model\n",
    "model.fit(games_tourney_train[['seed_diff', 'pred']],\n",
    "  \t\t  games_tourney_train[['score_1', 'score_2']],\n",
    "  \t\t  verbose=True,\n",
    "  \t\t  epochs=100,\n",
    "  \t\t  batch_size=16384)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840f3dc",
   "metadata": {},
   "source": [
    "## Single model for classification and regression\n",
    "* Here we will demonstrate how to build a simple model that performs *both* classification *and* regression.\n",
    "\n",
    "#### Build a simple regressor/classifier\n",
    "* This is another example of a model with two outputs\n",
    "* In this case, however, rather than using two regression outputs, we'll have a regression output and a classification output\n",
    "* For the classification part of the model, we use the regression model prediction as input and then add another Dense output layer on top of it using the sigmoid activation, which will map the predicted score differences to probabilities that team 1 wins or loses\n",
    "* With two output models, each output needs its own loss function:\n",
    "    * For this model, we've specified two different loss functions: 1 for the regression model and 1 for the classification model \n",
    "* To fit the combination classification/regression model, you must provide the $y$ data as a list\n",
    "\n",
    "<img src='data/reg_class_flow.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "input_tensor = Input(shape=(1,))\n",
    "output_tensor_reg = Dense(1)(input_tensor)\n",
    "output_tensor_class = Dense(1, activation='sigmoid')(output_tensor_reg)\n",
    "\n",
    "model = Model(input_tensor, [output_tensor_reg, output_tensor_class])\n",
    "model.compile(loss=['mean_absolute_error', 'binary_crossentropy'], optimizer='adam')\n",
    "\n",
    "X = games_tourney_train[['seed_diff']]\n",
    "y_reg = games_tourney_train[['score_diff']]\n",
    "y_class = games_tourney_train[['won']]\n",
    "model.fit(X, [y_reg, y_class], epochs=100)\n",
    "\n",
    "model.get_weights()\n",
    "```\n",
    "* This model's weight structure will be a bit different from the last model, where both outputs were regression targets\n",
    "* The first layer has a weight of 1.24 and a bias of almost 0\n",
    "    * This means that a 1 unit change in the teams' seed difference yields about 1.24 additional points in their score difference\n",
    "    * So, two teams with a seed difference of 1 would be expected to have team 1 win by 1\n",
    "    * But, 2 teams with a seed difference of 10 would be expected to have team 1 win by 12 points\n",
    "* The next layer maps predicted score difference to predicted win/loss.\n",
    "* Recall that the final layer in the model uses sigmoid activation\n",
    "* You can manually calculate the final layer in the model (using the weights we obtained with the `model.get_weights()` call above:\n",
    "    \n",
    "```\n",
    "from scipy.special import expit as sigmoid\n",
    "print(sigmoid(1 * 0.13870609 + 0.00734114)\n",
    "```\n",
    "\n",
    "#### Evaluate the model on new data \n",
    "* First, split the evaluation dataset into a regression target and a classification target, and provide the same list of two targets to the `evaluate` method\n",
    "* This outputs 3 numbers now, instead of 1\n",
    "\n",
    "```\n",
    "X = games_tourney_test[['seed_diff']]\n",
    "y_reg = games_tourney_test[['score_diff']]\n",
    "y_class = games_tourney_test[['won']]\n",
    "model.evaluate(X, [y_reg, y_class])\n",
    "```\n",
    "* **The first number is the loss function** used by the model, which is the sum of all the output losses\n",
    "* **The second numbe is the loss for the regression** part of the model\n",
    "* **The third number is the loss for the classification** part of the model \n",
    "\n",
    "\n",
    "* So, our model has a MAE of 9.28 and a logloss of 0.58\n",
    "\n",
    "```\n",
    "# Create an input layer with 2 columns\n",
    "input_tensor = Input((2,))\n",
    "\n",
    "# Create the first output\n",
    "output_tensor_1 = Dense(1, activation='linear', use_bias=False)(input_tensor)\n",
    "\n",
    "# Create the second output (use the first output as input here)\n",
    "output_tensor_2 = Dense(1, activation='sigmoid', use_bias=False)(output_tensor_1)\n",
    "\n",
    "# Create a model with 2 outputs\n",
    "model = Model(input_tensor, [output_tensor_1, output_tensor_2])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
