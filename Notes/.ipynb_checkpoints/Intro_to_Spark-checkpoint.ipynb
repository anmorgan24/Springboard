{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5109e865",
   "metadata": {},
   "source": [
    "# Intro to Spark with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33d43a",
   "metadata": {},
   "source": [
    "### What is Spark\n",
    "* Distributed data processing framework\n",
    "* \"Yet another Hadoop\"\n",
    "* Based on Resilient Distributed Datasets\n",
    "* Used for 'big' data processing\n",
    "\n",
    "\n",
    "\n",
    "* Relational databases are great, but they don't scale above one box.\n",
    "* Relational databases: query optimization\n",
    "\n",
    "### Resilient Distributed Datasets\n",
    "* How to distribute/parallelize a big set of objects\n",
    "* We can divide in slices and keep each slice in a different nodes\n",
    "    * **Values are computed only when needed**: speed\n",
    "    * To guarantee **fault tolerance** we also keep info about how we calculated each slice, so we can re-generate it if a node fails\n",
    "    * We can hint to keep in cache, or even save on disk\n",
    "* Immutable ! not designed for read/write\n",
    "    * Instead, transform an existing one into a new one\n",
    "* It is basically a huge list\n",
    "    * But distributed over many computers\n",
    "    \n",
    "### Shared Spark Variables\n",
    "* **Broadcase variables**\n",
    "    * copy is kept at each node\n",
    "* **Accumulators**\n",
    "    * you can only add; main node can read\n",
    "\n",
    "### Functional programming in Python\n",
    "* A lot of these concepts are already in python, which is an OOP\n",
    "    * But Python community tends to promote loops\n",
    "    * **List comprehensions are more similar to functional programming**\n",
    "* Functional tools in python\n",
    "    * `map`: applies a function to each element in a list; returns another list of results; the SELECT of Python\n",
    "    * `filter`: will only select the elements in a list that satisfy a given function; the WHERE of Python\n",
    "    * `reduce`: the AGG of Python; aggregates; reduces the elements in a list into a single value or values by applying a function repeatedly to pairs of elements until you get only one value\n",
    "    * `lambda`: writing functions, simplified\n",
    "    * itertools\n",
    "        * `chain`\n",
    "        * `flatmap`: specifically used in Spark\n",
    "        \n",
    "### Map in Python\n",
    "* Python supports the map operation, over any list\n",
    "* We apply an operation to each element of a list, return a new list with the results\n",
    "* **Note:** While in Python `map` is a function, in Spark, `map` is a method of the RDD object (call with `RDD.map()`).\n",
    "\n",
    "```\n",
    "a = [1, 2, 3]\n",
    "def add1(x):\n",
    "    return x + 1\n",
    "```\n",
    "   * `map(add1, a)` $\\Rightarrow$ `[2, 3, 4]`\n",
    "   * `map(add1, [1, 2, 3])` $\\Rightarrow$ `[2, 3, 4]`\n",
    "* We usually do this with a for loop\n",
    "* This (`map`) is a slightly different way of thinking\n",
    "* **Important to note:** the original list here is never changed, rather a new list is created.\n",
    "\n",
    "### Filter\n",
    "* Select only certain elements from a list\n",
    "* Example:\n",
    "\n",
    "```\n",
    "a = [1, 2, 3, 4]\n",
    "def isOdd(x):\n",
    "    return x%2==1\n",
    "```\n",
    "* `filter(isOdd, a)` $\\Rightarrow$ `[1, 3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a1de5",
   "metadata": {},
   "source": [
    "### Reduce in Python\n",
    "* Applies a function to all pairs of elements of a list; returns ONE value, not a list\n",
    "* In Spark, `reduce` is immediate, it is not lazy\n",
    "* Example:\n",
    "\n",
    "```\n",
    "a = [1, 2, 3, 4]\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "```\n",
    "* `reduce(add, a)` $\\Rightarrow$ `10`\n",
    "    * `add(1, add(2, add(3, 4)))`\n",
    "* **Better for functions that are commutative and association doesn't matter**\n",
    "    * Jobs in Spark work in parallel\n",
    "    \n",
    "### Lambda\n",
    "* When doing map/reduce/filter, we end up with many tiny functions\n",
    "* Lambda allows us to define a function as a value, without giving it a name\n",
    "* example: `lambda x: x + 1`\n",
    "    * Can only have one expression\n",
    "    * Do not write return\n",
    "    * Option to put parenthesis around it, but usually not needed by syntax\n",
    "* `(lambda x: x + 1)(3)` $\\Rightarrow$ `4`\n",
    "* `map(lambda X: x + 1, [1, 2, 3])` $\\Rightarrow$ `[2, 3, 4]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b23d4",
   "metadata": {},
   "source": [
    "#### Exercises (1)\n",
    "* `(lambda x: 2*x)(3)` $\\Rightarrow$ **`6`**\n",
    "* `map(lambda x: 2*x, [1, 2, 3])` $\\Rightarrow$ **`[2, 4, 6]`**\n",
    "* `map(lambda t: t[0], [(1,2), (3,4), (5,6)])` $\\Rightarrow$ **`[1, 3, 5]`**\n",
    "* `reduce(lambda x,y: x+y, [1,2,3])` $\\Rightarrow$ **`6`**\n",
    "* `reduce(lambda x,y: x+y, map(lambda t: t[0], [(1,2),(3,4),(5,6)]))` $\\Rightarrow$ **`9`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf832a6f",
   "metadata": {},
   "source": [
    "#### Exercises (2)\n",
    "* Given: `a = [(1,2), (3,4), (5,6)]`\n",
    "\n",
    "    * **(a)** Write an expression to get only the second elements of each tuple\n",
    "    * **(b)** Write an expression to get the sum of the second elements\n",
    "    * **(c)** Write an expression to get the sum of the odd first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a2426",
   "metadata": {},
   "source": [
    "* `map(lambda t: t[1], a)`\n",
    "* `reduce(lambda x,y: x+y, map(lambda t: t[1], a))`\n",
    "* `reduce(lambda x, y: x+y, filter(isOdd, map(lambda t: t[0], a)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72fbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb92b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1,2), (3,4), (5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40af947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda t: t[1], a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68bc678f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x,y: x+y, map(lambda t: t[1], a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bb2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOdd(x):\n",
    "    return x%2==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab4dcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, filter(isOdd, map(lambda t: t[0], a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa718d54",
   "metadata": {},
   "source": [
    "### Flatmap\n",
    "* Sometimes we end up with a list of lists, and we want a \"flat\" list\n",
    "* Python doesn't actually have a flatmap function, but provides something similar with `itertools.chain`\n",
    "* Many functional programming languages (and Spark) provide a function called flatMap, which flattens such a list\n",
    "* For example:\n",
    "    * `map(lambda t: range(t[0], t[1], [(1,5),(7,10)])` # Returns a list of lists\n",
    "* `itertools.chain` maps a list of iterables into a flat list\n",
    "    * And so enables us to define our own flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3e1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc190f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(1, 5), range(7, 10)]\n"
     ]
    }
   ],
   "source": [
    "list(chain(map(lambda t: range(t[0], t[1]), [(1, 5), (7,10)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e786bd",
   "metadata": {},
   "source": [
    "### Creating RDDs in Spark\n",
    "* All spark commands operate on RDDs (think big distributed list)\n",
    "* You can use `sc.parellelize` to go from list to RDD\n",
    "* Later we will see how to read from files\n",
    "* Many commands are lazy (they don't actually compute the results until you need them)\n",
    "* In pySpark, `sc` represents you SparkContext\n",
    "\n",
    "### Transformations vs Actions\n",
    "* We divide RDD methods into two kinds:\n",
    "    * Transformation\n",
    "        * return another RDD\n",
    "        * are not really performed until an action is called (lazy)\n",
    "    * Actions\n",
    "        * return a value other than an RDD\n",
    "        * are performed immediately\n",
    "        \n",
    "### Some RDD methods\n",
    "\n",
    "#### Transformations\n",
    "* `.map(f)`: returns a new RDD applying f to each element\n",
    "* `.filter(f)`: returns a new RDD containing elements that satisfy f \n",
    "* `.flatmap(f)`: returns a 'flattened' list\n",
    "\n",
    "#### Actions\n",
    "* `.reduce(f)`: returns a value reducing RDD elements with f\n",
    "* `.take(n)`: returns n items from the RDD\n",
    "* `.collect()`: returns all elements as a lits\n",
    "* `.sum()`: sum of (numeric) elements of an RDD\n",
    "    * `max`, `min`, `mean`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f2750",
   "metadata": {},
   "source": [
    "credle.io as a resume builder tool: makes a great data science resume and is easy to use\n",
    "* Write a summary that focuses on data science or analytics skills used in most recent project(s)\n",
    "    * highlight these skills early and obviously \n",
    "* Always include name in document title (maybe with current date)\n",
    "* Your resume should explicitly include only the exact items that will help you get a job interview.\n",
    "* “I have experience building really fast and accurate machine-learning models in Python. I also understand big data technology like Hadoop.”\n",
    "* “I have experience using stats and machine-learning to find useful insights in data. I also have experience presenting those insights with dashboards and automated reports, and I am good at public speaking.”\n",
    "* “I am an experienced data scientist, I have a great math background, and I am good at explaining complicated stuff.”\n",
    "* How proficient do I have to be before I put a skill or technology on my resume?\n",
    "    * What am I allowed to include? My general rule of thumb is that you should not put something on your resume unless you have actual used it. Just having read about it does not count. Generally, you don’t have to have used it in a massive scale production environment, but you should have at least used it in a personal project.\n",
    "* Which things should I emphasize?\n",
    "    *  What should I emphasize? In order to decide what to emphasize, you have two great sources of information. One is the job description itself. If the job description is all about R, you should obviously emphasize R. Another, more subtle, source is the collection of skills that current employees list on LinkedIn. If someone is part of your network or has a public profile, you can see their LinkedIn profile (if you can’t see their profile, it might be worth getting a free trial for LinkedIn premium). If all of the team members have 30 endorsements for Hive, then they probably use Hive at work. You should definitely list Hive if you know it.\n",
    "* Which things should I not include?\n",
    "    * What should I not include? Because your resume is there to tell a targeted story in order to get an interview, you really should not have any skills or technologies listed that do not fit with that story.\n",
    "    \n",
    "\n",
    "\n",
    "* Including general skills like HTML and CSS is probably good, but you probably do not need to list that you are an expert in Knockout.JS and elastiCSS. This advice is doubly true for non-technical skills like “customer service” or “phone direct sales.” Including things like that actually makes the rest of your resume look worse, because it emphasizes that you have been focused on a lot of things other than data science, and — worse — that you do not really understand what the team is looking for\n",
    "* If you want to include something like that to add color to your resume, you should add it in the “Additional Info” section at the end of the resume, not in the “Skills and Technologies” section.\n",
    "\n",
    "#### What if I have no working experience?\n",
    "* If you have no working experience as a data scientist, then you have to figure out how to signal that you can do the job anyway. There are three main ways to do this: independent projects, education, and competence triggers.\n",
    "\n",
    "#### Independent projects\n",
    "If you don’t have any experience as a data scientist, then you absolutely have to do independent projects. Luckily, it is very easy to get started. The simplest way to get started is do a Kaggle competition. Kaggle is a competition site for data science problems, and there are lots of great problems with clean datasets. I wrote a step-by-step tutorial for trying your first competition using R. I recommend working through a couple of Kaggle tutorials and posting your code on Github. Posting your code is extremely important. In fact, having a Github repository posted online is a powerful signal that you are a competent data scientist (it is a competence trigger, which we will discuss in a moment).\n",
    "\n",
    "Kaggle is great, because steps 1 and 2 are completed for you. But a huge amount of data science is exactly those parts, so Kaggle can’t fully prepare you for a job as a data scientist. I will help you now with steps 1 and 2 by giving you a list of a few ideas for independent data science projects. I encourage you to steal these.\n",
    "* 1) Use Latent Semantic Analysis to extract topics from tweets. Pull the data using the Twitter API.\n",
    "* 2) Use a bag of words model to cluster the top questions on /r/AskReddit. Pull the data using the Reddit API.\n",
    "* 3) Identify interesting traffic volume spikes for certain Wikipedia pages and correlate them to news events. Access and analyze the data by using AWS Open Datasets and Amazon Elastic MapReduce.\n",
    "* 4) Find topic networks in Wikipedia by examining the link graph in Wikipedia. Use another AWS Open Datasets.\n",
    "* A few other project ideas in [How to Become a Data Hacker](https://will-stanton.com/becoming-an-effective-data-hacker/)\n",
    "\n",
    "#### Education\n",
    "Another way to prove your ability is through your educational background. If you have a Masters or a PhD in a relevant field, you should absolutely list relevant coursework and brag about your thesis. Make sure that you put your thesis work in the context of data science as much as possible. Be creative! If you really can’t think of any way that your thesis is relevant to data science, then you problem should not make a big deal out of it on your resume.\n",
    "\n",
    "#### Competence triggers and social proof\n",
    "* A Github page\n",
    "* A Kaggle profile\n",
    "* A StackExchange or Quora profile\n",
    "* A technical blog\n",
    "\n",
    "#### Resume rules of thumb\n",
    "* Keep it to one side of one page: Most recruiters only look at a resume for a few seconds. They should be able to see that you are a good candidate immediately, without turning the page.\n",
    "* Use simple formatting: Don’t do anything too fancy. It should not be hard to parse what your resume says.\n",
    "* Use appropriate industry lingo, but otherwise keep it simple: Again, this goes to readability.\n",
    "* Don’t use weird file types: PDF is good, but you should probably also attach a DOCX file. You basically should not use any other file formats, because your resume is useless if people can’t open it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86602c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d700e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54586cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6cd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280d160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb5517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd865b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e06eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91b552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca19549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97b352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdd44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce8f482",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "* pandas**\n",
    "* numpy**\n",
    "* matplotlib**\n",
    "* seaborn**\n",
    "* plotly**\n",
    "* scipy**\n",
    "* scikit image**\n",
    "* imageio**\n",
    "* statsmodels**\n",
    "* scikit learn**\n",
    "* sqlalchemy**\n",
    "* NetworkX**\n",
    "* datetime**\n",
    "* pandas_profiling**\n",
    "* geopandas**\n",
    "* pycaret**\n",
    "* catboost**\n",
    "* hyperopt**\n",
    "* random**\n",
    "* lightgbm**\n",
    "* bayes_opt**\n",
    "* pickle**\n",
    "* (22)\n",
    "***\n",
    "* io\n",
    "* pydotplus\n",
    "* zipfile\n",
    "* requests\n",
    "* json\n",
    "* ppscore\n",
    "* shap\n",
    "* pathlib\n",
    "* os\n",
    "* featuretools\n",
    "* (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6233829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
