{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c96d50",
   "metadata": {},
   "source": [
    "# sklearn's Category Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb0a5b",
   "metadata": {},
   "source": [
    "#### Handling Categorical Data\n",
    "* Creating dummy features from categorical values makes it so that you can include them in your modeling project by converting a single categorical column into many binary columns indicating the presence and absence of each categorical level (one-hot encoding)\n",
    "    * **sklearn's Category Encoders package:**\n",
    "    * **TL;DR;**\n",
    "        * Use Category Encoders to improve model performance when you have nominal or ordinal data that may provide value.\n",
    "        * For nominal columns try OneHot, Hashing, LeaveOneOut, and Target encoding. Avoid OneHot for high cardinality columns and decision tree-based algorithms.\n",
    "        * For ordinal columns try Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial are less likely to be helpful, but if you have time or theoretic reason you might want to try them.\n",
    "        * For regression tasks, Target and LeaveOneOut probably won’t work well.\n",
    "        \n",
    "    * We should (arguably) classify data as 1 of 7 types to make better models faster\n",
    "        * **Useless** — useless for machine learning algorithms, that is — discrete\n",
    "        * **Nominal** — groups without order — discrete; groups do not overlap\n",
    "        * **Binary** — either/or — discrete\n",
    "        * **Ordinal** — groups with order — discrete; natural, ordered categories\n",
    "        * **Count** — the number of occurrences — discrete\n",
    "        * **Time** — cyclical numbers with a temporal component — continuous\n",
    "        * **Interval** — positive and/or negative numbers without a temporal component — continuous\n",
    "    * **Nominal Data:**\n",
    "        * Has values that cannot be ordered in any meaningful way\n",
    "        * mosst often one-hot (dummy) encoded\n",
    "    * **Ordinal Data:**\n",
    "        * Ordinal data can be rank-ordered in a meaningful way\n",
    "        * Can be encoded in one of three ways:\n",
    "            * 1) It can be assumed to be close enough to interval data — with relatively equal magnitudes between the values — to treat it as such. \n",
    "            * 2) It can be treated as nominal data, where each category has no numeric relationship to another. You can try one-hot encoding and other encodings appropriate for nominal data.\n",
    "            * 3) The magnitude of the difference between the numbers can be ignored. You can just train your model with different encodings and see which encoding works best.\n",
    "\n",
    "#### sklearn's Category Encoders package\n",
    "   * largely derived from StatsModel's Patsy package\n",
    "   \n",
    "   #### Classic Encoders:\n",
    "        * Ordinal — convert string labels to integer values 1 through k. Ordinal.\n",
    "        * OneHot — one column for each value to compare vs. all other values. Nominal, ordinal.\n",
    "        * Binary — convert each integer to binary digits. Each binary digit gets one column. Some info loss but fewer dimensions. Ordinal.\n",
    "        * BaseN — Ordinal, Binary, or higher encoding. Nominal, ordinal. Doesn’t add much functionality. Probably avoid.\n",
    "        * Hashing — Like OneHot but fewer dimensions, some info loss due to collisions. Nominal, ordinal.\n",
    "        * Sum — Just like OneHot except one value is held constant and encoded as -1 across all columns.\n",
    "        \n",
    "   #### Contrast Encoders:\n",
    "        * The five contrast encoders all have multiple issues that I argue make them unlikely to be useful for machine learning. They all output one column for each value found in a column.\n",
    "        * Helmert (reverse) — The mean of the dependent variable for a level is compared to the mean of the dependent variable over all previous levels.\n",
    "        * Backward Difference — the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level.\n",
    "        * Polynomial — orthogonal polynomial contrasts. The coefficients taken on by polynomial coding for k=4 levels are the linear, quadratic, and cubic trends in the categorical variable.\n",
    "\n",
    "   #### Bayesian Encoders:\n",
    "       * The Bayesian encoders use information from the dependent variable in their encodings. They output one column and can work well with high cardinality data.\n",
    "       * Target — use the mean of the DV, must take steps to avoid overfitting/ response leakage. Nominal, ordinal. For classification tasks.\n",
    "       * LeaveOneOut — similar to target but avoids contamination. Nominal, ordinal. For classification tasks.\n",
    "       * WeightOfEvidence — added in v1.3. Not documented in the docs as of April 11, 2019. The method is explained in this post.\n",
    "       * James-Stein — forthcoming in v1.4. Described in the code here.\n",
    "       * M-estimator — forthcoming in v1.4. Described in the code here. Simplified target encoder.\n",
    "       \n",
    "   * Note that all Category Encoders impute missing values automatically by default. However, I recommend filling missing data data yourself prior to encoding so you can test the results of several methods.\n",
    "   * **Some terminology:**\n",
    "       * *k* is the original number of unique values in your data column\n",
    "       * *High cardinality* means a lot of unique values (a large *k*)\n",
    "       * *High dimensionality* means a matrix with many dimensions; comes with Curse of Dimensionality (often results in overfitting)\n",
    "       * *Sparse* data is a matrix with lots of zeroes relative to other values. Some algorithms may not work well with sparse data\n",
    "\n",
    "#### The basic code setup for all examples to follow:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.options.display.float_format = '{:.f}'.format #to make legibile\n",
    "\n",
    "# make some data\n",
    "df = pd.DataFrame({\n",
    "    'color':[\"a\",\"c\",\"a\",\"a\",\"b\",\"b\"],\n",
    "    'outcome':[1,2,0,0,0,1]})\n",
    "\n",
    "# set up X and y\n",
    "X = df.drop('outcome', axis = 1)\n",
    "y = df.drop('color', axis = 1)\n",
    "```\n",
    "\n",
    "#### Ordinal\n",
    "* OrdinalEncoder converts each string value to a whole number. The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on.\n",
    "* If the column contains nominal data, stopping after you use OrdinalEncoder is a bad idea. Your machine learning algorithm will treat the variable as continuous and assume the values are on a meaningful scale. Instead, if you have a column with values car, bus, and truck you should first encode this nominal data using OrdinalEncoder. Then encode it again using one of the methods appropriate to nominal data\n",
    "* If your column values are truly ordinal, that means that the integer assigned to each value is meaningful. Assignment should be done with intention. Say your column had the string values “First”, “Third”, and “Second” in it. Those values should be mapped to the corresponding integers by passing OrdinalEncoder a list of dicts:\n",
    "\n",
    "```\n",
    "[{\"col\": \"finished_race_order\", \n",
    "  \"mapping\": [(\"First\", 1), \n",
    "              (\"Second\", 2), \n",
    "              (\"Third\", 3)]\n",
    "}]\n",
    "```\n",
    "* OrdinalEncoder to transform the color column values from letters to integers:\n",
    "\n",
    "```\n",
    "ce_ord = ce.OrdinalEncoder(col = ['color'])\n",
    "ce_ord.fit_transform(X, y['outcome'])\n",
    "```\n",
    "* Scikit-learn’s OrdinalEncoder does pretty much the same thing as Category Encoder’s OrdinalEncoder, but is not quite as user friendly. Scikit-learn’s encoder won’t return a pandas DataFrame. Instead it returns a NumPy array if you pass a DataFrame. It also outputs values starting with 0, compared to OrdinalEncoder’s default of outputting values starting with 1.\n",
    "\n",
    "#### OneHot\n",
    "* One-hot encoding is the classic approach to dealing with nominal, and maybe ordinal, data. It’s referred to as the “The Standard Approach for Categorical Data” in Kaggle’s Machine Learning tutorial series. It also goes by the names dummy encoding, indicator encoding, and occasionally binary encoding.\n",
    "\n",
    "```\n",
    "ce_one_hot = ce.OneHotEncoder(col = ['color'])\n",
    "ce_one_hot.fit_transform(X, y)\n",
    "```\n",
    "* One-hot encoding can perform very well, but the number of new features is equal to k, the number of unique values. This feature expansion can create serious memory problems if your dataset has high cardinality features. One-hot-encoded data can also be difficult for decision-tree-based algorithms\n",
    "* The pandas GetDummies and scikit-learn’s OneHotEncoder functions perform the same role as the Category Encoders OneHotEncoder.\n",
    "\n",
    "#### Binary \n",
    "* Binary encoding can be thought of as a hybrid of one-hot and hashing encoders. Binary creates fewer features than one-hot, while preserving some uniqueness of values in the column. It can work well with higher dimensionality ordinal data.\n",
    "* Here’s how it works:\n",
    "    * The categories are encoded by OrdinalEncoder if they aren’t already in numeric form.\n",
    "    * Then those integers are converted into binary code, so for example 5 becomes 101 and 10 becomes 1010\n",
    "    * Then the digits from that binary string are split into separate columns. So if there are 4–7 values in an ordinal column then 3 new columns are created: one for the first bit, one for the second, and one for the third.\n",
    "    * Each observation is encoded across the columns in its binary form.\n",
    "    \n",
    "```\n",
    "ce_bin =ce.BinaryEncoder(cols = ['color'])\n",
    "ce_bin.fit_transform(X, y)\n",
    "```\n",
    "* binary really shines when the cardinality of the column is higher\n",
    "* Binary encoding creates fewer columns than one-hot encoding. It is more memory efficient. It also reduces the chances of dimensionality problems with higher cardinality.\n",
    "* Binary encoding is a decent compromise for ordinal data with high cardinality.\n",
    "* For nominal data a hashing algorithm with more fine-grained control usually makes more sense.\n",
    "\n",
    "#### BaseN\n",
    "* When the BaseN base = 1 it is basically the same as one hot encoding. When base = 2 it is basically the same as binary encoding. McGinnis said of this encoder, “Practically, this adds very little new functionality, rarely do people use base-3 or base-8 or any base other than ordinal or binary in real problems.”\n",
    "* The main reason for BaseN’s existence is to possibly make grid searching easier. You could use BaseN with scikit-learn’s GridSearchCV. However, if you’re going to grid search with these encoding options, you can make the encoder part of your scikit-learn pipeline and put the options in your parameter grid.\n",
    "\n",
    "```\n",
    "ce_basen = ce.BaseNencoder(cols = ['color'])\n",
    "ce_casen.fit_transform(X, y)\n",
    "```\n",
    "* The default base for BaseNEncoder is 2, which is the equivalent of BinaryEncoder.\n",
    "\n",
    "#### Hashing\n",
    "* HashingEncoder implements the hashing trick. It is similar to one-hot encoding but with fewer new dimensions and some info loss due to collisions. The collisions do not significantly affect performance unless there is a great deal of overlap.\n",
    "\n",
    "```\n",
    "ce_hash = ce.HashingEncoder(cols = ['color'])\n",
    "ce_hash.fit_transform(X, y)\n",
    "```\n",
    "* The n_components parameter controls the number of expanded columns. The default is eight columns.\n",
    "* If you set n_components less than k you’ll have a small reduction in the value provided by the encoded data. You’ll also have fewer dimensions.\n",
    "* You can pass a hashing algorithm of your choice to HashingEncoder; the default is md5. Hashing algorithms have been very successful in some Kaggle competitions. It’s worth trying HashingEncoder for nominal and ordinal data if you have high cardinality features.\n",
    "\n",
    "#### Recap:\n",
    "* For nominal columns try OneHot, Hashing, LeaveOneOut, and Target encoding. Avoid OneHot for high cardinality columns.\n",
    "* For ordinal columns try Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial are less likely to be helpful, but if you have time or theoretic reason you might want to try them.\n",
    "* The Bayesian encoders can work well for some machine learning tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aacea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
