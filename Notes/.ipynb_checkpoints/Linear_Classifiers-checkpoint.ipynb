{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc319ade",
   "metadata": {},
   "source": [
    "# Linear Classifiers in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe0e30",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "* Logistic Regression is a linear classifier\n",
    "* sklearn's Logistic Regression can also output confidence scores rather than \"hard\" or definite predictions with `.predict_proba()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823efd2",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "#### Using Linear SVC\n",
    "* In sklearn the basic SVM classifier is called `LinearSVC()` or Linear Support Vector Classifier\n",
    "* Note that sklearn's Logistic Regression and SVM implementations handle multiple classes (if a dataset has more than 2 classes) automatically.\n",
    "\n",
    "#### Using SVC\n",
    "* The SVC class fits a nonlinear SVM by default\n",
    "\n",
    "* **Underfitting:** model is too simple, training accuracy low\n",
    "* **Overfitting:** model is too complex, testing accuracy low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331d1cc",
   "metadata": {},
   "source": [
    "#### Linear Decision Boundaries\n",
    "* A decision boundary tells us what class our classifier will predict for any value of x\n",
    "* A decision boundary is considered **linear** when it is a line (in any orientation)\n",
    "    * This definition extends to (classifying) more than 2 features\n",
    "    * For five features, the space of possible x-values would be five-dimensional. In this case, the boundary would be a higher-dimensional **hyperplane** cutting the space into two halves.\n",
    "* A **nonlinear** boundary is any other type of boundary.\n",
    "    * Sometimes this leads to non-contiguous regions regions of a certain prediction (\"islands\", etc).\n",
    "* In their basic forms, logistic regression and SVMs are linear classifiers, which means they learn linear decision boundaries.\n",
    "    * However in some more complex forms, both may learn nonlinear decision boundaries\n",
    "\n",
    "#### Vocabulary:\n",
    "* **Classification:** learning to predict categories\n",
    "* **Regression:** learning to predict a continuous value\n",
    "* **Decision boundary:** the surface separating different predicted classes\n",
    "* **Linear classifier:** a classifier that learns linear decision boundaries \n",
    "    * e.g. logistic regression, linear SVM\n",
    "* **Linearly separable:** A data set is called linearly separable if it can be perfectly explained by a linear classifier **(straight line)**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fb944",
   "metadata": {},
   "source": [
    "#### Linear Classifiers: Prediction Equations\n",
    "\n",
    "#### Dot products\n",
    "* Create two arrays, x and y:\n",
    "\n",
    "```\n",
    "x = np.arange(3)\n",
    "y = np.arange(3, 6)\n",
    "```\n",
    "* `x = array([0, 1, 2])`\n",
    "* `y = array([3, 4, 5])`\n",
    "\n",
    "* To take the **dot product** between these two arrays, we need to multiply them element-wise.\n",
    "* The result is:\n",
    "    * 1. `x` * `y` == `array([0, 4, 10])`\n",
    "    * 2. The sum of the numbers in this array (0 + 4 + 10) or `np.sum(x*y)` = `14`\n",
    "* A convenient notation for this is `@`\n",
    "    * `x@y` = 14\n",
    "    * In math notation, this is written x dot y\n",
    "* You can think of a **dot product** as multiplication in higher dimensions, since x and y are arrays of values\n",
    "* Using dot products, we can express how linear classifiers make predictions \n",
    "\n",
    "#### Linear classifier predictions:\n",
    "* `raw model output = coefficients * features + intercept`\n",
    "    * Dot product of coefficients and features, plus an intercept.\n",
    "* Linear classifier prediction: compute raw model output, check the **sign**:\n",
    "    * If **positive**, predict one class\n",
    "    * If **negative**, predict the other class\n",
    "    \n",
    "* Crucially, this pattern is the same for logistic regression and linear SVMs\n",
    "* In sklearn terms, we can say logistic regression and linear SVM have different `fit` functions but same `predict` function.\n",
    "    * The differences in `fit` relate to loss functions\n",
    "    \n",
    "* We can get the learned coefficients and intercept with:\n",
    "    * `lr.coef_`\n",
    "    * `lr.intercept_`\n",
    "* To compute raw model output for example 10:\n",
    "    * `lr.coef_ @ X[10] + lr.intercept_`\n",
    "        * If the raw model output is negative, then we predict the negative class (\"0\", for example)\n",
    "* In general, this is what the predict function does for *any* X: it computes the raw model output, checks if it's positive or negative, and then returns result based on the names of the classes in your data set (for example, \"0\" and \"1\").\n",
    "* The sign (positive or negative), tells you what side of the decision boundary you're on, and thus, your prediction\n",
    "* Along the decision boundary itself, the raw model output is zero\n",
    "* Furthermore, the values of the coefficients and intercept determine the boundary \n",
    "* When you call `fit` with scikit-learn, the logistic regression coefficients are automatically learned from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48794d52",
   "metadata": {},
   "source": [
    "#### Loss functions\n",
    "* Many machine learning algorithms involve minimizing a loss function (example loss function: least squares)\n",
    "* You can think of minimizing the loss as jiggling around the coefficients or parameters of the model until this error term, or loss function is as small as possible\n",
    "* In other words, the loss function is a penalty score that tells us how well (or, to be precise, how poorly) the model is doing on the training data\n",
    "* We can think of the `fit` function as **running code that minimizes the loss.**\n",
    "* \"Minimization\" is with respect to the coefficients or parameters of the model\n",
    "* **Note** that the `.score` function in sklearn isn't necessarily the loss function\n",
    "* **The loss is used to fit the model on the data and the score is used to see how well we're doing.**\n",
    "* Often, however, they are the same.\n",
    "\n",
    "#### Classification errors: the 0-1 loss\n",
    "* Squared loss is not appropriate for classification problems, because our y-values are categories, not numbers\n",
    "* For classification, a natural quantity to think about is the number of errors we've made \n",
    "* This is the **0-1 loss**: it's 0 for a correct prediction and 1 for an incorrect prediction\n",
    "* By summing this function over all the training samples, we get the total number of mistakes we've made on the training set, since we add 1 to the total for each mistake\n",
    "* While the 0-1 loss is important to understand conceptually, it turns out to be very hard to minimize it directly in practice (which is why logistic regression and SVM don't use it)\n",
    "\n",
    "#### Minimizing a loss\n",
    "* with `scipy.optimize.minimize`\n",
    "* `minimize(np.square, 0).x`\n",
    "    * first argument represents equation to be minimized: $y=x^2$\n",
    "    * the second argument is our initial guess \n",
    "    * `.x` at the end to grab the input value that makes the function as small as possible\n",
    "    * Think of the code as answering the question, \"What values of the model coefficients make my sqaured error as small as possible?\"\n",
    "    \n",
    "#### Implementing linear regression \"from scratch\"\n",
    "```\n",
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_pred - y_i_true)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa182d",
   "metadata": {},
   "source": [
    "#### Loss function diagrams\n",
    "* In linear regression, the raw model output is the prediction\n",
    "* Intuitively, the loss is higher as the prediction is further away\n",
    "* This makes sense for linear regression, but not for a linear classifier; we need specialized loss functions for classification, and can't just use the squared error from linear regression\n",
    "* Logistic loss is like a \"smooth version of the 0-1 loss.\"\n",
    "* Hinge loss, used in SVM: the general shape is the same as for logistic loss, both penalize incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf841ef",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "#### Logistic Regression and Regularization\n",
    "* Regularization combats overfitting by making the model coefficients smaller\n",
    "* In sklearn, the hyperparameter `C` is the inverse of the regularization strength\n",
    "    * In other words, larger `C` means less regularization, and smaller `C` means more regularization.\n",
    "* How does regularization affect training and testing accuracy?\n",
    "    * \n",
    "    \n",
    "```\n",
    "lr_weak_reg = LogisticRegression(C = 100)\n",
    "lr_strong_reg = LogisticRegression(C = 0.01)\n",
    "\n",
    "lr_weak_reg.fit(X_train, y_train)\n",
    "lr_strong_reg.fit(X_train, y_train)\n",
    "\n",
    "lr_weak_reg.score(X_train, y_train)\n",
    "lr_strong_reg.score(X_train, y_train)\n",
    "```\n",
    "* Outputs: \n",
    "* `1.0`\n",
    "* `0.92`\n",
    "* The model with weak regularization gets a higher training accuracy \n",
    "* Regularization is an extra term that we add to the original loss function, which penalizes large values of the coefficients\n",
    "* Intuitively, without regularization, we are maximizing the training accuracy, so we do well on that metric \n",
    "* Regularized loss = original loss + large coefficient penalty \n",
    "* But, how does regularization affect test accuracy?\n",
    "\n",
    "```\n",
    "lr_weak_reg.score(X_test, y_test)\n",
    "lr_strong_reg.score(X_test, y_test)\n",
    "```\n",
    "* Outputs: \n",
    "* `0.86`\n",
    "* `0.88`\n",
    "* More regularization (almost always) leads to higher test accuracy\n",
    "* Regularizing, and thus making your coefficient smaller, is like a compromise between not using the feature at all (setting the coefficient to zero) and fully using it (the un-regularized coefficient value). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49672e99",
   "metadata": {},
   "source": [
    "#### L1 vs L2 Regularization\n",
    "* For linear regression, we use the terms Ridge and Lasso for two different types of regularization\n",
    "* **Lasso = L1 Regularization**\n",
    "* **Ridge = L2 Regularization**\n",
    "* Both help reduce overfitting, and L1 also performs feature selection\n",
    "* **Scaling features is usually good practice, especially when using regularization.**\n",
    "* Plot coefficients of L1 and L2 regularized models:\n",
    "\n",
    "```\n",
    "plt.plot(lr_l1.coef_.flatten())\n",
    "plt.plot(lr_l2.coef_.flatter())\n",
    "```\n",
    "\n",
    "```\n",
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C=C_value)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "    \n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f4a7f",
   "metadata": {},
   "source": [
    "#### Logistic Regression and Probabilities\n",
    "* **Hard predictions:** we predict either one class or the other; \"yes\" or \"no\", \"black\" or \"white\", \"apple\" or \"banana\"\n",
    "* **Soft predictions:** interpretting the raw model output as a probability.\n",
    "* Soft vs. hard predictions similar to soft vs. hard voting\n",
    "* Remember: output probabilities with **`.predict_proba()`**\n",
    "* When we \"turn on\" regularization, we see the coefficients get smaller. The effect of regularization is that probabilities are close to 0.5; in other words, smaller coefficients mean less confident predictions\n",
    "* Connection between overconfidence and overfitting\n",
    "* If you have multiple features, you have multiple coefficients; **the ratio of the coefficients represents the slope of the line and the magnitude of the coefficients gives us our confidence level.**\n",
    "* **Regularization affects not only confidence, but also the orientation of the boundary.** \n",
    "\n",
    "#### How are logistic regression probabilities computed?\n",
    "* logistic regression predictions come from the sign of the raw model output\n",
    "* The raw model output can be any number but probabilities are from 0 to 1; so, logistic regression probabilities are \"squashed\" raw model output (\"squashed\" to be between 0 and 1): \n",
    "* **Sigmoid Function:** When the raw model output is 0, then the probability is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20603946",
   "metadata": {},
   "source": [
    "#### Multi-class logistic regression\n",
    "* Multi-class classification means having more than 2 classes\n",
    "* Two popular approaches to multi-class classification:\n",
    "\n",
    "#### 1. **Train a series of binary classifiers for each class**\n",
    "* aka combine binary classifiers with **one-vs-many**\n",
    "   \n",
    "```\n",
    "# One vs rest strategy\n",
    "lr0.fit(X, y == 0)\n",
    "lr1.fit(X, y == 1)\n",
    "lr2.fit(X, y == 2)\n",
    "```\n",
    "* **The code `y==0` returns an array the size of y, that's True when y equals 0 and False otherwise, so the classifier learns to predict these values.**\n",
    "    * In other words, it's a binary classifier learning to distinguish between \"zero\" and \"not zero.\"\n",
    "* In order to make predictions using one-vs-rest, we take the class (`y==0`, `y==1`, etc) whose classifier gives the largest raw model output- or, `decision_function` in sklearn terminology.\n",
    "\n",
    "```\n",
    "# Get raw model output \n",
    "lr0.decision_function(X)[0]\n",
    "lr1.decision_function(X)[0]\n",
    "lr2.decision_function(X)[0]\n",
    "```\n",
    "* In our example, let's say the largest raw model output comes from classifier 0. \n",
    "* This means, it's more confident that the class is 0 than any of the other classes, so we predict class 0.\n",
    "* **One vs rest is the default behavior of sklearn's Logistic Regression**\n",
    "* We can just let sklearn do the work by fitting a logistic regression model on the original multi-class dataset. \n",
    "\n",
    "* Another way to achieve multi-class classification with logistic regression is to modify the loss function so that it directly tries to optimize accuracy on the multi-class problem. Various words related to this concept:\n",
    "    * Multinomial logistic regression\n",
    "    * Softmax\n",
    "    * Cross-entropy loss\n",
    "    \n",
    "* **One-vs-rest:**\n",
    "    * fit a binary classifier for each class\n",
    "    * predict with all, take largest output\n",
    "    * **pro:** simple, modular; reuse your binary classifier implementation rather than needing a new one\n",
    "    * **con:** not directly optimizing accuracy\n",
    "    * common for SVMs as well\n",
    "    * can produce probabilities\n",
    "\n",
    "* **\"Multinomial\" or \"softmax\":**\n",
    "    * fit a single classifier to all classes \n",
    "    * prediction directly outputs best class\n",
    "    * **con:** more complicated, new code\n",
    "    * **pro:** tackles the problem directly (loss more directly aligned with accuracy than one-vs-rest)\n",
    "    * In the field of neural networks, the multinomial approach is standard\n",
    "    * possible for SVMs, but less common\n",
    "    * can product probabilities \n",
    "\n",
    "#### Model coefficients for multi-class\n",
    "\n",
    "```\n",
    "lr_mn = LogisticRegression(multi_class = \"multinomial\", solver= \"lbfgs\")\n",
    "lr_mn.fit(X, y)\n",
    "\n",
    "lr_mn.coef_.shape\n",
    "lr_mn.intercept_.shape\n",
    "```\n",
    "* We can instantiate the multinomial version by setting the multi_class argument. In sklearn, this also requires changing to a non-default solver (like \"lbfgs\").\n",
    "* The **solver** hyperparameter specifies the algorithm usd to minimize the loss; the default algorithm is for the binary problem, so it can be used for one-vs-rest but not multinomial.\n",
    "* The multinomial classifier has the same number of coefficients and intercepts as one-vs-rest\n",
    "\n",
    "```\n",
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression()\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class='multinomial', solver = 'lbfgs')\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdfc4c",
   "metadata": {},
   "source": [
    "### Support Vectors\n",
    "#### What is an SVM?\n",
    "* Linear SVMs are also **linear classifiers**, but they use **hinge loss** instead (as well as **L2 regularization**)\n",
    "* RECAP: logistic and hinge loss look very similar, but a key difference is in the \"flat\" part of the hinge loss, which occurs when the raw model output is greater than one (meaning you predicted some example correctly, beyond some margin of error\n",
    "    * If a training error falls in this \"zero loss\" region, it doesn't contribute to the fit (if we removed that example, nothing would change).\n",
    "    * $\\Uparrow$ **This** is a key property of SVMs.\n",
    "* **Support Vectors** are defined as examples that are **not** in the flat part of the loss diagram.\n",
    "    * **Another way of defining support vectors is that they include incorrectly classified examples, as well as correctly classified examples that are close to the boundary.**\n",
    "    * If you're wondering how close is close enough, this is controlled by the regularization strength.\n",
    "    * **Support Vectors** Are the examples that matter to your fit. \n",
    "    * If an example is not a support vector, removing it has no effect on the model, because its loss was already zero.\n",
    "    * **Critically important:** Having a small number of support vectors makes Kernel SVMs really fast. \n",
    "        * Part of the speed comes from clever algorithms whose running time only scales with the number of support vectors, rather than the total number of training examples.\n",
    "        \n",
    "#### Max-margin viewpoint\n",
    "* The SVM maximizes the margin for linearly separable datasets\n",
    "* **Margin:** distance from the boundary to the closest points.\n",
    "* If the regularization strength is not too strong, SVMs maximize the margin of linearly separable datasets\n",
    "* Unfortunately, most datasets are not linearly separable\n",
    "\n",
    "```\n",
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X, y)\n",
    "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small, y_small)\n",
    "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))\n",
    "```\n",
    "\n",
    "#### Kernel SVMs\n",
    "* How to fit nonlinear boundaries using linear classifiers\n",
    "* **Fitting a linear model in a transformed space corresponds to fitting a nonlinear model in the original space.**\n",
    "* Kernels and kernel SVMs implement feature transformation in a computationally efficient way\n",
    "* default behavior for SVC() is `rbf`, or **Radial Basis Function kernel**.\n",
    "* While many nonlinear kernels exist, in this course we focus on the RBF kernel.\n",
    "* We can control the shape of the boundary using the hyperparameters (`C` controls regularization)\n",
    "* The RBF kernel also introduces a new hyperparamter, `gamma`, **which controls the smoothness of the boundary.**\n",
    "    * By decreasing `gamma`, we can make the boundaries smoother.\n",
    "\n",
    "```\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma=1)\n",
    "```\n",
    "\n",
    "* With the right hyperparameters, RBF SVMs are capable of perfectly separating almost any data set (however, naturally this leads to overfitting and creates islands)\n",
    "* **Kernel hyperparameters affect the tradeoff between training and test accuracy.**\n",
    "#### Tuning gamma with GridSearchCV\n",
    "\n",
    "```\n",
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X, y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "```\n",
    "\n",
    "#### Tuning gamma and C with GridSearchCV\n",
    "\n",
    "```\n",
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1cdc9",
   "metadata": {},
   "source": [
    "#### Comparing Logistic Regression and SVMs \n",
    "* Here we'll compare our two linear classifiers: logistic regression and linear SVMs\n",
    "\n",
    "#### Logistic Regression:\n",
    "* Is a linear classifier\n",
    "* Can be used with kernels, but slow\n",
    "* Outputs meaningful probabilities\n",
    "* Can be extended to multi-class \n",
    "    * with one-vs-rest scheme or by directly modifying the loss\n",
    "* All data points affect fit\n",
    "* L2 or L1 regularization\n",
    "* imported from `sklearn.linear_model.LogisticRegression`\n",
    "* **Fundamental hyperparameters:**\n",
    "    * `C` = inverse regularization strength\n",
    "    * `penalty` = type of regularization; sklearn supports `L1` and `L2`\n",
    "    * `multi_class` = extends binary classifier to multi-class\n",
    "\n",
    "#### Support Vector Machines (SVMs)\n",
    "* Is a linear classifier\n",
    "* Can be used with kernels, and fast\n",
    "* Does not naturally output probabilities\n",
    "* Can be extended to multi-class \n",
    "    * with one-vs-rest scheme or by directly modifying the loss\n",
    "* Only \"support vectors\" affect fit\n",
    "* Conventionally just L2\n",
    "* imported from `sklearn.svm.LinearSVC` and `sklearn.svm.SVC`\n",
    "* **Fundamental hyperparameters:**\n",
    "    * `C` = inverse regularization strength\n",
    "    * `kernel` = type of kernel (we talked about linear and RBF here, but there are others as well)\n",
    "    * `gamma` = inverse RBF smoothness\n",
    "    \n",
    "#### SGDClassifier\n",
    "* `SGDClassifier`: scales well to large datasets\n",
    "* SGD stands for **stochastic gradient descent.**\n",
    "* can handle very large data sets much better than the other methods we've discussed.\n",
    "* to switch between logistic regression and a linear SVM within an SGDClassifier, we only need to set the loss hyperparameter of the SGDClassifier:\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "logreg = SGDClassifier(loss = 'log')\n",
    "linsvm = SGDClassifier(loss = 'hinge')\n",
    "```\n",
    "* SGDClassifier works much like the sklearn classifiers we've seen, however **the regularization hyperparameter is `alpha` and is like the inverse of `C` or `1/C`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163016ed",
   "metadata": {},
   "source": [
    "```\n",
    "# We set random_state=0 for reproducibility \n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss':['hinge', 'log'], 'penalty':['l1','l2']}\n",
    "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7a328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
