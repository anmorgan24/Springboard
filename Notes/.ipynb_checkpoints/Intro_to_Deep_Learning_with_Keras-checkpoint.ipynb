{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7958050",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras\n",
    "Deep learning is here to stay! It's the go-to technique to solve complex problems that arise with unstructured data and an incredible tool for innovation. Keras is one of the frameworks that make it easier to start developing deep learning models, and it's versatile enough to build industry-ready models in no time. In this course, you will learn regression and save the earth by predicting asteroid trajectories, apply binary classification to distinguish between real and fake dollar bills, use multiclass classification to decide who threw which dart at a dart board, learn to use neural networks to reconstruct noisy images and much more. Additionally, you will learn how to better control your models during training and how to tune them to boost their performance.\n",
    "\n",
    "**Instructor:** Miguel Esteban, co-founder of Xtreme AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38dbd8",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter 1: Introducing Keras\n",
    "In this first chapter, you will get introduced to neural networks, understand what kind of problems they can solve, and when to use them. You will also build several networks and save the earth by training a regression model that approximates the orbit of a meteor that is approaching us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa28966",
   "metadata": {},
   "source": [
    "* Keras is a high-level deep learning framework.\n",
    "\n",
    "#### Theano vs Keras\n",
    "* Theano is a lower level framework\n",
    "* Building a neural network in Theano can take many lines of code and requires a deep understanding of how they work internally \n",
    "\n",
    "<img src='data/Theano_vs_Keras.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* Building and training this very same network in Keras only takes a few lines of code\n",
    "\n",
    "#### Keras\n",
    "* Open source deep learning Framework\n",
    "* Enables fast experimentation with neural networks\n",
    "* Runs on top of other frameworks like TensorFlow, Theano or CNTK\n",
    "* Created by French AI researcher Fran√ßois Chollet\n",
    "\n",
    "#### Why Keras instead of other low-level libraries like TensorFlow?\n",
    "* Fast industry-ready models\n",
    "* For beginners and experts\n",
    "* Less code\n",
    "* Allows for quickly and easily checking if a neural neetwork will solve your problems\n",
    "* Build any architecture, from:\n",
    "    * simple networks\n",
    "    * more complex networks\n",
    "    * auto-encoders\n",
    "    * convolutional neural networks (CNNs)\n",
    "    * recurrent neural networks (RNNs)\n",
    "    * Deploy models in multiple platforms (Android, iOS, web-apps, etc.)\n",
    "* Keras is now fully integrated into TensorFlow2 and is TensorFlow's high-level framework of choice\n",
    "* Keras is complementary to TensorFlow\n",
    "* **Can also use TensorFlow in same code pipeline if, as you dive into deep learning, you find yourself needing to use low-level features, have a finer control of how your network applies gradients, etc.**\n",
    "\n",
    "#### Feature Engineering\n",
    "* Neural networks are good feature extractors, since they learn the best way to make sense of **unstructured data.**\n",
    "* NNs can learn the best features and their combinations, and can perform feature engineering themselves\n",
    "\n",
    "#### Unstructured data\n",
    "* **Unstructured data** is data that is not easily put into a table\n",
    "* Examples: sound, video, images, etc.\n",
    "* These are also the types of data where performing feature engineering can be more challenging, so leaving this task to NNs is often a good idea.\n",
    "\n",
    "#### When to use neural networks\n",
    "* Dealing with unstructured data\n",
    "* Don't need easily interpretable results\n",
    "* You can benefit from a known architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7d743",
   "metadata": {},
   "source": [
    "### Simple Neural Networks\n",
    "* A **neural network** is a machine learning algorithm with the training data being the input to the input layer and the predicted value being the value at the output layer\n",
    "* Each connection from one neuron to another has an associated weight, $w$.\n",
    "\n",
    "<img src='data/bias_weight.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cda15",
   "metadata": {},
   "source": [
    "* Each neuron, except the input layer which just holds the input value, also has an extra weight we call the bias weight, $b$.\n",
    "* During **forward propagation** (feed-forward), our input gets transformed by weight multiplications and additions at each layer, the output of each neuron can also get transformed by the application of what's called an **activation function**.\n",
    "\n",
    "<img src='data/grad_descent.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6095c6",
   "metadata": {},
   "source": [
    "* Learning in neural networks consists of tuning the weights or parameters to give the desired output\n",
    "* One way of achieving this is by using gradient descent and applying weight updates incrementally via back-propagation\n",
    "\n",
    "### The sequential API\n",
    "* Keras allows you to build models in 2 different ways; using either the Functional API or the Sequential API\n",
    "* The sequential API is a simple, yet very powerful way of building neural networks that will get you covered for most use cases\n",
    "* With the sequential API, you're essentially building a model as a stack of layers\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a new sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input AND dense layer\n",
    "model.add(Dense(2, input_shape=(3,)))\n",
    "```\n",
    "* **In this last line of code, *we add 2 layers*: a 2-neuron Dense fully-connected layer, and an input later consisting of 3 neurons.**\n",
    "\n",
    "```\n",
    "# Add a final 1-neuron layer\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "#### model.summary()\n",
    "* **Parameters** are the weights, including the bias weight of each neuron in a given layer (or the model as a whole)\n",
    "* **When the input layer is defined via the `input_shape` parameter, as we did before, it is not shown as a layer in the summary, but is included in the layer where it was defined** (in the case above, within the first Dense layer).\n",
    "\n",
    "<img src='data/visualize_parameters.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5b934",
   "metadata": {},
   "source": [
    "* The image above helps illustrate why the layer **`dense_3`** has **8 parameters**:\n",
    "    * **6 parameters** (or weights) come from the connections of the 3 input neurons to the 2 neurons in the layer (green lines)\n",
    "    * **2 parameters** come from the bias weights, `b0`, and `b1`, 1 per each neuron in the hidden layer\n",
    "    \n",
    "#### Exercises: Hello nets!\n",
    "You're going to build a simple neural network to get a feeling of how quickly it is to accomplish this in Keras.\n",
    "\n",
    "You will build a network that **takes two numbers as an input**, passes them through **a hidden layer of 10 neurons**, and finally **outputs a single non-constrained number**.\n",
    "\n",
    "A **non-constrained output can be obtained by avoiding setting an activation function in the output layer**. This is useful for problems like regression, when we want our output to be able to take any non-constrained value.\n",
    "\n",
    "<img src='data/ex1_vis.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "# Import the Sequential model and Dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066819e4",
   "metadata": {},
   "source": [
    "#### Exercises: Counting Parameters\n",
    "You've just created a neural network. But you're going to create a new one now, taking some time to think about the weights of each layer. The Keras `Dense` layer and the `Sequential` model are already loaded for you to use.\n",
    "\n",
    "This is the network you will be creating:\n",
    "\n",
    "<img src='data/ex2_vis.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3,), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()\n",
    "```\n",
    "**There are 20 parameters, 15 from the connections of our inputs to our hidden layer and 5 from the bias weight of each neuron in the hidden layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d6f1b",
   "metadata": {},
   "source": [
    "#### Exercises: Build as shown!\n",
    "You will take on a final challenge before moving on to the next lesson. Build the network shown in the picture below. Prove your mastered Keras basics in no time!\n",
    "\n",
    "<img src='data/ex3_vis.png' width=\"200\" height=\"100\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Build the input and hidden layer\n",
    "model.add(Dense(3, input_shape=(2,)))\n",
    "\n",
    "# Add the ouput layer\n",
    "model.add(Dense(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f9fd3",
   "metadata": {},
   "source": [
    "### Surviving a meteor\n",
    "* **`loss_function` is the function we are trying minimize during training.**\n",
    "* **Compiling a model produces no output.**\n",
    "    * But, our model is now ready to train.\n",
    "* Creating a model is useless if we don't train it.\n",
    "\n",
    "#### Training\n",
    "\n",
    "```\n",
    "# Train your model\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "#### Predicting\n",
    "* We can store predictions in a variable for later use.\n",
    "* The predictions are stored as numbers in a numpy array \n",
    "\n",
    "```\n",
    "# Predict on new data\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Look at the predicitons\n",
    "print(preds)\n",
    "```\n",
    "\n",
    "#### Evaluating\n",
    "* Feed-forward consists in computing a model's output from a given set of inputs\n",
    "* It then computes the error comparing the results to the true values stored in `y_test`\n",
    "\n",
    "```\n",
    "# Evaluate your results\n",
    "model.evaluate(X_test, y_test)\n",
    "```\n",
    "\n",
    "#### Exercises: Specifying a model \n",
    "You will build a simple regression model to predict the orbit of the meteor!\n",
    "\n",
    "Your training data consist of measurements taken at time steps from **-10 minutes before the impact region to +10 minutes after**. Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor orbit at that time step.\n",
    "\n",
    "*Note that you can view this problem as approximating a quadratic function via the use of neural networks.*\n",
    "\n",
    "<img src='data/impact_reg.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "This data is stored in two numpy arrays: one called `time_steps` , what we call *features*, and another called `y_positions`, with the *labels*. Go on and build your model! It should be able to predict the y positions for the meteor orbit at future time steps.\n",
    "\n",
    "Keras `Sequential` model and `Dense` layers are available for you to use.\n",
    "\n",
    "```\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "#### Training\n",
    "You're going to train your first model in this course, and for a good cause!\n",
    "\n",
    "Remember that **before training your Keras models you need to compile them**. This can be done with the `.compile()` method. The `.compile()` method takes arguments such as the `optimizer`, used for weight updating, and the `loss` function, which is what we want to minimize. Training your model is as easy as calling the `.fit()` method, passing on the features, labels and a number of epochs to train for.\n",
    "\n",
    "The regression `model` you built in the previous exercise is loaded for you to use, along with the `time_steps` and `y_positions` data. Train it and evaluate it on this very same data, let's see if your model can learn the meteor's trajectory.\n",
    "\n",
    "```\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))\n",
    "```\n",
    "\n",
    "#### Exercises: Predicting the orbit!\n",
    "You've already trained a `model` that approximates the orbit of the meteor approaching Earth and it's loaded for you to use.\n",
    "\n",
    "Since you trained your model for values between -10 and 10 minutes, your model hasn't yet seen any other values for different time steps. You will now visualize how your model behaves on unseen data.\n",
    "\n",
    "If you want to check the source code of `plot_orbit`, paste `show_code(plot_orbit)` into the console.\n",
    "\n",
    "Hurry up, the Earth is running out of time!\n",
    "\n",
    "*Remember `np.arange(x,y)` produces a range of values from **x** to **y-1**. That is the `[x, y)` interval.*\n",
    "\n",
    "```\n",
    "# Predict the twenty minutes orbit\n",
    "twenty_min_orbit = model.predict(np.arange(-10, 11))\n",
    "\n",
    "# Plot the twenty minute orbit \n",
    "plot_orbit(twenty_min_orbit)\n",
    "\n",
    "# Predict the eighty minute orbit\n",
    "eighty_min_orbit = model.predict(np.arange(-40, 41))\n",
    "\n",
    "# Plot the eighty minute orbit \n",
    "plot_orbit(eighty_min_orbit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58a297",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "* We use binary classification when we want to solve problems where you predict whether an observation belongs to one of two possible classes\n",
    "\n",
    "<img src='data/bin_class_plot.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f590203",
   "metadata": {},
   "source": [
    "* The coordinates are pairs of values corresponding to the X and Y coordinates of each circle in the graph\n",
    "* The labels are `1` for red circles and `0` for blue circles \n",
    "\n",
    "#### Pairplot\n",
    "* We can make use of seaborn's `pairplot` function to explore a small dataset and identify whether our classification problem will be easily separable\n",
    "* We can get an intuition for this is we see that the classes separate well-enough along several variables\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot a pairplot\n",
    "sns.pairplot(circles, hue='target')\n",
    "```\n",
    "\n",
    "<img src='data/pplots.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52461c6d",
   "metadata": {},
   "source": [
    "* In this case, for the circles dataset, there is a very clear boundary. The red circle concentrate at the center while the blue are outside. It should be easy for our network to find a way to separate them just based on x and y coordinates\n",
    "\n",
    "#### The sigmoid function\n",
    "* You can consider the output of the sigmoid function as the probability of a pair of coordinates being in one class or another\n",
    "* So we can set a threshold and say everything below 0.5 will be a blue circle and everything above 0.5, a red circle.\n",
    "\n",
    "#### Binary crossentropy\n",
    "* **Binary cross-entropy** is the function we use when our output neuron is using sigmoid as its activation function $\\star$\n",
    "* For binary classification; sigmoid function will be used to predict logistic regression/binary classification problems.\n",
    "\n",
    "```\n",
    "# Compile model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.train(coordinates, labels, epochs=20)\n",
    "\n",
    "# Predict with trained model\n",
    "preds = model.predict(coordinates)\n",
    "```\n",
    "\n",
    "* Note that we obtain the predicted labels by calling `predict` on `coordinates`\n",
    "\n",
    "<img src='data/circ_class.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc8bee",
   "metadata": {},
   "source": [
    "```\n",
    "# Import the sequential model and dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()\n",
    "\n",
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs = 20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c69ed",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "* If we have more than two classes to classify, then we have a multi-class classification problem.\n",
    "* Outcomes of multi-class classification problems must be **mutually exclusive**\n",
    "* The activation function of choice for the *final* layer of a mulit-class classification neural network is `softmax`.\n",
    "* Output values will be provided as probabilities, and the class with the highest probability is chosen as the model's prediction.\n",
    "\n",
    "```\n",
    "# Instantiate a sequential model \n",
    "# ...\n",
    "# Add an input and hidden layer\n",
    "# ...\n",
    "# Add more hidden layers\n",
    "# ...\n",
    "# Add your output layer\n",
    "model.add(Dense(4, activation='softmax')\n",
    "```\n",
    "\n",
    "#### Categorical cross-entropy\n",
    "* When compiling your model, instead of binary cross-entropy as is used for binary classification problems, we use categorical cross-entropy (aka **log loss**).\n",
    "* **Categorical cross-entropy** measures the difference between the predicted probabilities and the true label of the class we should have predicted\n",
    "\n",
    "<img src='data/log_loss1.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* So, if we should have predicted `1` for a given class, by taking a look at the graph above, we see we would get high loss values for predicting close to `0` (since we'd be \"very\" wrong) and low loss values for predicting closer to 1 (the true label).\n",
    "\n",
    "* Since our outputs are vectors containing the probabilities of each class, our neural network must also be trained with vectors representing this concept. To achieve that, we make use of the `keras.utils.to_categorical` function\n",
    "* We first turn our response variable into a categorical variable with pandas `Categorical`; This allows us to redefine the column using the categorical codes (**cat codes**) of the different categories \n",
    "* Once our categories are each represented by a unique integer, we can use the `to_categorical` function to turn them into one-hot-encoded vectors, where each component is 0 except for the one corresponding to the labeled categories\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Turn response variable into labeled codes\n",
    "df.response = pd.Categorical(df.response)\n",
    "df.response.cat.codes\n",
    "\n",
    "# Turn response variable into one-hot response vector\n",
    "y = to_categorical(df.response)\n",
    "```\n",
    "\n",
    "#### Label encoding vs one-hot encoding\n",
    "\n",
    "<img src='data/label_vs_ohe.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Keras `to_categorical` essentially performs the process described in the picture above (of transformed label-encoded variables into one-hot-encoded variables).\n",
    "\n",
    "#### Exercises: A multi-class model\n",
    "You're going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart's x and y coordinates on the board.)\n",
    "\n",
    "This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes/labels are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the `softmax` activation function to achieve a total sum of probabilities of 1 over all competitors.\n",
    "\n",
    "Keras `Sequential` model and `Dense` layer are already loaded for you to use.\n",
    "\n",
    "```\n",
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```          \n",
    "\n",
    "#### Exercises: Prepare your dataset\n",
    "In the console you can check that your labels, `darts.competitor` are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the `to_categorical()` function from `keras.utils` to turn these numbers into their one-hot encoded representation.\n",
    "\n",
    "This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated.\n",
    "\n",
    "The dart's dataset is loaded as `darts`. Pandas is imported as `pd`. Let's prepare this dataset!\n",
    "\n",
    "```\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes\n",
    "\n",
    "# Print the label encoded competitors\n",
    "print('Label encoded competitors: \\n',darts.competitor.head())\n",
    "\n",
    "# Drop the original competitor column with competitors' names\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "\n",
    "# Use to_categorical on your labels\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)\n",
    "```\n",
    "\n",
    "#### Exercises: Training on dart throwers\n",
    "Your model is now ready, just as your dataset. It's time to train!\n",
    "\n",
    "The `coordinates` features and `competitors` labels you just transformed have been partitioned into `coord_train`, `coord_test` and `competitors_train`, `competitors_test`.\n",
    "\n",
    "Your `model` is also loaded. Feel free to visualize your training data or `model.summary()` in the console.\n",
    "\n",
    "Let's find out who threw which dart just by looking at the board!\n",
    "\n",
    "```\n",
    "# Fit your model to the training data for 200 epochs\n",
    "model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f8cd93",
   "metadata": {},
   "source": [
    "#### Exercises: Softmax predictions\n",
    "Your recently trained `model` is loaded for you. This model is generalizing well!, that's why you got a high accuracy on the test set.\n",
    "\n",
    "Since you used the `softmax` activation function, for every input of 2 coordinates provided to your model there's an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors.\n",
    "\n",
    "When computing accuracy with the model's `.evaluate()` method, your model takes the class with the highest probability as the prediction. `np.argmax()` can help you do this since it returns the index with the highest value in an array.\n",
    "\n",
    "Use the collection of test throws stored in `coords_small_test` and `np.argmax()`to check this out!\n",
    "\n",
    "```\n",
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9127085",
   "metadata": {},
   "source": [
    "### Multi-label classification\n",
    "* Now that we know how multi-class classification works, we can take a look at **multi-label classification.**\n",
    "* Both deal with predicting classes, but in multi-label classification, a single input can be assigned to more than one class\n",
    "* Real world example: Movie genre classification- could be multi-label, for example: Drama, Suspense, Action\n",
    "\n",
    "<img src='data/multiclass_vs_multilabel.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "* Imagine we have three clases: `sun`, `moon`, and `clouds`\n",
    "* In **multi-class problems**, if we took a sample of our observations, each individual in the sample will belong to a unique class\n",
    "* However, in **multi-label problems**, each individual in the sample can have **all, none, or some subset of the available classes.**\n",
    "* As you can see in the image, multilabel vectors are also **one-hot encoded** (there's a 1 or a 0 representing the presence or absence of each class).\n",
    "\n",
    "### Multi-label architecture\n",
    "* Making a multi-label model is not very different from building a multi-class model\n",
    "* For the sake of this example, we will assume that to differentiate between these three different classes, we need just one input and 2 hidden neurons\n",
    "* The **biggest changes** (between this model and the multiclass model) happen in the **output layer** and in its **activation function**.\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input and hidden layers\n",
    "model.add(Dense(2, input_shape=(1,)))\n",
    "\n",
    "# Add an output layer for the 3 classes and sigmoid activation\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "* **In the output layer, we use as many neurons as possible classes** and we also use **sigmoid activation**\n",
    "* **We use sigmoid outputs because we no longer care about the sum of probabilities.** We are not deciding **between** or **among** possible outcomes, but rather **selecting any and all possible outcomes with a probabilty greater than 0.5.**\n",
    "\n",
    "<img src='data/multilabel_outcome_probabilities.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc0df7",
   "metadata": {},
   "source": [
    "* We want each output neuron to be able to individually take a value between 0 and 1\n",
    "* This can be achieved with the sigmoid activation because it constrains our neuron output in range 0-1. (This is what we did in binary classification, though we only had one output neuron there).\n",
    "* **Binary cross-entropy** is now used as the loss function when compiling the model \n",
    "* You can look at is **as if we were performing several binary classification problems; for each output we are deciding whether or not its corresponding label is present.**\n",
    "\n",
    "```\n",
    "# Compile the model with binary crossentropy\n",
    "model.compile(optimizer='adam', loss = binary_crossentropy')\n",
    "\n",
    "# Train your model, recall validation_split\n",
    "model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
    "```\n",
    "\n",
    "* By using `validation_split`, a percentage of training data is left out for testing at each epoch.\n",
    "* Using neural networks for multi-label classification can be performed by minor tweaks in our model architecture\n",
    "\n",
    "<img src='data/one_vs_rest.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* If we were to use a classical machine learning approach to solve multi-label problems, we would need more complex methods.\n",
    "    * One way to do so would be to train several classifiers to distinguish each particular class from the rest\n",
    "    * This is called **one-vs-rest classification** and is illustrated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ac42b",
   "metadata": {},
   "source": [
    "### Keras Callbacks\n",
    "* Now that we've trained quite a few models, it's time to learn more about how to better control and supervise model training by using **callbacks**.\n",
    "* A **callback** is a function that is executed after some other function, event, or task has finished.\n",
    "* A Keras callback is a block of code that gets executed after each epoch during training or after the training is finished\n",
    "    * `EarlyStopping`\n",
    "    * `ModelCheckpoint`\n",
    "    * `History`\n",
    "* They are useful to store metrics as the model trains and to make decisions as the training goes by\n",
    "* Every time you call the fit method on a keras model, there's a callback object that gets returned after the model finishes training\n",
    "    * This is the **`history`** attribute, which is a python dictionary.\n",
    "    * Within the `history` attribute, we can check the **saved metrics of the model at each epoch during training as an array of numbers.**\n",
    "    \n",
    "```\n",
    "# Training a model and saving its history\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 100,\n",
    "                    metrics = ['accuracy'])\n",
    "print(history.history['loss'])                    \n",
    "```\n",
    "* To get the most out of the history object, we should use the the `validation_data` parameter in our fit method, passing `X_test` and `y_test` as a tuple, as shown below:\n",
    "\n",
    "```\n",
    "# Training a model and saving its history\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs = 100,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    metrics=['accuracy'])\n",
    "print(history.history['val_loss']              \n",
    "```\n",
    "* The `validation_split` parameter can be used instead too, specifying a percentage of the training data that will be left out for testing purposes\n",
    "* That way, we not only have the training metrics, but also the validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db27e3f",
   "metadata": {},
   "source": [
    "## History plots\n",
    "* You can compare training and validation metrics with a few matplotlib commands\n",
    "* We just need to define a figure\n",
    "* Plot the values of the history attribute for the training accuracy (`acc`) and the validation accuracy (`val_acc`)\n",
    "\n",
    "```\n",
    "# Plot train vs test accuracy per epoch\n",
    "plt.figure()\n",
    "\n",
    "# Use history metrics\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "# Make it pretty\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src='data/history_plots.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* **We can see our model accuracy increases for both training and test sets until it reaches epoch 25**\n",
    "* Then accuracy flattens for the test set whilst the training keeps improving\n",
    "* At that point, overfitting is occurring, since we see the training set keeps improving, as the test set plateaus and then even decreases in accuracy).\n",
    "\n",
    "### Early stopping\n",
    "* Early stopping a model can solve the overfitting problem, since it stops training when it no longer improves\n",
    "* This is extremely useful since deep neural models can take a long time to train and we don't know beforehand how many epochs will be needed\n",
    "* The early stopping callback can monitor several metrics, like validation accuracy, validation loss, etc. specified with the **`monitor`** parameter.\n",
    "* It is also important to define a **`patience`** argument, or, the number of epochs to wait for the model to improve before stopping its training\n",
    "    * There aren't any rules to decide which patience number works best at any time, and this depends mostly on the implementation\n",
    "\n",
    "```\n",
    "# Import early stopping from keras callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Instantiate an early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train your model with the callback\n",
    "model.fit(X_train, y_train, epochs=100, \n",
    "                            validation_data = (X_test, y_test),\n",
    "                            callbacks = [early_stopping])\n",
    "```\n",
    "* The callback is passed as a list to the `callbacks` parameter in the model `fit` method\n",
    "\n",
    "### Model checkpoint\n",
    "* The model checkpoint callback allows us to save our model as it trains \n",
    "* We specify the model filename with a name and the `.hdf5` extension\n",
    "* You can also decide what to monitor to determine which model is best with the **`monitor`** parameter; **by default validation loss is monitored**\n",
    "* Setting the `save_best_only` parameter to `True` guarantees that the latest best model according to the quantity monitored wil not be overwritten\n",
    "\n",
    "```\n",
    "# Import model checkpoint from keras callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Instantiate a model checkpoint callback\n",
    "model_save = ModelCheckpoint('best_model.hdf5', save_best_only=True)\n",
    "\n",
    "# Train your model with the callback\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),\n",
    "                                        callbacks = [model_save])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605bb5f",
   "metadata": {},
   "source": [
    "# $\\star$ Improving Your Model\n",
    "In the previous chapters, you've trained a lot of models! You will now learn how to interpret learning curves to understand your models as they train. You will also visualize the effects of activation functions, batch-sizes, and batch-normalization. Finally, you will learn how to perform automatic hyperparameter optimization to your Keras models using sklearn.\n",
    "\n",
    "### Learning curves\n",
    "* Learning curves provide a lot of insight into your model.\n",
    "* Now that we know how to use the `history` callback to plot them, we will learn how to get the most value out of them\n",
    "* So far, we've see two types of learning curves\":\n",
    "    * **Loss curves**\n",
    "    * **Accuracy curves**\n",
    "    \n",
    "<img src='data/loss_vs_acc_curves.png' width=\"800\" height=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef35bc",
   "metadata": {},
   "source": [
    "* Loss tends to decrease as epochs go by\n",
    "    * This is expected since our model is essentially learning to minimize the loss function\n",
    "    * After a certain number of epochs, **the value converges**, meaning it no longer gets much lower; we have arrived at a minimum.\n",
    "* Accuracy tends to increase as epochs go by\n",
    "    * This shows our model makes fewer mistkes as it learns\n",
    "   \n",
    "* **If we plot training versus validation data, we can identify overfitting**: we will see the training and validation curves start to diverge.\n",
    "\n",
    "<img src='data/identifying_overfitting.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* Again, the `EarlyStopping` callback is useful to stop our model before it starts overfitting\n",
    "* **But, not all curves are smooth and pretty; many times we will find unstable curves.**\n",
    "\n",
    "<img src='data/unstable_curves.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63052d7",
   "metadata": {},
   "source": [
    "* **There are many reasons that can lead to unstable learning curves; the chosen optimizer, learning rate, batch-size, network architecture, weight initialization, etc.**\n",
    "* All of these parameters can be tuned to improve our model learning curves, as we aim for better accuracy and generalization power\n",
    "* NNs are well-known for surpassing traditional ML techniques as we increase the size of our datasets\n",
    "* We can check whether collecting more data would increase a model's generalization and accuracy\n",
    "* We aim to produce a graph like the one below, where we have fit the model with increasing amounts of data and plotted the values for the training and test accuracies of each run \n",
    "\n",
    "<img src='data/acc_vs_train.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* If, after using all of our data, we see that our test set still has a tendency to improve (that is, it's not parallel to our train set curve, and it's increasing), then it's worth gathering more data if possible to allow the model to keel learning\n",
    "\n",
    "<img src='data/acc_vs_train2.png' width=\"700\" height=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e7192",
   "metadata": {},
   "source": [
    "* How would we go about coding a graph like the two above?\n",
    "* Imagine we want to evaluate an already-built-and-compiled model and have partitioned our data into `X_train`, `y_train`, `X_test`, and `y_test`.\n",
    "\n",
    "```\n",
    "# Store initial model weights\n",
    "init_weights = model.get_weights()\n",
    "\n",
    "# Lists for storing accuracies \n",
    "train_accs = []\n",
    "tests_accs = []\n",
    "\n",
    "# Loop over predefine list of train sizes, and for each we get the corresponding training data fraction\n",
    "for train_size in train_sizes:\n",
    "    \n",
    "    # Split a fraction according to train_size\n",
    "    X_train_frac, _, y_train_frac, _ = train_test_split(X_train, y_train, train_size=train_size)\n",
    "    \n",
    "    # Set model initial weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    # Fit model on the training set fraction\n",
    "    model.fit(X_train_frac, y_train_frac, epochs=100, verbose=0, \n",
    "                                          callbacks=[EarlyStopping(monitor='loss', patience=1)])\n",
    "          \n",
    "    # Get the accuracy for this training set fraction\n",
    "    train_acc = model.evaluate(X_train_frac, y_train_frac, verbose=0)[1]\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Get the accuracy on the whole test set\n",
    "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    test_accs.append(test_acc)\n",
    "    print(\"Done with size: \", train_size)\n",
    "```\n",
    "* Loop over predefined list of train sizes, and for each we get the corresponding training data fraction\n",
    "* Then, fit the model on the training fraction\n",
    "    * Use an EarlyStopping callback which monitors **loss**\n",
    "    * It is important to note that it is not validation loss, since we haven't provided the fit method with validation data.\n",
    "* After the training is done, we can get the accuracy for the training set fraction and the accuracy from the test set and append it to our lists of accuracies\n",
    "    * Observe that the same amount of test data was used for each iteration\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20d352",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "* Inside the neurons of any neural network, the same process takes place; a summation of the inputs reaching the neuron multiplied by the weights of each connection and the addition of the bias weight.\n",
    "\n",
    "<img src='data/act_func.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* This operation results in a number, $a$, which can be anything (it isn't bounded).\n",
    "* We pass this number into an activation function that esentially takes it as an input and decides how the neuron fires and which output it produces\n",
    "\n",
    "<img src='data/act_func2.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* **Activation function impact learning time, making our model converge faster or slower and achieving lower or higher accuracy.**\n",
    "* They also allow us to learn more complex functions\n",
    "* Four very well known activation functions are:\n",
    "\n",
    "### Sigmoid\n",
    "* Varies between 0 and 1 for all possible X input values\n",
    "\n",
    "<img src='data/sig.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "### Tanh\n",
    "* Aka Hyperbolic Tangent\n",
    "* Similar to the sigmoid shape, but varies between -1 and 1\n",
    "\n",
    "<img src='data/tanh.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "### ReLU\n",
    "* Rectified Linear Unit\n",
    "* Varies between 0 and infinity\n",
    "\n",
    "<img src='data/relu.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "### Leaky ReLU\n",
    "* We can look at as a smoothed version of ReLU that doesn't sit at 0, allow negative values for negative inputs.\n",
    "\n",
    "<img src='data/leaky_relu.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ec899",
   "metadata": {},
   "source": [
    "### Effect of activation function\n",
    "* Changing the activation function used in the hidden layer of the model we built for binary classification results in different classification boundaries\n",
    "\n",
    "#### Sigmoid vs. Tanh\n",
    "<img src='data/sig_vs_tanh.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### ReLU vs Leaky ReLU\n",
    "<img src='data/relu_vs_leaky.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165073d6",
   "metadata": {},
   "source": [
    "* It's important to note that **these boundaries will be different for every run of the same model because of the random initialization of weights and other random variables that aren't fixed.**\n",
    "* Each activation function comes with its pros aand cons; there's no magic formula\n",
    "    * Based on their properties, the problem at hand, and the layer we are looking at in our network, one activation function will perform better in terms of achieve our goal. \n",
    "    * What is the goal to achieve in a given layer?\n",
    "    * ReLUs are usually a good place to start, as they train quickly and will tend to generalize well to most problems.\n",
    "    * Sigmoids are not recommended for deep models\n",
    "    * Tune with experimentation\n",
    "    \n",
    "#### Comparing activation functions\n",
    "* It's easy to compare how model with different activation functions perform if they are small enough and train fast\n",
    "* It is important to set a random see with numpy, that way the model weights are initialized the same for each activation function\n",
    "* We then define a function that returns a fresh, new model each time, using the `act_function` parameter\n",
    "\n",
    "```\n",
    "# Set a random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Return a new model with the given activation\n",
    "def get_model(act_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_shape=(2,), activation=act_function))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "```\n",
    "* We can then use this function as we loop over several activation functions, training different models and saving their `history` callbacks.\n",
    "\n",
    "```\n",
    "# Activaation functions to try out\n",
    "activations = 'relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Dictionary to store results\n",
    "activation_results = {}\n",
    "\n",
    "for funct in activations:\n",
    "    model = get_model(act_function=funct)\n",
    "    history = model.fit(X_train, y_train, \n",
    "                                validation_data = (X_test, y_test),\n",
    "                                epochs = 100, verbose = 0)\n",
    "    activation_results[funct] = history\n",
    "```\n",
    "* We store all these callbacks in a dictionary.\n",
    "* With this dictionary of histories, we can extract the metrics we want to plot, build a pandas dataframe, and plot it\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "# Extract val_loss history of each activation function\n",
    "val_loss_per_funct = {k:v.history['val_loss'] for k,v in activation_results.items()}\n",
    "\n",
    "# Turn the dictionary into a pandas dataframe\n",
    "val_loss_curves = pd.DataFrame(val_loss_per_funct)\n",
    "\n",
    "# Plot the curves\n",
    "val_loss_curves.plot(title='Loss per activation function')\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs =20, verbose=0)\n",
    "  activation_results[act] = h_callback\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d8427",
   "metadata": {},
   "source": [
    "## Batch size and batch normalization\n",
    "* A mini-batch is a subset of data samples\n",
    "* If we were training a neural network with images, each image in our training set would be a **sample** and we could take **mini-batches** of different sizes (different numbers of samples) from the overall training set **batch**.\n",
    "* Remember that during an epoch, we feed our network, calculate the errors, and update the network weight\n",
    "\n",
    "<img src='data/mini_batches.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa336fe",
   "metadata": {},
   "source": [
    "* It is not very practical to update our network weights only once per epoch after looking at the error produced by all training samples\n",
    "* In practice, we take a mini-batch of training samples\n",
    "* **Mini-batches will be of same size**\n",
    "\n",
    "#### Mini-batches\n",
    "* **Networks tend to train faster with mini-batches since weights are updated often.**\n",
    "* Sometimes datasets are so huge that they would struggle to fit in RAM memory if we didn't use mini-batches\n",
    "* Also, noise can help networks reach a lower error, escaping local minima\n",
    "\n",
    "<img src='data/minibatch_pro_con.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Here you can see how different batch sizes converge towards a minimum as training goes on \n",
    "* Training with all samples is shown in blue.\n",
    "* Stochastic Gradient Descent, in red, uses a `batch_size` of 1\n",
    "* We can see how the path towards the best value for our weights is noisier the smaller the `batch_size`\n",
    "\n",
    "<img src='data/minibatch2.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87865ec6",
   "metadata": {},
   "source": [
    "### Batch size in Keras\n",
    "* You can set your own batch size with the `batch_size` parameters on the model's fit method\n",
    "* Keras uses a default batch size of 32\n",
    "    * Increasing powers of 2 tend to be used\n",
    "    * As a rule of thumb, **you tend to make your batch size bigger, the bigger your dataset is.**\n",
    "    \n",
    "## Normalization \n",
    "* Normalization is a common pre-processing step in ML algorithms, especially when features have different scales.\n",
    "* One way to normalize data is with the equation below:\n",
    "\n",
    "<img src='data/norm_eq.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* We always tend to normalize our model inputs\n",
    "* This leaves everything centered around 0, with a standard deviation of 1\n",
    "\n",
    "<img src='data/norm2.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Normalizing neural network inputs improves our model \n",
    "* But deeper layers are trained based on previous layer outputs and since weights get updated via gradient descent, consecutive layers no longer benefit from normalization and they need to adapt to previous layers' weight changes, finding more trouble to learn their own weights.\n",
    "\n",
    "<img src='data/norm3.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Batch normalization makes sure that, independently of the changes, the inputs to the next layers are normalized\n",
    "* It does this in a smart way, with trainable parameters that also learn how much of this normalization is kept, scaling or shifting it.\n",
    "\n",
    "#### Batch normaliztion advantages\n",
    "* Improves gradient flow\n",
    "* Allows higher lerning rates\n",
    "* Reducces dependence on weight initializations\n",
    "* Acts as an unintended form of regularization\n",
    "* Limits internal covariate shift\n",
    "* Batch normalization is widely used today in many deep learning models\n",
    "\n",
    "#### Batch normalization in Keras\n",
    "* Batch normalization in Keras is applied as a layer\n",
    "\n",
    "```\n",
    "from keras.layers import BatchNormalization\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer\n",
    "model.add(Dense(3, input_shape=(2,), activation = 'relu'))\n",
    "\n",
    "# Add batch normalization for the outputs of the layer above\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add an output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "#### Exercises: Changing batch sizes\n",
    "You've seen models are usually trained in batches of a fixed size. The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Especially if the batch size is too small and it's not representative of the entire training set.\n",
    "\n",
    "Let's see how different batch sizes affect the accuracy of a simple binary classification model that separates red from blue dots.\n",
    "\n",
    "You'll use a batch size of one, updating the weights once per sample in your training set for each epoch. Then you will use the entire dataset, updating the weights only once per epoch.\n",
    "\n",
    "```\n",
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))\n",
    "print(\"\\n The accuracy when using the whole training set as batch-size was: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885da5e0",
   "metadata": {},
   "source": [
    "```\n",
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4471fff",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "* Our aim is to identify those parameters that make our model generalize better\n",
    "\n",
    "#### Neural network hyperparameters\n",
    "* A NN is full of parameters that can be tweaked:\n",
    "    * Number of layers\n",
    "    * Number of neurons per layer\n",
    "    * Layer order\n",
    "    * Layer activations\n",
    "    * Batch sizes\n",
    "    * Learning rates\n",
    "* In sklean we can perform hyperparameter search by using methods like RandomizedSearchCv\n",
    "\n",
    "```\n",
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Instantiate your classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Define a series of parameters to look over\n",
    "params = {'max_depth':[3, None], \"max_features\": range(1, 4), \"min_samples_leaf\": range(1,4)}\n",
    "\n",
    "# Perform random search with cross validation\n",
    "tree_cv = RandomizedSearchCV(tree, params, cv=5)\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "#Print the best parameters\n",
    "print(tree_cv.best_params_)\n",
    "```\n",
    "   \n",
    "## Turn a Keras model into a sklearn estimator\n",
    "* We can do the same with our Keras models\n",
    "* But we first have to transform them into sklearn estimators\n",
    "* We do this by first defining a function that creates our model\n",
    "* Then we import the `KerasClassifier` wrapper from `keras.wrappers.scikit_learn`\n",
    "\n",
    "```\n",
    "# Function that creates our Keras model\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    return model\n",
    "    \n",
    "# Import sklearn wrapper from keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a model as a sklearn estimator\n",
    "model = KerasClassifier(build_fn=create_model, epochs=6, batch_size=16)\n",
    "```\n",
    "* **Note** that parameters likes `epochs` and `batch_size` are **optional** but should be passed if we want to specify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1312941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5b17d",
   "metadata": {},
   "source": [
    "```\n",
    "# Import sklearn wrapper from keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a model as a sklearn estimator\n",
    "model = KerasClassifier(build_fn=create_model, epochs=6, batch_size=16)\n",
    "```\n",
    "* This is very cool!\n",
    "* Our model is now just like any other sklearn estimator, so we can, for instance, perform cross-validation on it to see the stability of its predictions across folds\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "```\n",
    "# Import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Check how your keras model performs with a 5 fold crossvalidation\n",
    "kfold = cross_val_score(model, X, y, cv = 5)\n",
    "\n",
    "# Print the mean accuracy per fold\n",
    "kfold.mean()\n",
    "\n",
    "# Print the standard deviation per fold\n",
    "kfold.std()\n",
    "```\n",
    "\n",
    "### Tips for neural networks hyperparameter tuning\n",
    "* RandomSearch is preferred over grid search\n",
    "* Don't use many epochs\n",
    "* Use a smaller sample of your dataset (makes thing faster if you have a huge dataset, and it makes it easier to play with things like optimizers, batch sizes, activations, and learning rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f6b4a",
   "metadata": {},
   "source": [
    "## Random Search on Keras model\n",
    "* To perform randomized search on a Keras model, we just need to define the parameters to try.\n",
    "* We can try different optimizers, activation functions for the hidden layers and batch sizes.\n",
    "* **Note:** The keys in the parameter dictionary must be named exactly as the parameters in our `create_model` function\n",
    "* We then instantiate a RandomizedSearchCV object passing our model and parameters with 3 fold cross-validation\n",
    "\n",
    "```\n",
    "# Define a series of parameters\n",
    "params = dict(optimizer = ['sgd', 'adam'], \n",
    "              epochs =3, \n",
    "              batch_size = [5, 10, 20], \n",
    "              activation = ['relu', 'tanh'])\n",
    "              \n",
    "# Create a random search cv object and fit it to the data\n",
    "random_search = RandomizedSearchCV(model, params_dist=params, cv=3)\n",
    "random_search_results = random_search.fit(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Best %f using %s\".format(random_search_results.best_score_, random_search_results.best_params_))\n",
    "```\n",
    "\n",
    "## Tuning other hyperparameter\n",
    "* Parameters like the number of neurons per layer and the number of layers can also be tuned using the same method. \n",
    "* We just need to make some changes in our `create_model` function:\n",
    "* `nl` = number of layers\n",
    "* `nn` = number of neurons\n",
    "\n",
    "```\n",
    "def create_model(nl=1, nn=256):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation='relu'))\n",
    "    \n",
    "    # Add as many hidden layers as specified in nl\n",
    "    for i in range(nl):\n",
    "        # Layers have nn neurons\n",
    "        model.add(Dense(nn, activation='relu'))\n",
    "        \n",
    "    # End defining and compiling your model...\n",
    "```\n",
    "* Then we just need to use the same exact names in the parameter dictionary as we have in our function and repeat the process.\n",
    "\n",
    "```\n",
    "# Define parameters, named just like in create_model()\n",
    "params =dict(nl=[1, 2, 9], nn=[128, 256, 1000])\n",
    "\n",
    "# Repeat the random search...\n",
    "\n",
    "# Print results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f870168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nl=1, nn=256):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation='relu'))\n",
    "    \n",
    "    # Add as many hidden layers as specified in nl\n",
    "    for i in range(nl):\n",
    "        # Layers have nn neurons\n",
    "        model.add(Dense(nn, activation='relu'))\n",
    "        \n",
    "        # End defining and compiling your model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b448e",
   "metadata": {},
   "source": [
    "```\n",
    "# Define parameters, named just like in create_model()\n",
    "params =dict(nl=[1, 2, 9], nn=[128, 256, 1000])\n",
    "\n",
    "# Repeat the random search...\n",
    "\n",
    "# Print results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43f3a2",
   "metadata": {},
   "source": [
    "#### Exercises: Preparing a model for tuning\n",
    "Let's tune the hyperparameters of a **binary classification model** that does well classifying the **breast cancer dataset**.\n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
    "\n",
    "Build a simple `create_model()` function that receives both a learning rate and an activation function as arguments. The `Adam` optimizer has been imported as an object from `keras.optimizers` so that you can also change its learning rate parameter.\n",
    "\n",
    "```\n",
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr = learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "  \tmodel.add(Dense(256, activation = activation))\n",
    "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "  \treturn model\n",
    "```\n",
    "\n",
    "#### Exercises: Tuning the model parameters\n",
    "It's time to try out different parameters on your model and see how well it performs!\n",
    "\n",
    "The `create_model()` function you built in the previous exercise is ready for you to use.\n",
    "\n",
    "Since fitting the `RandomizedSearchCV` object would take too long, the results you'd get are printed in the `show_results()` function. You could try `random_search.fit(X,y)` in the console yourself to check it does work after you have built everything else, but you will probably timeout the exercise (so copy your code first if you try this or you can lose your progress!).\n",
    "\n",
    "You don't need to use the optional `epochs` and `batch_size` parameters when building your `KerasClassifier` object since you are passing them as `params` to the random search and this works already.\n",
    "\n",
    "```\n",
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()\n",
    "```\n",
    "\n",
    "#### Exercises: Training with cross-validation\n",
    "Time to train your model with the best parameters found: **0.001** for the **learning rate, 50 epochs, a 128 batch_size** and **relu activations**.\n",
    "\n",
    "The `create_model()` function from the previous exercise is ready for you to use. `X` and `y` are loaded as features and labels.\n",
    "\n",
    "Use the best values found for your model when creating your `KerasClassifier` object so that they are used when performing cross_validation.\n",
    "\n",
    "End this chapter by training an awesome tuned model on the **breast cancer dataset**!\n",
    "\n",
    "```\n",
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = 'relu'), epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8376ba",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 4: Advanced Model Architectures\n",
    "It's time to get introduced to more advanced architectures! You will create an autoencoder to reconstruct noisy images, visualize convolutional neural network activations, use deep pre-trained models to classify images and learn more about recurrent neural networks and working with text as you build a network that predicts the next word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809a33a",
   "metadata": {},
   "source": [
    "## Tensors, layers, and autoencoders\n",
    "* Now that you know how to tune your models, it's time to better understand how they work and learn about new neural network architectures\n",
    "\n",
    "#### Accessing Keras layers\n",
    "* Model layers are easily accessible, we just need to call `layers` on a built model and access the index of the layer we want\n",
    "* From a chosen layer we can print its inputs, outputs and weights\n",
    "\n",
    "```\n",
    "# Accessing the first layer of a Keras model\n",
    "first_layer = model.layers[0]\n",
    "\n",
    "# Printing th elayer, and its input, output and weights\n",
    "print(first_layer.input)\n",
    "print(first_layer.output)\n",
    "print(first_layer.weights)\n",
    "```\n",
    "* Inputs and outputs are tensors of a given shape built with TensorFlow tensor objects, weights are just tensors that change their value as the neural network learns the best weights\n",
    "\n",
    "#### What are tensors?\n",
    "* Tensors are the main data structures used in deep learning; inputs, outputs and transformations in neural networks are all represented using tensors \n",
    "* A **tensor** is a multi-dimensional array of numbers\n",
    "* A **2-dimensional tensor** is a **matrix**\n",
    "* A **3-dimensional tensor** is an **array of matrices**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea88732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a rank 2 tensor (2 dimensions)\n",
    "T2 = [[1, 2, 3],\n",
    "      [4, 5, 6],\n",
    "      [7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c43c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a rank 3 tensor (3 dimensions)\n",
    "T3 = [[1, 2, 3],\n",
    "      [4, 5, 6],\n",
    "      [7, 8, 9],\n",
    "      \n",
    "      [10, 11, 12],\n",
    "      [13, 14, 15],\n",
    "      [16, 17, 18],\n",
    "      \n",
    "      [19, 20, 21],\n",
    "      [22, 23, 24],\n",
    "      [25, 26, 27]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37df60e",
   "metadata": {},
   "source": [
    "* If we import Keras backend, we can build a function that takes in an input tensor from a given layer and returns an output tensor from another or the same layer\n",
    "* To define the function with our backend K, we need to give it a list of inputs and outputs, even if we just want 1 input and 1 output\n",
    "* Then we can use it on a tensor with the same shape as the input layer given during its definition\n",
    "* If the weights of the layers between our inputs and outputs change, the function output for the same input will change as well. \n",
    "* We can use this to see the output of certain layers as weights change during training\n",
    "\n",
    "```\n",
    "# Import Keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Get the input and output tensors of a model layer\n",
    "inp = model.layers[0].input\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Function that maps layer input to outpus\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "print(inp_to_out([X_test])\n",
    "```\n",
    "\n",
    "## Autoencoders\n",
    "* A new type of architecture: autoencoders\n",
    "* **Autoencoders** are models that aim at producing the same inputs as outputs\n",
    "\n",
    "<img src='data/autoencoders.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* This task alone wouldn't be very useful\n",
    "* But, since along the way we decrease the number of neurons, **we are effectively making our network learn to compress its inputs into a small set of neurons.**\n",
    "* This makes autoencoders very useful\n",
    "\n",
    "<img src='data/autoencoders2.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### Autoencoder use cases\n",
    "* **Dimensionality reduction:**\n",
    "    * Smaller dimensional space representation of our inputs\n",
    "* **De-noising data:**\n",
    "    * If trained with clean data, irrelevant noise will be filtered out during reconstruction \n",
    "* **Anomaly detection:**\n",
    "    * A poor reconstruction will result when the model is fed with unseen inputs\n",
    "    * In other words, if you train an autoencoder to map inputs to outputs with data but you then pass in strange values, the network will fail at giving accurate output values \n",
    "* Many other applications can also benefit from this architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465288d",
   "metadata": {},
   "source": [
    "### Building a simple autoencoder\n",
    "* To make an autoencoder that maps a hundred inputs to a hundred outputs, encoding the inputs into a layer of 4 neurons, we would do the following:\n",
    "\n",
    "```\n",
    "# Instantiate a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a hidden layer of 4 neurons and an input layer of 100\n",
    "autoencoder.add(Dense(4, input_shape=(100,), activation='relu'))\n",
    "\n",
    "# Add an output layer of 100 neurons\n",
    "autoencoder.add(Dense(100, activation='sigmoid'))\n",
    "\n",
    "# Compile your model with the appropriate loss\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "```\n",
    "* Once you've built and trained your autoencoder you might want to encode your inputs\n",
    "* To do this, you need to build a new model and add the first layer of your previously trained autoencoder.\n",
    "* This new model's predictions returns the 4 numbers given by the 4 neurons of the hidden layer for each observation in the input dataset\n",
    "\n",
    "```\n",
    "# Building a separate model to encode inputs\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Predicting returns the four hidden layer neuron outputs \n",
    "encoder.predict(X_test)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcdea9",
   "metadata": {},
   "source": [
    "### Intro to CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e805e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb82895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4763e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc7f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f487b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f964a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca61fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4d17b8a",
   "metadata": {},
   "source": [
    "<img src='data/visualize_parameters.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
