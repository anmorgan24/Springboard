{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17222b62",
   "metadata": {},
   "source": [
    "Note: The notes contained in this notebook were taken from DataCamp's \"Machine Learning with PySpark\"\n",
    "# Machine Learning with PySpark\n",
    "Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!\n",
    "\n",
    "**Instructor:** Andrew Collier, Data Scientist @ Exegetic Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61c488",
   "metadata": {},
   "source": [
    "#### First, some recap:\n",
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
    "\n",
    "However, with greater computing power comes greater complexity.\n",
    "\n",
    "Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n",
    "    * Is my data too big to work with on a single machine?\n",
    "    * Can my calculations be easily parallelized?\n",
    "    \n",
    "The first step in using Spark is connecting to a cluster.\n",
    "\n",
    "In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n",
    "\n",
    "When you're just getting started with Spark it's simpler to just run a cluster locally. Thus, for this course, instead of connecting to another computer, all computations will be run on DataCamp's servers in a simulated cluster.\n",
    "\n",
    "Creating the connection is as simple as creating an instance of the `SparkContext` class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you're connecting to.\n",
    "\n",
    "An object holding all these attributes can be created with the `SparkConf()` constructor. Take a look at the [documentation](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html) for all the details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187d558",
   "metadata": {},
   "source": [
    "#### Using DataFrames\n",
    "Spark's core data structure is the **Resilient Distributed Dataset (RDD)**. This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface with that connection.\n",
    "\n",
    "#### Put some Spark in your data\n",
    "In the last exercise, you saw how to move data from Spark to `pandas`. However, maybe you want to go the other direction, and put a `pandas` DataFrame into a Spark cluster! The `SparkSession` class has a method for this as well.\n",
    "\n",
    "The `.createDataFrame()` method takes a `pandas` DataFrame and returns a Spark DataFrame.\n",
    "\n",
    "The output of this method is stored locally, not in the `SparkSession` catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the `.sql()` method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table.\n",
    "\n",
    "You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific `SparkSession` used to create the Spark DataFrame.\n",
    "\n",
    "There is also the method `.createOrReplaceTempView()`. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables.\n",
    "\n",
    "Check out the diagram to see all the different ways your Spark data structures interact with each other.\n",
    "\n",
    "<img src='data/spark_createTempView.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370ffd9",
   "metadata": {},
   "source": [
    "* Similar to `.withColumn()`, you can do column-wise computations within a `SELECT` statement. \n",
    "* `SELECT origin, dest, air_time / 60 FROM flights;`\n",
    "* the following two expressions will produce the same output:\n",
    "* `flights.filter(\"air_time > 120\").show()`\n",
    "* `flights.filter(flights.air_time > 120).show()`\n",
    "* The difference between `.select()` and `.withColumn()` methods is that `.select()` returns only the columns you specify, while .`withColumn()` returns all the columns of the DataFrame in addition to the one you defined. \n",
    "* At the core of the `pyspark.ml` module are the `Transformer` and `Estimator` classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
    "* `Transformer` classes have a `.transform()` method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class `Bucketizer` to create discrete bins from a continuous feature or the class `PCA` to reduce the dimensionality of your dataset using principal component analysis.\n",
    "* `Estimator` classes all implement a `.fit()` method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a `StringIndexerModel` for including categorical data saved as strings in your models, or a `RandomForestModel` that uses the random forest algorithm for classification or regression.\n",
    "* Before you get started modeling, it's important to know that Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals (called 'doubles' in Spark).\n",
    "* To remedy this, you can use the `.cast()` method in combination with the `.withColumn()` method. It's important to note that `.cast()` works on columns, while `.withColumn()` works on DataFrames.\n",
    "* In Spark it's important to make sure you split the data **after** all the transformations. This is because operations like `StringIndexer` don't always produce the same index even when given the same list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e55649",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Introduction\n",
    "\n",
    "## Machine Learning and Spark\n",
    "Spark is a framework for working with Big Data. In this chapter you'll cover some background about Spark and Machine Learning. You'll then find out how to connect to Spark using Python and load CSV data.\n",
    "\n",
    "Here, we'll learn how to build Machine Learning models on large data sets using distributed computing techniques\n",
    "\n",
    "* The performance of an ML depends on data; in general, more data is a good thing\n",
    "* If the data can fit entirely in RAM then the algorithm can operate efficiently\n",
    "* When the data no longer fit into memory, the computer will start to use **virutal memory** and the data will be **paged** back and forth between RAM and disk\n",
    "    * Relative to **RAM** access, retrieving data from disk is slow\n",
    "    * And as the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets.\n",
    "\n",
    "<img src='data/datasize_RAM.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* One option is to **distribute the problem across multiple computers in a cluster;** rather than trying to handle a large dataset on a single machine, it's divided up into partitions which are processed separately.\n",
    "    * Ideally each data partition can fit into RAM on a single computer in the cluster.\n",
    "    * This is the approach used by **Spark**\n",
    "    \n",
    "#### What is Spark?\n",
    "* Compute across a distributed cluster\n",
    "* Data processing in memory\n",
    "* Well-documented high-level API\n",
    "* **It is generally much faster than other Big Data technologies like Hadoop, because it does most processing in memory .**\n",
    "* **It has a developer-friendly interface which hides much of the complexity of distributed computing.**\n",
    "\n",
    "#### Cluster components\n",
    "* A cluster consists of one or more **nodes**\n",
    "* Each **node** is a computer with CPU, RAM, and physical storage\n",
    "* A **cluster manager** allocates resources and coordinates activity across the cluster\n",
    "* Every application running on the Spark cluster has a **driver** program\n",
    "* Using the **Spark API**, the driver communicates with the cluster manager, which in turn distributes work to the nodes.\n",
    "* On each node, Spark launches an **executor** process which persists for the duration of the application\n",
    "* Work is divided up into **tasks**, which are simply units of computation\n",
    "* The executors run tasks in multiple **threads** across the **cores** in a node\n",
    "\n",
    "<img src='data/cluster_structure.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b986ee1",
   "metadata": {},
   "source": [
    "#### Interacting with Spark\n",
    "* Languages for interacting with Spark:\n",
    "    * Java: low-level, compiled\n",
    "    * Scala, Python and R: high-level and interactive REPL (Read-Eval-Print-Loop, which is crucial for **interactive development**)\n",
    "    \n",
    "#### Importing pyspark\n",
    "* Python doesn't \"speak\" natively with spark\n",
    "* From Python import the `pyspark` module first, which makes the Spark functionality available in the Python interpreter\n",
    "* Spark is under vigorous development and because it is constantly evolving, it is import to check your version before getting started \n",
    "* In this course we'll be using version `2.4.1` (released March 2019)\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "pyspark.__version__\n",
    "```\n",
    "\n",
    "#### Sub-modules\n",
    "* In addition to `pyspark`, there are:\n",
    "    * Structured Data -- `pyspark.sql`\n",
    "    * Streaming Data -- `pyspark.streaming`\n",
    "    * Machine Learning -- `pyspark.mllib` (deprecated) and **`pyspark.ml`**\n",
    "* With the `pyspark` module loaded, you're able to connect to Spark. The next thing you need to do is **tell Spark where the cluster is located.** Two options:\n",
    "\n",
    "#### Remote Cluster\n",
    "   * Connect to a **Remote Cluster** using Spark URL:\n",
    "        * `spark://<IP address | DNS name>:<port>`\n",
    "        * *Example with IP:* `spark://13.59.151.161:7077`\n",
    "        * *Example with DNS:* `spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077`\n",
    "        * The Spark URL gives the location of the cluster's master node\n",
    "        * The URL is composed of an **IP address or DNS name** and a **port number**\n",
    "        * The **default port** for Spark is 7077 (but this must still be explicitly specified\n",
    "        \n",
    "#### Local Cluster\n",
    "* When you're figuring out how Spark works, the infrastructure of a distributed network can get in the way\n",
    "* For this reason it may be helpful to create a **local cluster**, where everything happens on a single computer\n",
    "    * This is the setup that you're going to use throughout this course\n",
    "* For a local cluster, you need only specify \"local\" and, optionally, the number of cores to use.\n",
    "    * *Examples:*\n",
    "        * `local` -- only 1 core;\n",
    "        * `local[4]` -- 4 cores; or\n",
    "        * `local[*]` -- all available cores.\n",
    "* By **default** a local cluster will run on a single core.\n",
    "\n",
    "### Creating a SparkSession\n",
    "* You connect to Spark by creating a `SparkSession` object\n",
    "* You then specify the **location** of the cluster using the **`master()`** method:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]')\n",
    "                    .appName('first_spark_application') \\\n",
    "                    .getOrCreate()\n",
    "```\n",
    "* Or: `spark = SparkSession.builder.master('local[*]').appName('first_spark_application').getOrCreate()`\n",
    "\n",
    "\n",
    "* Optionally, you can also assign a name to the application using the `appName()` method\n",
    "* Finally, we call the `getOrCreate()` method, which will either create a new session object or return an existing object.\n",
    "* *Once the session has been created, you are able to interact with Spark.*\n",
    "* Although it's possible for multiple SparkSessions to co-exist, it's good practice to stop the SparkSession when you're done. \n",
    "* `# Close connection to Spark`\n",
    "* **`>>> spark.stop()`**\n",
    "* For more info on: [SparkSession](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbe23e",
   "metadata": {},
   "source": [
    "#### Creating a SparkSession\n",
    "* For more info on: [SparkSession](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n",
    "* The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "    * Specify the location of the master node;\n",
    "    * Name the application (optional); and\n",
    "    * Retrieve an existing SparkSession or, if there is none, create a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5b658",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "* Selected methods:\n",
    "    * `count()` : returns number of rows \n",
    "    * `show()` : displays a subset of rows\n",
    "    * `printSchema()` : column types\n",
    "* Selected attributes:\n",
    "    * `dtypes` : column types\n",
    "    \n",
    "#### Reading data from CSV\n",
    "* The `.csv()` method reads a CSV file and returns a `DataFrame`\n",
    "* `cars = spark.read.csv('cars.csv', header=True)`\n",
    "* **Optional arguments:**\n",
    "    * **`header`:** is first row a header? (default: `False`)\n",
    "    * **`sep`:** field separator (default: a comma `','`)\n",
    "    * **`schema`:** explicit column data types\n",
    "    * **`inferSchema`:** deduce column data types from data?\n",
    "    * **`nullValue`:** placeholder for missing data; *case-sensitive*\n",
    "    \n",
    "#### Check column types\n",
    "* `cars.printSchema()`\n",
    "* **The `.csv()` method treats all columns as strings by default.** Or, \n",
    "    * **(1)** Infer the columns from the data:\n",
    "        * `cars = spark.read.csv('cars.csv', header=True, inferSchema=True)`\n",
    "        * In this scenario, Spark needs to make an extra pass over the data to figure out the column types before reading the data\n",
    "        * Con: if the data file is big, this will notably increase the load time\n",
    "        * Con: While usually accurate, there may also misidentified types\n",
    "        * Con: interprets NA as a string (and therefore columns with NA as string types)\n",
    "    * **(2)** Manually specify the types\n",
    "    \n",
    "#### Manually specifying column types\n",
    "* Manually specify the type of each column in an explicit schema\n",
    "* During this process, it is also possible to choose alternative column names\n",
    "\n",
    "```\n",
    "schema = StructType([\n",
    "                StructField(\"maker\", StringType()),\n",
    "                StructField(\"model\", StringType()),\n",
    "                StructField(\"origin\", StringType()),\n",
    "                StructField(\"type\", StringType()),\n",
    "                StructField(\"cyl\", StringType()),\n",
    "                StructField(\"size\", StringType()),\n",
    "                StructField(\"weight\", StringType()),\n",
    "                StructField(\"length\", StringType()),\n",
    "                StructField(\"rpm\", StringType()),\n",
    "                StructField(\"consumption\", StringType())\n",
    "])\n",
    "cars = spark.read.csv(\"cars.csv\", header = True, schema = schema, nullValue = 'NA')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca95ac",
   "metadata": {},
   "source": [
    "#### Exercises: Loading flights data\n",
    "\n",
    "```\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)\n",
    "```\n",
    "\n",
    "#### Exercises: Loading SMS spam data\n",
    "\n",
    "```\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e719d",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Classification\n",
    "Now that you are familiar with getting data into Spark, you'll move onto building two types of classification model: Decision Trees and Logistic Regression. You'll also find out about a few approaches to data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656bc59",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "#### Dropping columns\n",
    "* There are two approaches:\n",
    "    * Drop the columns you don't want; or,\n",
    "    * Select the fields you do want\n",
    "\n",
    "```\n",
    "# Drop the columns you don't want\n",
    "cars = cars.drop('maker', 'model')\n",
    " \n",
    " \n",
    "# Select the columns you do want\n",
    "cars = cars.select(\"origin\", \"type\", \"cyl\", \"size\", \"weight\", \"length\", \"rpm\", \"consumption\")\n",
    "```\n",
    "\n",
    "#### Filtering out missing data\n",
    "* Use the `.filter()` method and provide a **logical predicate** using **SQL syntax** that identifies NULL values:\n",
    "\n",
    "```\n",
    "# How many missing values?\n",
    "cars = filter('cyl IS NULL').count()\n",
    "\n",
    "# Drop records with missing values in the `cylinders` column\n",
    "cars = cars.filter('cyl IS NOT NULL')\n",
    "\n",
    "# Drop records with missing values in any column\n",
    "cars = cars.dropna()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6722b",
   "metadata": {},
   "source": [
    "#### Mutating Columns \n",
    "* Use the `.withColumn()` method to create a new mass column in units of kilograms\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create a new \"mass\" column\n",
    "cars = cars.withColumn(\"mass\", round(cars.weight / 2.205, 0))\n",
    "\n",
    "# Convert length to meters\n",
    "cars = cars.withColumn('length', round(cars.length * 0.0254, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611756e4",
   "metadata": {},
   "source": [
    "### Indexing categorical data\n",
    "* Use `StringIndexer` class\n",
    "* Within constructor, provide string input column and a name for the new output column to be created\n",
    "* The indexer is first fit to the data, creating a `StringIndexerModel`\n",
    "* During the fitting process the distinct string values are identified and an index is assigned to each value\n",
    "* The model is then used to transform the data, creating a new column with the index values\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='type',\n",
    "                        outputCol='type_idx')\n",
    "                        \n",
    "# Assign index values to strings\n",
    "indexer = indexer.fit(cars)\n",
    "\n",
    "# Create column with index values\n",
    "cars = indexer.transform(cars)\n",
    "```\n",
    "* **By default, the index values are assigned according to the descending relative frequency of each of the string values.**\n",
    "\n",
    "<img src='data/index_cat_data.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2a3c",
   "metadata": {},
   "source": [
    "* Note, too, that indexing starts at zero (most common)\n",
    "* It is also possible to choose different strategies for assigning index values, by apecifying the **`stringOrderType`** argument.\n",
    "\n",
    "* **Indexing country of origin:**\n",
    "\n",
    "```\n",
    "# Index country of origin:\n",
    "#\n",
    "# USA          -> 0\n",
    "# non-USA      -> 1\n",
    "#\n",
    "cars = StringIndexer(\n",
    "    inputCol = \"origin\",\n",
    "    outputCol = \"label\"\n",
    ").fit(cars).transform(cars)\n",
    "```\n",
    "\n",
    "#### Assembling columns\n",
    "* The final step in preparing a dataset is to consolidate the various input columns into a single column\n",
    "* This is necessary because **the Machine Learning algorithms in Spark operate on a single vector of predictors**(although each element in that vector may consist of multiple values).\n",
    "* First you create an instance of the `VectorAssembler` class, providing it with the names of the columns that you want to consolidate and the name of the new output column:\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['cyl', 'size'], outputCol='features')\n",
    "assembler.transform(cars)\n",
    "```\n",
    "\n",
    "<img src='data/column_vectors.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5eb3d1",
   "metadata": {},
   "source": [
    "**Note** above the new `features` column, which consists of values from the `cylinders` and `size` columns consolidated into a vector.\n",
    "\n",
    "```\n",
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf85398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a20ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666af81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c66af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8608d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ef1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b2cc01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b671bf",
   "metadata": {},
   "source": [
    "<img src='data/course_datasets.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
