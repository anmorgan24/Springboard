{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17222b62",
   "metadata": {},
   "source": [
    "Note: The notes contained in this notebook were taken from DataCamp's \"Machine Learning with PySpark\"\n",
    "# Machine Learning with PySpark\n",
    "Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!\n",
    "\n",
    "**Instructor:** Andrew Collier, Data Scientist @ Exegetic Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61c488",
   "metadata": {},
   "source": [
    "#### First, some recap:\n",
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
    "\n",
    "However, with greater computing power comes greater complexity.\n",
    "\n",
    "Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n",
    "    * Is my data too big to work with on a single machine?\n",
    "    * Can my calculations be easily parallelized?\n",
    "    \n",
    "The first step in using Spark is connecting to a cluster.\n",
    "\n",
    "In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n",
    "\n",
    "When you're just getting started with Spark it's simpler to just run a cluster locally. Thus, for this course, instead of connecting to another computer, all computations will be run on DataCamp's servers in a simulated cluster.\n",
    "\n",
    "Creating the connection is as simple as creating an instance of the `SparkContext` class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you're connecting to.\n",
    "\n",
    "An object holding all these attributes can be created with the `SparkConf()` constructor. Take a look at the [documentation](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html) for all the details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187d558",
   "metadata": {},
   "source": [
    "#### Using DataFrames\n",
    "Spark's core data structure is the **Resilient Distributed Dataset (RDD)**. This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface with that connection.\n",
    "\n",
    "#### Put some Spark in your data\n",
    "In the last exercise, you saw how to move data from Spark to `pandas`. However, maybe you want to go the other direction, and put a `pandas` DataFrame into a Spark cluster! The `SparkSession` class has a method for this as well.\n",
    "\n",
    "The `.createDataFrame()` method takes a `pandas` DataFrame and returns a Spark DataFrame.\n",
    "\n",
    "The output of this method is stored locally, not in the `SparkSession` catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the `.sql()` method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table.\n",
    "\n",
    "You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific `SparkSession` used to create the Spark DataFrame.\n",
    "\n",
    "There is also the method `.createOrReplaceTempView()`. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables.\n",
    "\n",
    "Check out the diagram to see all the different ways your Spark data structures interact with each other.\n",
    "\n",
    "<img src='data/spark_createTempView.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370ffd9",
   "metadata": {},
   "source": [
    "* Similar to `.withColumn()`, you can do column-wise computations within a `SELECT` statement. \n",
    "* `SELECT origin, dest, air_time / 60 FROM flights;`\n",
    "* the following two expressions will produce the same output:\n",
    "* `flights.filter(\"air_time > 120\").show()`\n",
    "* `flights.filter(flights.air_time > 120).show()`\n",
    "* The difference between `.select()` and `.withColumn()` methods is that `.select()` returns only the columns you specify, while .`withColumn()` returns all the columns of the DataFrame in addition to the one you defined. \n",
    "* At the core of the `pyspark.ml` module are the `Transformer` and `Estimator` classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
    "* `Transformer` classes have a `.transform()` method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class `Bucketizer` to create discrete bins from a continuous feature or the class `PCA` to reduce the dimensionality of your dataset using principal component analysis.\n",
    "* `Estimator` classes all implement a `.fit()` method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a `StringIndexerModel` for including categorical data saved as strings in your models, or a `RandomForestModel` that uses the random forest algorithm for classification or regression.\n",
    "* Before you get started modeling, it's important to know that Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals (called 'doubles' in Spark).\n",
    "* To remedy this, you can use the `.cast()` method in combination with the `.withColumn()` method. It's important to note that `.cast()` works on columns, while `.withColumn()` works on DataFrames.\n",
    "* In Spark it's important to make sure you split the data **after** all the transformations. This is because operations like `StringIndexer` don't always produce the same index even when given the same list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e55649",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Introduction\n",
    "\n",
    "## Machine Learning and Spark\n",
    "Spark is a framework for working with Big Data. In this chapter you'll cover some background about Spark and Machine Learning. You'll then find out how to connect to Spark using Python and load CSV data.\n",
    "\n",
    "Here, we'll learn how to build Machine Learning models on large data sets using distributed computing techniques\n",
    "\n",
    "* The performance of an ML depends on data; in general, more data is a good thing\n",
    "* If the data can fit entirely in RAM then the algorithm can operate efficiently\n",
    "* When the data no longer fit into memory, the computer will start to use **virutal memory** and the data will be **paged** back and forth between RAM and disk\n",
    "    * Relative to **RAM** access, retrieving data from disk is slow\n",
    "    * And as the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets.\n",
    "\n",
    "<img src='data/datasize_RAM.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* One option is to **distribute the problem across multiple computers in a cluster;** rather than trying to handle a large dataset on a single machine, it's divided up into partitions which are processed separately.\n",
    "    * Ideally each data partition can fit into RAM on a single computer in the cluster.\n",
    "    * This is the approach used by **Spark**\n",
    "    \n",
    "#### What is Spark?\n",
    "* Compute across a distributed cluster\n",
    "* Data processing in memory\n",
    "* Well-documented high-level API\n",
    "* **It is generally much faster than other Big Data technologies like Hadoop, because it does most processing in memory .**\n",
    "* **It has a developer-friendly interface which hides much of the complexity of distributed computing.**\n",
    "\n",
    "#### Cluster components\n",
    "* A cluster consists of one or more **nodes**\n",
    "* Each **node** is a computer with CPU, RAM, and physical storage\n",
    "* A **cluster manager** allocates resources and coordinates activity across the cluster\n",
    "* Every application running on the Spark cluster has a **driver** program\n",
    "* Using the **Spark API**, the driver communicates with the cluster manager, which in turn distributes work to the nodes.\n",
    "* On each node, Spark launches an **executor** process which persists for the duration of the application\n",
    "* Work is divided up into **tasks**, which are simply units of computation\n",
    "* The executors run tasks in multiple **threads** across the **cores** in a node\n",
    "\n",
    "<img src='data/cluster_structure.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b986ee1",
   "metadata": {},
   "source": [
    "#### Interacting with Spark\n",
    "* Languages for interacting with Spark:\n",
    "    * Java: low-level, compiled\n",
    "    * Scala, Python and R: high-level and interactive REPL (Read-Eval-Print-Loop, which is crucial for **interactive development**)\n",
    "    \n",
    "#### Importing pyspark\n",
    "* Python doesn't \"speak\" natively with spark\n",
    "* From Python import the `pyspark` module first, which makes the Spark functionality available in the Python interpreter\n",
    "* Spark is under vigorous development and because it is constantly evolving, it is import to check your version before getting started \n",
    "* In this course we'll be using version `2.4.1` (released March 2019)\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "pyspark.__version__\n",
    "```\n",
    "\n",
    "#### Sub-modules\n",
    "* In addition to `pyspark`, there are:\n",
    "    * Structured Data -- `pyspark.sql`\n",
    "    * Streaming Data -- `pyspark.streaming`\n",
    "    * Machine Learning -- `pyspark.mllib` (deprecated) and **`pyspark.ml`**\n",
    "* With the `pyspark` module loaded, you're able to connect to Spark. The next thing you need to do is **tell Spark where the cluster is located.** Two options:\n",
    "\n",
    "#### Remote Cluster\n",
    "   * Connect to a **Remote Cluster** using Spark URL:\n",
    "        * `spark://<IP address | DNS name>:<port>`\n",
    "        * *Example with IP:* `spark://13.59.151.161:7077`\n",
    "        * *Example with DNS:* `spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077`\n",
    "        * The Spark URL gives the location of the cluster's master node\n",
    "        * The URL is composed of an **IP address or DNS name** and a **port number**\n",
    "        * The **default port** for Spark is 7077 (but this must still be explicitly specified\n",
    "        \n",
    "#### Local Cluster\n",
    "* When you're figuring out how Spark works, the infrastructure of a distributed network can get in the way\n",
    "* For this reason it may be helpful to create a **local cluster**, where everything happens on a single computer\n",
    "    * This is the setup that you're going to use throughout this course\n",
    "* For a local cluster, you need only specify \"local\" and, optionally, the number of cores to use.\n",
    "    * *Examples:*\n",
    "        * `local` -- only 1 core;\n",
    "        * `local[4]` -- 4 cores; or\n",
    "        * `local[*]` -- all available cores.\n",
    "* By **default** a local cluster will run on a single core.\n",
    "\n",
    "### Creating a SparkSession\n",
    "* You connect to Spark by creating a `SparkSession` object\n",
    "* You then specify the **location** of the cluster using the **`master()`** method:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]')\n",
    "                    .appName('first_spark_application') \\\n",
    "                    .getOrCreate()\n",
    "```\n",
    "* Or: `spark = SparkSession.builder.master('local[*]').appName('first_spark_application').getOrCreate()`\n",
    "\n",
    "\n",
    "* Optionally, you can also assign a name to the application using the `appName()` method\n",
    "* Finally, we call the `getOrCreate()` method, which will either create a new session object or return an existing object.\n",
    "* *Once the session has been created, you are able to interact with Spark.*\n",
    "* Although it's possible for multiple SparkSessions to co-exist, it's good practice to stop the SparkSession when you're done. \n",
    "* `# Close connection to Spark`\n",
    "* **`>>> spark.stop()`**\n",
    "* For more info on: [SparkSession](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbe23e",
   "metadata": {},
   "source": [
    "#### Creating a SparkSession\n",
    "* For more info on: [SparkSession](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n",
    "* The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "    * Specify the location of the master node;\n",
    "    * Name the application (optional); and\n",
    "    * Retrieve an existing SparkSession or, if there is none, create a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5b658",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "* Selected methods:\n",
    "    * `count()` : returns number of rows \n",
    "    * `show()` : displays a subset of rows\n",
    "    * `printSchema()` : column types\n",
    "* Selected attributes:\n",
    "    * `dtypes` : column types\n",
    "    \n",
    "#### Reading data from CSV\n",
    "* The `.csv()` method reads a CSV file and returns a `DataFrame`\n",
    "* `cars = spark.read.csv('cars.csv', header=True)`\n",
    "* **Optional arguments:**\n",
    "    * **`header`:** is first row a header? (default: `False`)\n",
    "    * **`sep`:** field separator (default: a comma `','`)\n",
    "    * **`schema`:** explicit column data types\n",
    "    * **`inferSchema`:** deduce column data types from data?\n",
    "    * **`nullValue`:** placeholder for missing data; *case-sensitive*\n",
    "    \n",
    "#### Check column types\n",
    "* `cars.printSchema()`\n",
    "* **The `.csv()` method treats all columns as strings by default.** Or, \n",
    "    * **(1)** Infer the columns from the data:\n",
    "        * `cars = spark.read.csv('cars.csv', header=True, inferSchema=True)`\n",
    "        * In this scenario, Spark needs to make an extra pass over the data to figure out the column types before reading the data\n",
    "        * Con: if the data file is big, this will notably increase the load time\n",
    "        * Con: While usually accurate, there may also misidentified types\n",
    "        * Con: interprets NA as a string (and therefore columns with NA as string types)\n",
    "    * **(2)** Manually specify the types\n",
    "    \n",
    "#### Manually specifying column types\n",
    "* Manually specify the type of each column in an explicit schema\n",
    "* During this process, it is also possible to choose alternative column names\n",
    "\n",
    "```\n",
    "schema = StructType([\n",
    "                StructField(\"maker\", StringType()),\n",
    "                StructField(\"model\", StringType()),\n",
    "                StructField(\"origin\", StringType()),\n",
    "                StructField(\"type\", StringType()),\n",
    "                StructField(\"cyl\", StringType()),\n",
    "                StructField(\"size\", StringType()),\n",
    "                StructField(\"weight\", StringType()),\n",
    "                StructField(\"length\", StringType()),\n",
    "                StructField(\"rpm\", StringType()),\n",
    "                StructField(\"consumption\", StringType())\n",
    "])\n",
    "cars = spark.read.csv(\"cars.csv\", header = True, schema = schema, nullValue = 'NA')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca95ac",
   "metadata": {},
   "source": [
    "#### Exercises: Loading flights data\n",
    "\n",
    "```\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.dtypes)\n",
    "```\n",
    "\n",
    "#### Exercises: Loading SMS spam data\n",
    "\n",
    "```\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e719d",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Classification\n",
    "Now that you are familiar with getting data into Spark, you'll move onto building two types of classification model: Decision Trees and Logistic Regression. You'll also find out about a few approaches to data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656bc59",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "#### Dropping columns\n",
    "* There are two approaches:\n",
    "    * Drop the columns you don't want; or,\n",
    "    * Select the fields you do want\n",
    "\n",
    "```\n",
    "# Drop the columns you don't want\n",
    "cars = cars.drop('maker', 'model')\n",
    " \n",
    " \n",
    "# Select the columns you do want\n",
    "cars = cars.select(\"origin\", \"type\", \"cyl\", \"size\", \"weight\", \"length\", \"rpm\", \"consumption\")\n",
    "```\n",
    "\n",
    "#### Filtering out missing data\n",
    "* Use the `.filter()` method and provide a **logical predicate** using **SQL syntax** that identifies NULL values:\n",
    "\n",
    "```\n",
    "# How many missing values?\n",
    "cars = filter('cyl IS NULL').count()\n",
    "\n",
    "# Drop records with missing values in the `cylinders` column\n",
    "cars = cars.filter('cyl IS NOT NULL')\n",
    "\n",
    "# Drop records with missing values in any column\n",
    "cars = cars.dropna()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6722b",
   "metadata": {},
   "source": [
    "#### Mutating Columns \n",
    "* Use the `.withColumn()` method to create a new mass column in units of kilograms\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create a new \"mass\" column\n",
    "cars = cars.withColumn(\"mass\", round(cars.weight / 2.205, 0))\n",
    "\n",
    "# Convert length to meters\n",
    "cars = cars.withColumn('length', round(cars.length * 0.0254, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611756e4",
   "metadata": {},
   "source": [
    "### Indexing categorical data\n",
    "* Use `StringIndexer` class\n",
    "* Within constructor, provide string input column and a name for the new output column to be created\n",
    "* The indexer is first fit to the data, creating a `StringIndexerModel`\n",
    "* During the fitting process the distinct string values are identified and an index is assigned to each value\n",
    "* The model is then used to transform the data, creating a new column with the index values\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='type',\n",
    "                        outputCol='type_idx')\n",
    "                        \n",
    "# Assign index values to strings\n",
    "indexer = indexer.fit(cars)\n",
    "\n",
    "# Create column with index values\n",
    "cars = indexer.transform(cars)\n",
    "```\n",
    "* **By default, the index values are assigned according to the descending relative frequency of each of the string values.**\n",
    "\n",
    "<img src='data/index_cat_data.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca2a3c",
   "metadata": {},
   "source": [
    "* Note, too, that indexing starts at zero (most common)\n",
    "* It is also possible to choose different strategies for assigning index values, by apecifying the **`stringOrderType`** argument.\n",
    "\n",
    "* **Indexing country of origin:**\n",
    "\n",
    "```\n",
    "# Index country of origin:\n",
    "#\n",
    "# USA          -> 0\n",
    "# non-USA      -> 1\n",
    "#\n",
    "cars = StringIndexer(\n",
    "    inputCol = \"origin\",\n",
    "    outputCol = \"label\"\n",
    ").fit(cars).transform(cars)\n",
    "```\n",
    "\n",
    "#### Assembling columns\n",
    "* The final step in preparing a dataset is to consolidate the various input columns into a single column\n",
    "* This is necessary because **the Machine Learning algorithms in Spark operate on a single vector of predictors**(although each element in that vector may consist of multiple values).\n",
    "* First you create an instance of the `VectorAssembler` class, providing it with the names of the columns that you want to consolidate and the name of the new output column:\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['cyl', 'size'], outputCol='features')\n",
    "assembler.transform(cars)\n",
    "```\n",
    "\n",
    "<img src='data/column_vectors.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8608b",
   "metadata": {},
   "source": [
    "**Note** above the new `features` column, which consists of values from the `cylinders` and `size` columns consolidated into a vector.\n",
    "\n",
    "```\n",
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62850d4a",
   "metadata": {},
   "source": [
    "#### Exercises: Categorical columns \n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show(5)\n",
    "```\n",
    "\n",
    "#### Exercises: Assembling Columns \n",
    "\n",
    "```\n",
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9da76",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "* Decision trees are perhaps the most intuitive models\n",
    "* Decision trees are constructed using an algorithm called **Recursive Partitioning**.\n",
    "* Ideally, you want the groups to be as **homogeneous** (or \"pure\") as possible (without overfitting)\n",
    "* There are a variety of stopping criteria which can cause splitting to stop along a branch\n",
    "\n",
    "<img src='data/classifying_cars.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* **Note** the updated `features` and `label` columns above\n",
    "* Here, we want to build a decision tree which will use \"features\" to predict \"label\"\n",
    "\n",
    "#### Split train/test\n",
    "* Split data into training and testing sets\n",
    "\n",
    "```\n",
    "# Specify a seed for reproducability\n",
    "cars_train, cars_test = cars.randomSplit([0.8, 0.2], seed=23)\n",
    "```\n",
    "* Two DataFrames: `cars_train` and `cars_test`\n",
    "\n",
    "#### Build a Decision Tree model\n",
    "\n",
    "```\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree_model = tree.fit(cars_train)\n",
    "\n",
    "prediction = tree_model.transform(cars_test)\n",
    "```\n",
    "\n",
    "<img src='data/pred_cars.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37990e",
   "metadata": {},
   "source": [
    "#### Evaluating\n",
    "* Make predictions on the testing data and compare to known values\n",
    "* The `transform()` method added new columns to the DataFrame\n",
    "* The `prediction` column gives the class assigned by the model; you can compare this directly to the known labels in the testing data\n",
    "* Althought the model gets the first example wrong (above), it's correct for the following four examples\n",
    "* There's also a `probability` column which gives the probabilities assigned to each of the outcome classes\n",
    "\n",
    "#### Confusion matrix\n",
    "* A confusion matrix is a table which describes performance of a model on testing data\n",
    "* `prediction.groupBy(\"label\", \"prediction\").count().show()`\n",
    "\n",
    "<img src='data/cars_conf_mat.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4f03e",
   "metadata": {},
   "source": [
    "* **Accuracy = (TN + TP) / (TN + TP + FN + FP) -- proportion of correct predictions.**\n",
    "* A good way to understand the performance of a model is to create a confusion matrix, which gives a breakdown of the model predictions versus the known labels\n",
    "* The counts above can be used to calculate the accuracy, or any other classification metric (like precision, recall, f1, etc...)\n",
    "\n",
    "#### Exercises: Train/test split\n",
    "\n",
    "```\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print(training_ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932614a",
   "metadata": {},
   "source": [
    "#### Exercises: Build a Decision Tree\n",
    "\n",
    "```\n",
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)\n",
    "```\n",
    "\n",
    "#### Exercises: Evaluate the Decision Tree\n",
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "\n",
    "   * *True Negatives* (TN) — model predicts negative outcome & known outcome is negative\n",
    "   * *True Positives* (TP) — model predicts positive outcome & known outcome is positive\n",
    "   * *False Negatives* (FN) — model predicts negative outcome but known outcome is positive\n",
    "   * *False Positives* (FP) — model predicts positive outcome but known outcome is negative.\n",
    "   \n",
    "```\n",
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817da6b",
   "metadata": {},
   "source": [
    "### Logisitic Regression\n",
    "* Logistic regression is another commonly used classification model.\n",
    "* It uses a logistic function to model a binary target, where the target states are usually denoted by 1 and 0, or `True` and `False`.\n",
    "* **For a Logistic Regression model, the x-axis is a linear combination of predictor variables and the y-axis is the output of the model.**\n",
    "* Since the value of the logistic function is a number between zero and one, it's often thought of as a probability.\n",
    "* In order to translate this number into one or another of the target states, it's compared to a threshold, which is normally (but not always) set at one half.\n",
    "* To prepare for modeling, once again:\n",
    "    * assemble (consolidate) the predictors into a single column (called `features`) and\n",
    "    * split data into training and testing sets\n",
    "    \n",
    "#### Build a Logistic Regression model\n",
    "\n",
    "```\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic = logistic.fit(cars_train)\n",
    "prediction = logistic.transform(cars_test)\n",
    "```\n",
    "\n",
    "### Precision and recall\n",
    "* **Precision** is the proportion of positive predictions which are correct\n",
    "\n",
    "```\n",
    "# Precision (positive)\n",
    "TP / (TP + FP)\n",
    "```\n",
    "* **Recall** is the proportion of positive targers which are correctly predicted\n",
    "\n",
    "```\n",
    "# Recall (positive)\n",
    "TP / (TP + FN)\n",
    "```\n",
    "\n",
    "#### Weighted metrics\n",
    "* Another way of looking at these ratios is to weight them across the positive and negative predictions\n",
    "* You can do this by creating an evaluator object and then calling the `evaluate()` method\n",
    "\n",
    "```\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(prediction, {evaluator.metricName: 'weightedPrecision'})\n",
    "```\n",
    "* The `evaluate()` method (called on the `evaluator` object) accepts an argument which specifies the required metric\n",
    "* It's possible to request the following metrics:\n",
    "    * `weightedPrecision`\n",
    "    * `weightedRecall`\n",
    "    * `accuracy`\n",
    "    * `f1` : the harmonic mean of precision and recall, which is generally more robust than the accuracy\n",
    "* All of the above metrics have an assumed threshold of 0.5 but can be changed\n",
    "    * Choosing a larger or smaller value for the threshold will affect the performance of the model\n",
    "* A **threshold** is used to decide whether the number returned by the Logistic Regression model translates into either the positive or the negative class\n",
    "\n",
    "### ROC and AUC \n",
    "* The **ROC curve** plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left)\n",
    "* **ROC** = **Receiver Operating Characteristic**\n",
    "\n",
    "<img src='data/ROC_AUC.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "* TP versus FP\n",
    "* threshold = 0 (top right)\n",
    "* threshold = 1 (bottom left)\n",
    "#### AUC\n",
    "* The **AUC** summarizes the ROC curve in a single number.\n",
    "* **AUC** = **Area under the curve**\n",
    "* Ideally AUC = 1\n",
    "* An ideal model that performs perfectly regardless of the threshold, would have an AUC of 1\n",
    "* AUC indicates how well a model performs across all values of the threshold\n",
    "\n",
    "#### Exercises: Build a Logistic Regression Model\n",
    "\n",
    "```\n",
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "```\n",
    "\n",
    "#### Exercises: Evaluate the Logistic Regression model \n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target class.\n",
    "\n",
    "There are two other useful metrics, however:\n",
    "    * Precision\n",
    "    * Recall\n",
    "    \n",
    "Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "\n",
    "Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\n",
    "\n",
    "The precision and recall are generally formulated in terms of the positive target class. But it's also possible to calculate *weighted* versions of these metrics which look at both target classes.\n",
    "\n",
    "\n",
    "```\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc7016",
   "metadata": {},
   "source": [
    "### Turning Text into Tables\n",
    "* It is said that 80% of Machine Learning is data preparation (this is particularly true for text data)\n",
    "* Before you can use ML algorithms, you need to take unstructured text data and create structure, ultimately transforming the data into a table.\n",
    "* **Term-document matrix:**\n",
    "\n",
    "<img src='data/term_document_matrix.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Removing punctuation\n",
    "* Use **REGEX** (regular expressions), a mini-language for pattern-matching, to remove the punctuation symbols\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Regular expression (REGEX) to match commas and hyphens\n",
    "REGEX = '[,\\\\-]`\n",
    "\n",
    "books = books.withColumn('text', regexp_replace(books.text, REGEX, ' '))\n",
    "```\n",
    "* Above, **the hypen is escaped by the backslashes** because hyphens have another meaning in the context of regular expressions. **By escaping it, you tell Spark to interpret the hyphen literally.**\n",
    "* You need to specify a column name (`books.text`), a pattern to be matched (`REGEX`), and the replacement text, which is simply a space `' '`.\n",
    "* The results have some double spaces, but we can use REGEX to clean those up too.\n",
    "\n",
    "<img src='data/regex1.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65532fa",
   "metadata": {},
   "source": [
    "#### Text to tokens\n",
    "* Next, we split the text into words or **tokens.**\n",
    "* First, create a tokenizer object, giving it the name of the input column containing the text and the output column which will contain the tokens\n",
    "* The tokenizer is then applied to the text using the `transform()` method\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "books = Tokenizer(inputCol='text', outputCol='tokens').transform(books)\n",
    "```\n",
    "\n",
    "<img src='data/tokens1.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a2de0",
   "metadata": {},
   "source": [
    "* In the results, you can see a new column in which each document has been transformed into a list of words\n",
    "* As a side effect, all the words have been reduced to lower case and are comma-delimited\n",
    "\n",
    "#### What are stop words?\n",
    "* Common \"stop\" words convey very little information (like \"the\", \"at\", \"a\", etc), so you will typically also remove them\n",
    "* Remove stop words using an instance of the `StopWordsRemover` class\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopwords = StopWordsRemover()\n",
    "\n",
    "# Take a look at the list of stop words\n",
    "stopwords.getStopWords()\n",
    "```\n",
    "* This contains a list of stop words, which **can be customized if necessary.**\n",
    "* First, specify input and output column names and apply the transform method (we could also have given the input and output names when we created the remover)\n",
    "\n",
    "```\n",
    "# Specify the input and output column names\n",
    "stopwords = stopwords.setInputCol('tokens').setOutputCol('words')\n",
    "\n",
    "books = stopwords.transform(books)\n",
    "```\n",
    "\n",
    "#### Feature hashing\n",
    "* Documents could contain a large variety of words, so in principle our table could end up with an enormous number of columns, many of which would be only sparsely populated\n",
    "* It would also be handy to convert the words into numbers\n",
    "* Enter the **hashing trick**, which in simple terms converts words into numbers\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hasher = HashingTF(inputCol='words', outputCol='hash', numFeatures=32)\n",
    "books = hasher.transform(books)\n",
    "```\n",
    "* Create an instance of the HashingTF class, providing the names of the input and output columns \n",
    "* Also give the number of features, which is effectively the largest number that will be produced by the hashing trick\n",
    "    * This needs to be sufficiently big to capture the diversity in the words\n",
    "* The output in the hash column is presented in sparse format \n",
    "\n",
    "<img src='data/feature_hashing.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96e9f4",
   "metadata": {},
   "source": [
    "* **Note that there are two lists:**\n",
    "    * The first list contains the hashed values\n",
    "    * The second list indicates how many times each of those values occurs\n",
    "* For example, in the first document, the word `long` has a hash of `8` and occurs twice (`2.0`).\n",
    "\n",
    "#### Dealing with common words\n",
    "* The final step is to account for some words occurring frequently across many documents\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "books = IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)\n",
    "```\n",
    "* If a word appears in many documents, then it's probably going to be less useful for building a classifier\n",
    "* We want to weight the number of counts for a word in particular document against how frequently that word occurs across all documents. To do this you reduce the effective count for more common words, giving what is known as the **inverse document frequency**.\n",
    "* **Inverse document frequency** is generated by the IDF class, which is first fit to the hashed data and then used to generate weighted counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7667ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e0c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd541da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af167870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ca224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c363f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b2cc01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b671bf",
   "metadata": {},
   "source": [
    "<img src='data/course_datasets.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
