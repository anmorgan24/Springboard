{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33dc6ba9",
   "metadata": {},
   "source": [
    "# Machine Learning Model Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e9696",
   "metadata": {},
   "source": [
    "It's crucial that you develop an ability to determine which modeling metric to use based on the type of response variable you have and the business problem you're trying to solve. Likewise, it's also essential to be able to communicate your model's performance in terms that a stakeholder can understand. For example, if you can say your model predicts home values with an error range of \\\\$5,000 to $10,000 instead of an error rate of 5-10%, your stakeholders will quickly understand the impact of what you've created. \n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the machine learning process starts; it is a value that controls the learning process. Hyperparameter tuning is when you determine a set of hyperparameters to govern a learning algorithm. Hyperparameter tuning can make a world of difference â€” it can make or break your model's prediction abilities. For example, you may be able to improve your model's accuracy by 5% just by optimizing the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9c9f5",
   "metadata": {},
   "source": [
    "#### Supervised Machine Learning Model Evaluation Metrics\n",
    "* You should consider your response variable when choosing a machine learning model to build. \n",
    "* There are a variety of models out there, so it's important to choose one that's right for your data and the business problem you're trying to solve. \n",
    "\n",
    "* What's an evaluation metric?\n",
    "    * A way to quantify performance of a machine learning model\n",
    "    * Evaluation metric $\\neq$ Loss function\n",
    "        * The two can be the same thing, but don't have to be and aren't always.\n",
    "    * A loss function is something that you use while you are training your model, while you're optimizing, while evaluation metrics are used on an alredy trained machine learning model \n",
    "    #### Supervised Learning Metrics:\n",
    "    * **Classification:** Classification accuracy, Precision, Recall, F1 Score, ROC/AUC, Precision/Recall AUC, Matthews Correlation Coefficient, Log loss... etc...\n",
    "    * **Regression:** $R^{2}$, MAE, MSE, RMSE, RMSLE, etc...\n",
    "    \n",
    "### Classification Metrics:\n",
    "#### Binary Classificstion:\n",
    "\n",
    "* **Accuracy** = (Number of correct predictions) / (Total number of predictions)\n",
    "    * Ranges from 0%-100% or 0 to 1\n",
    "    * Very intuitive\n",
    "    * **Easily calculate with `sklearn` with `.score` method**\n",
    "    \n",
    "* **`Dummy Clasifier`:** in `sklearn` something that doesn't learn anything from the data; follows a simple strategy of: either generate numbers uniformly at random, or just predict most common/most frequent class it has seen in the data\n",
    "* If we don't know what our data looks like, we canmot determine if an accuracy score is good or not\n",
    "\n",
    "* **Confusion Matrix:** \n",
    "    * Tachnically not a metric, more of a diagnostic tool\n",
    "    * Helps to gain insight into the type of errors a model is making\n",
    "    * Helps to understand some other metrics\n",
    "    * `from skearn.metrics import confusion_matrix`\n",
    "    * `confusion_matrix(y_test, model.predict(X_test))`\n",
    "    * Convention is to first pass actual values, then predictions $\\Rightarrow$ so that in the rows you get the actual values and in the column you get the predicted values\n",
    "        * This convention used in `sklearn` and `tensorflow`. \n",
    "        * Be careful: other tools may use a different convention\n",
    "\n",
    "* **Precision:** (True positives) / (True positives + False positives)\n",
    "    * For when false positives are really important (like labeling an important email 'spam' and sending it to spam folder.\n",
    "    \n",
    "* **Recall:** (True positives) / (True positives + False negatives)\n",
    "    * For when false negatives are really important (like when testing for a really contagious/deadly disease like Ebola)\n",
    "    \n",
    "* **F1 Score:** Another way of summarizing the confusion matrix in one number, taking into account both precision and recall\n",
    "    * F1 Score is a harmonic mean of Precision and Recall\n",
    "    * `(2 * Precision * Recall) / (Precision + Recall)` =\n",
    "    * `(2 * TP) / 2*(TP + FP + FN)`\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e77dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4a3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fc72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8e9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3da602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
