{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b71ad68",
   "metadata": {},
   "source": [
    "# Machine Learning Model Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7ce0f",
   "metadata": {},
   "source": [
    "It's crucial that you develop an ability to determine which modeling metric to use based on the type of response variable you have and the business problem you're trying to solve. Likewise, it's also essential to be able to communicate your model's performance in terms that a stakeholder can understand. For example, if you can say your model predicts home values with an error range of \\\\$5,000 to $10,000 instead of an error rate of 5-10%, your stakeholders will quickly understand the impact of what you've created. \n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the machine learning process starts; it is a value that controls the learning process. Hyperparameter tuning is when you determine a set of hyperparameters to govern a learning algorithm. Hyperparameter tuning can make a world of difference â€” it can make or break your model's prediction abilities. For example, you may be able to improve your model's accuracy by 5% just by optimizing the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecbfba",
   "metadata": {},
   "source": [
    "#### Supervised Machine Learning Model Evaluation Metrics\n",
    "* You should consider your response variable when choosing a machine learning model to build. \n",
    "* There are a variety of models out there, so it's important to choose one that's right for your data and the business problem you're trying to solve. \n",
    "\n",
    "* What's an evaluation metric?\n",
    "    * A way to quantify performance of a machine learning model\n",
    "    * Evaluation metric $\\neq$ Loss function\n",
    "        * The two can be the same thing, but don't have to be and aren't always.\n",
    "    * A loss function is something that you use while you are training your model, while you're optimizing, while evaluation metrics are used on an alredy trained machine learning model \n",
    "    #### Supervised Learning Metrics:\n",
    "    * **Classification:** Classification accuracy, Precision, Recall, F1 Score, ROC/AUC, Precision/Recall AUC, Matthews Correlation Coefficient, Log loss... etc...\n",
    "    * **Regression:** $R^{2}$, MAE, MSE, RMSE, RMSLE, etc...\n",
    "    \n",
    "## Classification Metrics:\n",
    "### Binary Classification:\n",
    "#### Terminology\n",
    "* **Recall or Sensitivity or TPR (True Positive Rate):** Number of items correctly identified as positive out of total true positives- TP/(TP+FN)\n",
    "* **Specificity or TNR (True Negative Rate):** Number of items correctly identified as negative out of total negatives- TN/(TN+FP)\n",
    "* **Precision:** Number of items correctly identified as positive out of total items identified as positive- TP/(TP+FP)\n",
    "* **False Positive Rate or Type I Error:** Number of items wrongly identified as positive out of total true negatives- FP/(FP+TN)\n",
    "* **False Negative Rate or Type II Error:** Number of items wrongly identified as negative out of total true positives- FN/(FN+TP)\n",
    "* **F1 Score:** It is a harmonic mean of precision and recall given by- F1 = 2*Precision*Recall/(Precision + Recall)\n",
    "* **Accuracy:** Percentage of total items classified correctly- (TP+TN)/(N+P)\n",
    "    * (Number of correct predictions) / (Total number of predictions)\n",
    "    * Ranges from 0%-100% or 0 to 1\n",
    "    * Very intuitive\n",
    "    * **Easily calculate with `sklearn` with `.score` method**\n",
    "    \n",
    "* **`Dummy Clasifier`:** in `sklearn` something that doesn't learn anything from the data; follows a simple strategy of: either generate numbers uniformly at random, or just predict most common/most frequent class it has seen in the data\n",
    "* If we don't know what our data looks like, we canmot determine if an accuracy score is good or not\n",
    "\n",
    "* **Confusion Matrix:** \n",
    "    * Tachnically not a metric, more of a diagnostic tool\n",
    "    * Helps to gain insight into the type of errors a model is making\n",
    "    * Helps to understand some other metrics\n",
    "    * `from skearn.metrics import confusion_matrix`\n",
    "    * `confusion_matrix(y_test, model.predict(X_test))`\n",
    "    * Convention is to first pass actual values, then predictions $\\Rightarrow$ so that in the rows you get the actual values and in the column you get the predicted values\n",
    "        * This convention used in `sklearn` and `tensorflow`. \n",
    "        * Be careful: other tools may use a different convention\n",
    "\n",
    "* **Precision:** (True positives) / (True positives + False positives)\n",
    "    * For when minimizing false positives is really important (like labeling an important email 'spam' and sending it to spam folder).\n",
    "    \n",
    "* **Recall:** (True positives) / (True positives + False negatives)\n",
    "    * For when minimizing false negatives is really important (like when testing for a really contagious/deadly disease like Ebola).\n",
    "    \n",
    "* **F1 Score:** Another way of summarizing the confusion matrix in one number, taking into account both precision and recall\n",
    "    * F1 Score is a harmonic mean of Precision and Recall\n",
    "    * `(2 * Precision * Recall) / (Precision + Recall)` =\n",
    "    * `(2 * TP) / 2*(TP + FP + FN)`\n",
    "\n",
    "```\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: ', precision_score(y_test, model.predict(X_test)))\n",
    "print('Recall: ', recall_score(y_test, model.predict(X_test)))\n",
    "print('F1 Score: ', f1_score(y_test, model.predict(X_test)))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators' : [10, 200],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2', 0.5],\n",
    "}\n",
    "gs=GridSearchCV(estimator=model, param_grid=param_grid, scoring = 'recall', n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "recall_score(y_test, gs.best_estimator_.predict(X_test)))\n",
    "```\n",
    "* In this case, GridSearchCV is going to give us the estimator that is going to give us the best recall among the possible versions of hyperparameters\n",
    "\n",
    "\n",
    "* **Matthews Correlation Coefficient:**\n",
    "\n",
    "    * Takes into account all four confusion matrix categories\n",
    "    * One plus is the MCC throws errors to red flag certain situations, whereas the above mention metrics wouldn't throw errors\n",
    "    * If we flip what values are considered positives vs negatives $\\Rightarrow$ MCC score stays the same.\n",
    "        * In contrast to F1-score, which is very sensitive to what you call a positive and what you call a negative\n",
    "    * If you want to summarize a binary confusion matrix in one number, MCC is (arguably) the best way to do so.\n",
    "    * Downside to MCC: does not work well in multi-class problems\n",
    "\n",
    "\\begin{equation}\n",
    "MCC = \\frac{(TP * TN) - (FP * FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "\\end{equation}\n",
    "    \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "* Generating metrics based on correct vs incorrect $\\Rightarrow$ VS $\\Rightarrow$ calculating metrics based on probabilities    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e77688",
   "metadata": {},
   "source": [
    "* **Receiver Operating Characteristic (ROC) curve**:\n",
    "    * x-axis = False positive rate\n",
    "    * y-axis = True positive rate\n",
    "    * When we have a probability generated of an example belonging to one class or the other\n",
    "    \n",
    "\n",
    "\\begin{equation}\n",
    "True Positive Rate =\\frac {TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "False Positive Rate =\\frac {FP}{FP + TN}\n",
    "\\end{equation}\n",
    "\n",
    "```\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "sns.lineplot([0,1],[0,1], linestyle='--')\n",
    "plt = sns.lineplot(fpr, tpr, marker='.')\n",
    "auc_score = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.3f' % auc_score)\n",
    "```\n",
    "* **Area Under the Curve (AUC):**\n",
    "\n",
    "* **ROC-AUC Score:** \n",
    "    * The probabilistic interpretation of ROC-AUC score is that if you randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC. Here, rank is determined according to order by predicted values.\n",
    "    * Mathematically, it is calculated by area under curve of sensitivity (TPR) vs. FPR(1-specificity). Ideally, we would like to have high sensitivity & high specificity, but in real-world scenarios, there is always a tradeoff between sensitivity & specificity.\n",
    "    * The value can range from 0 to 1. However auc score of a random classifier for balanced data is 0.5\n",
    "    * ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d390980",
   "metadata": {},
   "source": [
    "* **Precision/Recall Curve)**\n",
    "\n",
    "```\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "pr_auc_score = auc(recall, precision)\n",
    "sns.lineplot([0,1],[0.5,0.5], linestyle='--'\n",
    "plt = sns.lineplot(recall, precision, marker='.')\n",
    "print('AUC: %.3f' % pr_auc_score)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c573749",
   "metadata": {},
   "source": [
    "* **Log loss:**\n",
    "    * Log-loss is a measurement of accuracy that incorporates the idea of probabilistic confidence given by following expression for binary class\n",
    "    * It takes into account the uncertainty of your prediction based on how much it varies from the actual label. \n",
    "    * Takes into account uncertainty of model predictions\n",
    "    * Larger penalty for confident false predictions\n",
    "    * Also often used as loss function (obviously)\n",
    "    * Intuition for log loss is easier to plot\n",
    "    \n",
    "* **For balanced data:**\n",
    "    * If you care for absolute probabilistic difference, go with log-loss\n",
    "    * If you care only for the final class prediction and you donâ€™t want to tune threshold, go with AUC score\n",
    "    * F1 score is sensitive to threshold and you would want to tune it first before comparing the models\n",
    "\n",
    "* **For imbalanced data:**\n",
    "    * Log-loss is failing in this case because according to log-loss both the models are performing equally. This is because log-loss function is symmetric and does not differentiate between classes \n",
    "    * Both F1-score and ROC-AUC are methods that can be used for class imbalanced scenarios.\n",
    "    \n",
    "    \n",
    "* If you care for a class which is smaller in number independent of the fact whether it is positive or negative, go for ROC-AUC score.\n",
    "* When you have a small positive class, then F1 score makes more sense. This is the common problem in fraud detection where positive labels are few."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c349a36",
   "metadata": {},
   "source": [
    "### Multi-class Classification\n",
    "* You can plot a confusion matrix for a multi-class problem\n",
    "* You can also use precision, recall, and F1 score \n",
    "* **Three types of non-binary classification:**\n",
    "    * **Multi-Class:** classification task with more than two classes such that the input is to be classified into one, and only one of these classes. Example: classify a set of images of fruits into any one of these categoriesâ€Šâ€”â€Šapples, bananas, and oranges.\n",
    "    * **Multi-labels:** classifying a sample into a set of target labels. Example: tagging a blog into one or more topics like technology, religion, politics etc. Labels are isolated and their relations are not considered important.\n",
    "    * **Hierarchical:** each category can be grouped together with similar categories, creating meta-classes, which in turn can be grouped again until we reach the root level (set containing all data). Examples include text classification & species classification. For more details, refer this blog.\n",
    "    * For a multi-class problem, the notions of **True positive**, **True negative**, and so on, don't really apply directly.\n",
    "    * But... this matrix can be extended to a multi-class problem by calculating them per label and then averaging them.\n",
    "        * There are many ways to average the precision, recall and F1 scores per label, but the three most common ways are:\n",
    "        * **macro:** `precision_score(actuals, predictions, average = 'macro')`\n",
    "            * each sample is equally represented in the average\n",
    "            * precision per class\n",
    "            * every class, regardless of its size contributes equally \n",
    "        * **micro:** `precision_score(actuals, predictions, average = 'micro')`\n",
    "            * each sample is equally represented in the average\n",
    "            * calculated using totals\n",
    "        * **weighted:** `precision_score(actuals, predictions, average = 'weighted')`\n",
    "            * precision per class weighted by how many samples per class\n",
    "            \n",
    "* **Micro-average:** all *samples* equally contribute to the average\n",
    "* **Macro-average:** all *classes* equally contribute to the average\n",
    "* **Weighted-average:** each class's contribution to the average is weighted by size.\n",
    "\n",
    "\n",
    "* When do you want each average? This largely depends on your data\n",
    "* If you have class-imbalanced data and you have one class that is under-represented, but you really want to correctly classify it, you might want to use **mscro-average** to make sure this under-represented class's contribution is amplified.\n",
    "* sklearn documentation recommends using **micro-average** for multi-label problems.\n",
    "* There is also a **multi-class log loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbd066",
   "metadata": {},
   "source": [
    "### Regression Metrics:\n",
    "* Regression metrics tend to be a little bit easier because you're not dealing with probabilities and you only have a continuous value, and you're predicting a continuous value and subtract one from the other, you get residuals.\n",
    "* Now the question is how to evaluate a model based on all the individual residuals \n",
    "\n",
    "\n",
    "#### R Squared (Coefficient of Determination):\n",
    "    * Indicates how well the model predictions approximate the true values\n",
    "    * 1 = perfect fit vs 0 = DummyRegressor predicting average\n",
    "    * $R^2$ has an intuitive scale and doesn't depend on y units\n",
    "    * $R^2$ gives you no information about prediction error.\n",
    "    * A negative $R^2$ *is* possible, but indicates something is very, very wrong with the model\n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, targets, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(max_features=0.5, n_estimators= 20)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "* `sklearn` gives classifiers a default metric of accuracy\n",
    "* `sklearn` gives regressors a default metric of $R^2$\n",
    "* The numerator is MSE ( average of the squares of the residuals) and the denominator is the variance in Y values. Higher the MSE, smaller the R_squared and poorer is the model.\n",
    "\n",
    "#### Adjusted R Squared\n",
    "\n",
    "* R Squared & Adjusted R Squared are often used for explanatory purposes and explains how well your selected independent variable(s) explain the variability in your dependent variable(s).\n",
    "* Just like RÂ², adjusted RÂ² also shows how well terms fit a curve or line but adjusts for the number of terms in a model.\n",
    "* Adjusted RÂ² will always be less than or equal to RÂ²\n",
    "\n",
    "#### Why should you choose Adjusted RÂ² over RÂ²?\n",
    "* There are some problems with normal RÂ² which are solved by Adjusted RÂ².\n",
    "* An adjusted RÂ² will consider the marginal improvement added by an additional term in your model. So it will increase if you add the useful terms and it will decrease if you add less useful predictors. However, RÂ² increases with increasing terms even though the model is not actually improving.\n",
    "\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "* Average of absolute value of the residuals\n",
    "* Because absolute value, retains original unit values\n",
    "* `np.average(np.abs(y_true - y_pred))\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "#### Mean Squared Error (MSE)\n",
    "* Because it is squared, the metric is now not in the units of the data\n",
    "\n",
    "* `np.average((y_true - y_pred)**2)`\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "* Minimizing the squared error over a set of numbers results in finding its mean, and minimizing the absolute error results in finding its median. This is the reason why MAE is robust to outliers whereas RMSE is not.\n",
    "\n",
    "#### RMSE (Root Mean Squared Error)\n",
    "* Because it is the root, the metric is not back to the units of the data\n",
    "* Even after being more complex and biased towards higher deviation, RMSE is still the default metric of many models because loss function defined in terms of RMSE is smoothly differentiable and makes it easier to perform mathematical operations.\n",
    "* If you care only about prediction accuracy then RMSE is best. It is computationally simple, easily differentiable and present as default metric for most of the models.\n",
    "\n",
    "\n",
    "\n",
    "#### What do MAE and RMSE have in common?**\n",
    "* range = 0 $\\Rightarrow$ $\\infty$\n",
    "* MAE and RMSE have the same units as y values \n",
    "* Indifferent to the direction of the errors\n",
    "* The lower the metric, the better (/\"more effective\" the model)\n",
    "* Neither metric is robust oon a small test set (<100)\n",
    "\n",
    "#### MAE vs RMSE: what's different?\n",
    "* RMSE gives a relatively high weight to large errors\n",
    "* MAE is more robust to outliers\n",
    "* RMSE is differentiable \n",
    "* (Arguably) RMSE is better is you expect the error distribution to be Gaussian\n",
    "* It is easy to understand and interpret MAE because it directly takes the average of offsets whereas RMSE penalizes the higher difference more than MAE.\n",
    "* Generally, RMSE will be higher than or equal to MAE. The only case where it equals MAE is when all the differences are equal or zero\n",
    "\n",
    "#### RMSLE Root Mean Squared Logarithmic Error\n",
    "* Similar to RMSE: uses natural logarithm of (y+1) instead of y\n",
    "* +1 because log of 0 is not defined\n",
    "* shows relative error\n",
    "    * important in cases where your targets have an exponential growth\n",
    "* Penalizes under-predicted estimate more than over-predicted\n",
    "\n",
    "\n",
    "\n",
    "* There's no one-size-fits-all evaluation metric\n",
    "* Get to know your data\n",
    "* Keep in mind business objective of your ML problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d6c8d",
   "metadata": {},
   "source": [
    "## Choosing the Right Metric for Evaluating ML Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b55626",
   "metadata": {},
   "source": [
    "[Full article part 1 here](https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html)\n",
    "\n",
    "[Full article part 2 here](https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html)\n",
    "\n",
    "What is the usefulness of each error metric depending on the objective and the problem we are trying to solve? Each machine learning model is trying to solve a problem with a different objective using a different dataset and hence, it is important to understand the context before choosing a metric.\n",
    "\n",
    "#### Most Useful Metrics\n",
    "* **Regression:**\n",
    "* MSPE\n",
    "* MSAE\n",
    "* R Squared\n",
    "* Adjusted R Squares\n",
    "\n",
    "\n",
    "* **Classification:**\n",
    "* Precision-Recall\n",
    "* ROC-AUC\n",
    "* Accuracy\n",
    "* Log loss\n",
    "\n",
    "\n",
    "* **Unsupervised Models:**\n",
    "* Rand Index\n",
    "* Mutual Information\n",
    "\n",
    "\n",
    "* **Others:**\n",
    "* CV Error\n",
    "* Heuristic methods to find K\n",
    "* BLEU score (NLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e33e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b83b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a36aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedab86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
