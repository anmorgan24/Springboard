{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1503416a",
   "metadata": {},
   "source": [
    "# Machine Learning Model Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac1f7d",
   "metadata": {},
   "source": [
    "It's crucial that you develop an ability to determine which modeling metric to use based on the type of response variable you have and the business problem you're trying to solve. Likewise, it's also essential to be able to communicate your model's performance in terms that a stakeholder can understand. For example, if you can say your model predicts home values with an error range of \\\\$5,000 to $10,000 instead of an error rate of 5-10%, your stakeholders will quickly understand the impact of what you've created. \n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the machine learning process starts; it is a value that controls the learning process. Hyperparameter tuning is when you determine a set of hyperparameters to govern a learning algorithm. Hyperparameter tuning can make a world of difference â€” it can make or break your model's prediction abilities. For example, you may be able to improve your model's accuracy by 5% just by optimizing the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc891cf8",
   "metadata": {},
   "source": [
    "#### Supervised Machine Learning Model Evaluation Metrics\n",
    "* You should consider your response variable when choosing a machine learning model to build. \n",
    "* There are a variety of models out there, so it's important to choose one that's right for your data and the business problem you're trying to solve. \n",
    "\n",
    "* What's an evaluation metric?\n",
    "    * A way to quantify performance of a machine learning model\n",
    "    * Evaluation metric $\\neq$ Loss function\n",
    "        * The two can be the same thing, but don't have to be and aren't always.\n",
    "    * A loss function is something that you use while you are training your model, while you're optimizing, while evaluation metrics are used on an alredy trained machine learning model \n",
    "    #### Supervised Learning Metrics:\n",
    "    * **Classification:** Classification accuracy, Precision, Recall, F1 Score, ROC/AUC, Precision/Recall AUC, Matthews Correlation Coefficient, Log loss... etc...\n",
    "    * **Regression:** $R^{2}$, MAE, MSE, RMSE, RMSLE, etc...\n",
    "    \n",
    "### Classification Metrics:\n",
    "#### Binary Classificstion:\n",
    "\n",
    "* **Accuracy** = (Number of correct predictions) / (Total number of predictions)\n",
    "    * Ranges from 0%-100% or 0 to 1\n",
    "    * Very intuitive\n",
    "    * **Easily calculate with `sklearn` with `.score` method**\n",
    "    \n",
    "* **`Dummy Clasifier`:** in `sklearn` something that doesn't learn anything from the data; follows a simple strategy of: either generate numbers uniformly at random, or just predict most common/most frequent class it has seen in the data\n",
    "* If we don't know what our data looks like, we canmot determine if an accuracy score is good or not\n",
    "\n",
    "* **Confusion Matrix:** \n",
    "    * Tachnically not a metric, more of a diagnostic tool\n",
    "    * Helps to gain insight into the type of errors a model is making\n",
    "    * Helps to understand some other metrics\n",
    "    * `from skearn.metrics import confusion_matrix`\n",
    "    * `confusion_matrix(y_test, model.predict(X_test))`\n",
    "    * Convention is to first pass actual values, then predictions $\\Rightarrow$ so that in the rows you get the actual values and in the column you get the predicted values\n",
    "        * This convention used in `sklearn` and `tensorflow`. \n",
    "        * Be careful: other tools may use a different convention\n",
    "\n",
    "* **Precision:** (True positives) / (True positives + False positives)\n",
    "    * For when minimizing false positives is really important (like labeling an important email 'spam' and sending it to spam folder).\n",
    "    \n",
    "* **Recall:** (True positives) / (True positives + False negatives)\n",
    "    * For when minimizing false negatives is really important (like when testing for a really contagious/deadly disease like Ebola).\n",
    "    \n",
    "* **F1 Score:** Another way of summarizing the confusion matrix in one number, taking into account both precision and recall\n",
    "    * F1 Score is a harmonic mean of Precision and Recall\n",
    "    * `(2 * Precision * Recall) / (Precision + Recall)` =\n",
    "    * `(2 * TP) / 2*(TP + FP + FN)`\n",
    "\n",
    "```\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: ', precision_score(y_test, model.predict(X_test)))\n",
    "print('Recall: ', recall_score(y_test, model.predict(X_test)))\n",
    "print('F1 Score: ', f1_score(y_test, model.predict(X_test)))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators' : [10, 200],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2', 0.5],\n",
    "}\n",
    "gs=GridSearchCV(estimator=model, param_grid=param_grid, scoring = 'recall', n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "recall_score(y_test, gs.best_estimator_.predict(X_test)))\n",
    "```\n",
    "* In this case, GridSearchCV is going to give us the estimator that is going to give us the best recall among the possible versions of hyperparameters\n",
    "\n",
    "\n",
    "* **Matthews Correlation Coefficient:**\n",
    "\n",
    "    * Takes into account all four confusion matrix categories\n",
    "    * One plus is the MCC throws errors to red flag certain situations, whereas the above mention metrics wouldn't throw errors\n",
    "    * If we flip what values are considered positives vs negatives $\\Rightarrow$ MCC score stays the same.\n",
    "        * In contrast to F1-score, which is very sensitive to what you call a positive and what you call a negative\n",
    "    * If you want to summarize a binary confusion matrix in one number, MCC is (arguably) the best way to do so.\n",
    "    * Downside to MCC: does not work well in multi-class problems\n",
    "\n",
    "\\begin{equation}\n",
    "MCC = \\frac{(TP * TN) - (FP * FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "\\end{equation}\n",
    "    \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "* Generating metrics based on correct vs incorrect $\\Rightarrow$ VS $\\Rightarrow$ calculating metrics based on probabilities    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46924c0f",
   "metadata": {},
   "source": [
    "* **Receiver Operating Characteristic (ROC) curve**:\n",
    "    * x-axis = False positive rate\n",
    "    * y-axis = True positive rate\n",
    "    * When we have a probability generated of an example belonging to one class or the other\n",
    "    \n",
    "\n",
    "\\begin{equation}\n",
    "True Positive Rate =\\frac {TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "False Positive Rate =\\frac {FP}{FP + TN}\n",
    "\\end{equation}\n",
    "\n",
    "* **Area Under the Curve (AUC):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52888ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93481df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fade3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671d538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
