{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a378c4",
   "metadata": {},
   "source": [
    "# Machine Learning Model Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfd86f",
   "metadata": {},
   "source": [
    "It's crucial that you develop an ability to determine which modeling metric to use based on the type of response variable you have and the business problem you're trying to solve. Likewise, it's also essential to be able to communicate your model's performance in terms that a stakeholder can understand. For example, if you can say your model predicts home values with an error range of \\\\$5,000 to $10,000 instead of an error rate of 5-10%, your stakeholders will quickly understand the impact of what you've created. \n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the machine learning process starts; it is a value that controls the learning process. Hyperparameter tuning is when you determine a set of hyperparameters to govern a learning algorithm. Hyperparameter tuning can make a world of difference â€” it can make or break your model's prediction abilities. For example, you may be able to improve your model's accuracy by 5% just by optimizing the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfcdf7",
   "metadata": {},
   "source": [
    "#### Supervised Machine Learning Model Evaluation Metrics\n",
    "* You should consider your response variable when choosing a machine learning model to build. \n",
    "* There are a variety of models out there, so it's important to choose one that's right for your data and the business problem you're trying to solve. \n",
    "\n",
    "* What's an evaluation metric?\n",
    "    * A way to quantify performance of a machine learning model\n",
    "    * Evaluation metric $\\neq$ Loss function\n",
    "        * The two can be the same thing, but don't have to be and aren't always.\n",
    "    * A loss function is something that you use while you are training your model, while you're optimizing, while evaluation metrics are used on an alredy trained machine learning model \n",
    "    #### Supervised Learning Metrics:\n",
    "    * **Classification:** Classification accuracy, Precision, Recall, F1 Score, ROC/AUC, Precision/Recall AUC, Matthews Correlation Coefficient, Log loss... etc...\n",
    "    * **Regression:** $R^{2}$, MAE, MSE, RMSE, RMSLE, etc...\n",
    "    \n",
    "## Classification Metrics:\n",
    "### Binary Classification:\n",
    "\n",
    "* **Accuracy** = (Number of correct predictions) / (Total number of predictions)\n",
    "    * Ranges from 0%-100% or 0 to 1\n",
    "    * Very intuitive\n",
    "    * **Easily calculate with `sklearn` with `.score` method**\n",
    "    \n",
    "* **`Dummy Clasifier`:** in `sklearn` something that doesn't learn anything from the data; follows a simple strategy of: either generate numbers uniformly at random, or just predict most common/most frequent class it has seen in the data\n",
    "* If we don't know what our data looks like, we canmot determine if an accuracy score is good or not\n",
    "\n",
    "* **Confusion Matrix:** \n",
    "    * Tachnically not a metric, more of a diagnostic tool\n",
    "    * Helps to gain insight into the type of errors a model is making\n",
    "    * Helps to understand some other metrics\n",
    "    * `from skearn.metrics import confusion_matrix`\n",
    "    * `confusion_matrix(y_test, model.predict(X_test))`\n",
    "    * Convention is to first pass actual values, then predictions $\\Rightarrow$ so that in the rows you get the actual values and in the column you get the predicted values\n",
    "        * This convention used in `sklearn` and `tensorflow`. \n",
    "        * Be careful: other tools may use a different convention\n",
    "\n",
    "* **Precision:** (True positives) / (True positives + False positives)\n",
    "    * For when minimizing false positives is really important (like labeling an important email 'spam' and sending it to spam folder).\n",
    "    \n",
    "* **Recall:** (True positives) / (True positives + False negatives)\n",
    "    * For when minimizing false negatives is really important (like when testing for a really contagious/deadly disease like Ebola).\n",
    "    \n",
    "* **F1 Score:** Another way of summarizing the confusion matrix in one number, taking into account both precision and recall\n",
    "    * F1 Score is a harmonic mean of Precision and Recall\n",
    "    * `(2 * Precision * Recall) / (Precision + Recall)` =\n",
    "    * `(2 * TP) / 2*(TP + FP + FN)`\n",
    "\n",
    "```\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: ', precision_score(y_test, model.predict(X_test)))\n",
    "print('Recall: ', recall_score(y_test, model.predict(X_test)))\n",
    "print('F1 Score: ', f1_score(y_test, model.predict(X_test)))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators' : [10, 200],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2', 0.5],\n",
    "}\n",
    "gs=GridSearchCV(estimator=model, param_grid=param_grid, scoring = 'recall', n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "recall_score(y_test, gs.best_estimator_.predict(X_test)))\n",
    "```\n",
    "* In this case, GridSearchCV is going to give us the estimator that is going to give us the best recall among the possible versions of hyperparameters\n",
    "\n",
    "\n",
    "* **Matthews Correlation Coefficient:**\n",
    "\n",
    "    * Takes into account all four confusion matrix categories\n",
    "    * One plus is the MCC throws errors to red flag certain situations, whereas the above mention metrics wouldn't throw errors\n",
    "    * If we flip what values are considered positives vs negatives $\\Rightarrow$ MCC score stays the same.\n",
    "        * In contrast to F1-score, which is very sensitive to what you call a positive and what you call a negative\n",
    "    * If you want to summarize a binary confusion matrix in one number, MCC is (arguably) the best way to do so.\n",
    "    * Downside to MCC: does not work well in multi-class problems\n",
    "\n",
    "\\begin{equation}\n",
    "MCC = \\frac{(TP * TN) - (FP * FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "\\end{equation}\n",
    "    \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "* Generating metrics based on correct vs incorrect $\\Rightarrow$ VS $\\Rightarrow$ calculating metrics based on probabilities    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1854334",
   "metadata": {},
   "source": [
    "* **Receiver Operating Characteristic (ROC) curve**:\n",
    "    * x-axis = False positive rate\n",
    "    * y-axis = True positive rate\n",
    "    * When we have a probability generated of an example belonging to one class or the other\n",
    "    \n",
    "\n",
    "\\begin{equation}\n",
    "True Positive Rate =\\frac {TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "False Positive Rate =\\frac {FP}{FP + TN}\n",
    "\\end{equation}\n",
    "\n",
    "```\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "sns.lineplot([0,1],[0,1], linestyle='--')\n",
    "plt = sns.lineplot(fpr, tpr, marker='.')\n",
    "auc_score = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.3f' % auc_score)\n",
    "```\n",
    "* **Area Under the Curve (AUC):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db9701",
   "metadata": {},
   "source": [
    "* **Precision/Recall Curve)**\n",
    "\n",
    "```\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "pr_auc_score = auc(recall, precision)\n",
    "sns.lineplot([0,1],[0.5,0.5], linestyle='--'\n",
    "plt = sns.lineplot(recall, precision, marker='.')\n",
    "print('AUC: %.3f' % pr_auc_score)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c72c8e",
   "metadata": {},
   "source": [
    "* **Log loss:**\n",
    "    * Takes into account uncertainty of model predictions\n",
    "    * Larger penalty for confident false predictions\n",
    "    * Also often used as loss function (obviously)\n",
    "    * Intuition for log loss is easier to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7f8cf",
   "metadata": {},
   "source": [
    "### Multi-class Classification\n",
    "* You can plot a confusion matrix for a multi-class problem\n",
    "* You can also use precision, recall, and F1 score \n",
    "    * For a multi-class problem, the notions of **True positive**, **True negative**, and so on, don't really apply directly.\n",
    "    * But... this matrix can be extended to a multi-class problem by calculating them per label and then averaging them.\n",
    "        * There are many ways to average the precision, recall and F1 scores per label, but the three most common ways are:\n",
    "        * **macro:** `precision_score(actuals, predictions, average = 'macro')`\n",
    "            * each sample is equally represented in the average\n",
    "            * precision per class\n",
    "            * every class, regardless of its size contributes equally \n",
    "        * **micro:** `precision_score(actuals, predictions, average = 'micro')`\n",
    "            * each sample is equally represented in the average\n",
    "            * calculated using totals\n",
    "        * **weighted:** `precision_score(actuals, predictions, average = 'weighted')`\n",
    "            * precision per class weighted by how many samples per class\n",
    "            \n",
    "* **Micro-average:** all *samples* equally contribute to the average\n",
    "* **Macro-average:** all *classes* equally contribute to the average\n",
    "* **Weighted-average:** each class's contribution to the average is weighted by size.\n",
    "\n",
    "\n",
    "* When do you want each average? This largely depends on your data\n",
    "* If you have class-imbalanced data and you have one class that is under-represented, but you really want to correctly classify it, you might want to use **mscro-average** to make sure this under-represented class's contribution is amplified.\n",
    "* sklearn documentation recommends using **micro-average** for multi-label problems.\n",
    "* There is also a **multi-class log loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209740bc",
   "metadata": {},
   "source": [
    "### Regression Metrics:\n",
    "* Regression metrics tend to be a little bit easier because you're not dealing with probabilities and you only have a continuous value, and you're predicting a continuous value and subtract one from the other, you get residuals.\n",
    "* Now the question is how to evaluate a model based on all the individual residuals \n",
    "\n",
    "\n",
    "#### $R^2$ (Coefficient of Determination):\n",
    "    * Indicates how well the model predictions approximate the true values\n",
    "    * 1 = perfect fit vs 0 = DummyRegressor predicting average\n",
    "    * $R^2$ has an intuitive scale and doesn't depend on y units\n",
    "    * $R^2$ gives you no information about prediction error.\n",
    "    * A negative $R^2$ *is* possible, but indicates something is very, very wrong with the model\n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, targets, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(max_features=0.5, n_estimators= 20)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "* `sklearn` gives classifiers a default metric of accuracy\n",
    "* `sklearn` gives regressors a default metric of $R^2$\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "* Average of absolute value of the residuals\n",
    "* Because absolute value, retains original unit values\n",
    "* `np.average(np.abs(y_true - y_pred))\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "#### Mean Squared Error (MSE)\n",
    "* Because it is squared, the metric is now not in the units of the data\n",
    "\n",
    "* `np.average((y_true - y_pred)**2)`\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "#### RMSE (Root Mean Squared Error)\n",
    "* Because it is the root, the metric is not back to the units of the data\n",
    "\n",
    "\n",
    "#### What do MAE and RMSE have in common?**\n",
    "* range = 0 $\\Rightarrow$ $\\infty$\n",
    "* MAE and RMSE have the same units as y values \n",
    "* Indifferent to the direction of the errors\n",
    "* The lower the metric, the better (/\"more effective\" the model)\n",
    "* Neither metric is robust oon a small test set (<100)\n",
    "\n",
    "#### MAE vs RMSE: what's different?\n",
    "* RMSE gives a relatively high weight to large errors\n",
    "* MAE is more robust to outliers\n",
    "* RMSE is differentiable \n",
    "* (Arguably) RMSE is better is you expect the error distribution to be Gaussian\n",
    "\n",
    "#### RMSLE Root Mean Squared Logarithmic Error\n",
    "* Similar to RMSE: uses natural logarithm of (y+1) instead of y\n",
    "* +1 because log of 0 is not defined\n",
    "* shows relative error\n",
    "    * important in cases where your targets have an exponential growth\n",
    "* Penalizes under-predicted estimate more than over-predicted\n",
    "\n",
    "\n",
    "\n",
    "* There's no one-size-fits-all evaluation metric\n",
    "* Get to know your data\n",
    "* Keep in mind business objective of your ML problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dead901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80338b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179aa134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24138d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c4abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b6ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
