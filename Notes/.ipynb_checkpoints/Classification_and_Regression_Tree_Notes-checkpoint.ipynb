{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e0247e",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682809ec",
   "metadata": {},
   "source": [
    "* Decision trees are supervised learning models used for problems involving classification and regression. \n",
    "* Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. \n",
    "* By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. \n",
    "* Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. \n",
    "\n",
    "\n",
    "* **Classification and Regression Trees (CART)** are a set of supervised learning models used for problems involving classification and regression.\n",
    "* Given a labeled dataset, classification tree learns a sequence of if-else questions about individual features in order to infer the labels \n",
    "    * **Objective: infer class labels**\n",
    "* Able to capture non-linear relationships between features and labels\n",
    "* Don't require feature scaling (ex: Standardization, etc..)\n",
    "* When a classification tree is trained, the tree learns a sequence of if-else questions, with each question involving one feature and one split point.\n",
    "* The maximum number of branches separating the top from an extreme end is known as the `maximum depth`. For example, a max_depth of 2 means 2 levels of branches (and 3 levels of if-else statements).\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(X_test, y_pred)\n",
    "```\n",
    "* `stratify= y` means train and test sets to have the same proportion of class labels as the unsplit dataset.\n",
    "\n",
    "\n",
    "* **Decision Regions:** A classification model divides the feature space into regions where all instances in one region are assigned to only one class label. These regions are known as **decision regions**.\n",
    "* **Decision Boundary:** surface (line, plane, hyperplane) separating different decision regions. \n",
    "* A classification tree produces rectangular decision regions in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c586c",
   "metadata": {},
   "source": [
    "#### Classification-Tree Learning\n",
    "* **Decision Tree:** data structure consisting of a hierarchy of nodes.\n",
    "* **Node:** question or prediction.\n",
    "* **Root:** *no* parent node, question giving rise to *two* children nodes.\n",
    "* **Internal Node:** *one* parent node, question giving rise to *two* children nodes.\n",
    "* **Leaf:** *one* parent node, *no* children nodes $\\Rightarrow$ *prediction*\n",
    "* **Information Gain:** The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature f and a split-point sp. But how does it know which feature and which split-point to pick? It does so by maximizing Information gain! The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split.\n",
    "\n",
    "* When a classification is trained on a labeled data set, the tree learns patterns from the features in such a way to produce the purest leaves.\n",
    "* When an **unconstrained tree** is trained, the nodes are grown recursively. In other words, a node is grown based on the state of its predecessors \n",
    "* At the non-leaf node the data is split based on feature *f* and and split point *sp* in such a way as to maximize *IG*\n",
    "* If the *IG* obtained by splitting a node is nil (0), the node is declared as a leaf.\n",
    "\n",
    "`dt = DecisionTreeClassifier(criterion= 'gini', random_state=1)`\n",
    "\n",
    "#### Decision Tree for Regression\n",
    "* In regression, the target variable is continuous (a real value)\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "dt= DecisionTreeRegressor(max_depth=4, min_samples_leaf= 0.1, random_state=3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "print(rmse_dt)\n",
    "\n",
    "```\n",
    "\n",
    "* When a regression tree is trained on a dataset, the impurity of a node is measure using the MSE of the targets in that node\n",
    "* This means that the Regression Tree tries to find the splits that produce the leaves where, in each leaf, the target values are, on average, the closest possible to the mean value of the labels in that particular leaf.\n",
    "* As a new instance traverses the tree and reaches a leaf, its target variable, *y*, is computed as the average of the target variables contained in that leaf.\n",
    "* Regression trees are able to capture greater flexibility than Linear Regression models (though are also more prone to overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832ec4b",
   "metadata": {},
   "source": [
    "gini-index and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467a01f",
   "metadata": {},
   "source": [
    "#### Generalization Error\n",
    "* In supervised learning, you make the assumption that there is a mapping, $f$, between features and labels:\n",
    "\\begin{equation}\n",
    "y = f(x)\n",
    "\\end{equation}\n",
    "where *f* is unknown.\n",
    "* In reality, data generation is always accompanied by randomness, or noise.\n",
    "* The goal of supervised learning is to find $\\hat f$ that best approximates $f$.\n",
    "* $\\hat f$ can be Logistic Regression, Decision Tree, Neural Network...\n",
    "* When training $\\hat f$, you want to make sure that noise is discarded as much as possible.\n",
    "* **End goal:** $\\hat f$ should achieve a low predictive error on unseen datasets.\n",
    "#### Difficulties in approximating $f$\n",
    "* Two main difficulties:\n",
    "    * **Overfitting:** $\\hat f(x)$ fits the training set *noise*.\n",
    "    * **Underfitting:** $\\hat f$ is not flexible enough to approximate $f$; here, the training set error will be roughly equivalent to the test set error and both errors are relatively high. \n",
    "    \n",
    "* **Generalization Error:** The generalization error of a model tells you how well it generalizes to unseen data\n",
    "    * Can be decomposed into three terms: bias, variance, and irreducible error\n",
    "    * $\\hat f$ = $bias^2$ + $variance$ + irreducible error\n",
    "    * irreducible error = error contribution of noise\n",
    "    \n",
    "* **Bias:** by how much are $\\hat f$ and $f$ different?\n",
    "    * High bias models lead to underfitting\n",
    "* **Variance:** tells you how much $\\hat f$ is inconsistent over different datasets. \n",
    "    * High variance models lead to overfitting\n",
    "* **Model Complexity:** sets the flexibility of $\\hat f$\n",
    "    * Example: maximum tree depth, minimum samples per leaf\n",
    "    * The goal is to find the model complexity that achieves the lowest generalization error \n",
    "    * Since this (generalization) error is the sum of three terms, with the irreducible error being constant, you need to find a balance between bias and variance (**\"Bias-Variance Tradeoff\"**) because as one increases, the other decreases.\n",
    "\n",
    "#### Diagnosing Bias and Variance Problems\n",
    "* How do you estimate $\\hat f$'s generalization error?\n",
    "* This cannot be done directly because:\n",
    "    * $f$ is unknown\n",
    "    * usually you only have one dataset\n",
    "    * noise is unpredictable\n",
    "* Solution:\n",
    "    * split the data into training and test sets\n",
    "    * train on training set, evaluate error of $\\hat f$ on test set\n",
    "    * **Test should only be used to evaluate $\\hat f$'s *final* performance.**\n",
    "    * To obtain a reliable estimate of $\\hat f$'s performance, use **Cross-Validation (CV).**\n",
    "        * K-Fold CV\n",
    "        * Hold-Out CV\n",
    "* **K-Fold CV:**\n",
    "    * If $\\hat f$'s cross-validation error is greater than $\\hat f$'s training error, then $\\hat f$ suffers from **high variance** (overfitting).\n",
    "        * Try decreasing model complexity\n",
    "        * For example: reduce maximum tree depth or increase maximum samples per leaf (for Decision Trees)\n",
    "        * Also try: gathering more data (if possible)\n",
    "    * If $\\hat f$'s CV error is roughly equal to the training set error, but much greater than desired error, then $\\hat f$ suffers from **high bias** (underfitting).\n",
    "        * Try increasing model complexity \n",
    "        * For example: increase max depth, decrease min samples per leaf (for Decision Trees)\n",
    "        * Also try: gathering *more relevant* features\n",
    "\n",
    "**K-Fold CV:**\n",
    "`n_jobs = -1` to exploit all available CPUs and computation.\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "from sklearn.model_selection import cross_val_score\n",
    "SEED=123\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=SEED)\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv=10, scoring= 'neg_mean_squared_error', n_jobs=-1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict_train = dt.predict(X_train)\n",
    "y_predict_test = dt.predict(X_test)\n",
    "```\n",
    "`print('CV MSE: {:.2f}'.format(MSE_CV.mean()))` \n",
    "\n",
    "`print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))` \n",
    "\n",
    "`print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))` \n",
    "\n",
    "```\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_train = dt.predict(X_train)\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd47ab7",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "* **Advantages of CARTs:**\n",
    "    * Simple to understand\n",
    "    * Simple to interpret\n",
    "    * Easy to use\n",
    "    * Flexibility: ability to describe non-linear dependencies\n",
    "    * Preprocessing: no need to standardize or normalize features, ...\n",
    "* **Limitations of CARTs:**\n",
    "    * Classification: Can only produce orthogonal decision boundaries\n",
    "    * Sensitive to small variations in the training set\n",
    "    * High variance: unconstrained CARTs may overfit the training set.\n",
    "    * **Solution: ensemble learning**\n",
    "    \n",
    "* **Ensemble Learning:** \n",
    "    * Train different models on the same dataset\n",
    "    * Let each model make its predictions\n",
    "    * Meta-model: aggregates predictions of individual models \n",
    "    * Final prediction: more robust and less prone to errors (than each individual model\n",
    "    * Best results: models are skillful in different ways \n",
    "    * One example ensemble learner in practice: Voter Classifier (different classifiers hard vote on final prediction)\n",
    "    \n",
    "**Voting Classifier in sklearn:**\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Logistic Regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemblsion(random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "classifiers = [('Logistic Regression', lr), \n",
    "                ('K Nearest Neighbors', knn)\n",
    "                ('Classification Tree', dt)]\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "```\n",
    "\n",
    "```\n",
    "vc = VotingClassifier(estimators = classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "```\n",
    "\n",
    "```\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "```\n",
    "\n",
    "```\n",
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c6ca1",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "* **Bootstrap aggregation = Bagging**\n",
    "* Voting Classifier is an ensemble of models that are fit to the training set using different algorithms. The final predictions were obtaining with majority voting.\n",
    "* In **bagging**, the ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire training set. Instead, each model is trained on a different subset of the data\n",
    "* Bagging has the effect of reducing the variance of individual models in an ensemble\n",
    "* Bootstrapping = sampling with replacement\n",
    "* Bagging = **B**ootstrap **agg**regation\n",
    "* In the training phase, bagging consists of drawing *n* different bootstrap samples from the training set\n",
    "* Each of these bootstrapped samples are then used to train *n* models that use the same algorithm\n",
    "* When a new instance is fed to the different models forming the bagging ensemble, each model outputs its prediction\n",
    "* The meta model collects these predictions and outputs a final prediction depending on the nature of the problem\n",
    "#### Bagging classification\n",
    "* Aggregates predictions be majority voting\n",
    "* `BaggingClassifier` in sklearn\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = SEED)\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {.3f}'.format(accuracy))\n",
    "```\n",
    "* `bc` consists of 300 decision trees: this done by setting `base_estimator` to `dt` and `n_estimators` to 300.\n",
    "#### Bagging regression\n",
    "* Aggregates predictions through averaging\n",
    "* `BaggingRegressor` in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ddbe8",
   "metadata": {},
   "source": [
    "#### Out Of Bag (OOB) Evaluation\n",
    "* In bagging, some instances may be sampled several times for one model, while other instances may not be sampled at all\n",
    "* On average, for each model, 63% of the training instances are sampled\n",
    "* The remaining 37% constitute **OOB instances.**\n",
    "* Since OOB instances are not seen by a model in training, they can be used to estimate/evaluate the performance of the ensemble, without the need for cross-validation\n",
    "* This technique is known as **OOB Evaluation.**\n",
    "* This leads to the the obtainment of *n* number of OOB scores, labelled $OOB_1$... $OOB_n$\n",
    "* The **OOB score** is the sum of *n* OOB scores, divided by *N*\n",
    "\n",
    "\\begin{equation}\n",
    "OOB_{score} = \\frac{{OOB}_1 + ... + {OOB_N}}{N}\n",
    "\\end{equation}\n",
    "* Note that in sklearn, the OOB score corresponds to accuracy for classifiers and $R^2$ score for regressors\n",
    "* **OOB Evaluation in sklearn:**\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state=SEED)\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf= 0.16, random_state=SEED)\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "bc.fit((X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "oob_accuracy = bc.oob_score_\n",
    "print('Test set accuracy: {.3f}'format(test_accuracy))\n",
    "print('OOB accuracy: {.3f}'.format(oob_accuracy))\n",
    "```\n",
    "\n",
    "```\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, oob_score=True, random_state=1)\n",
    "\n",
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a94be",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "* Random Forests are another ensemble learning method\n",
    "* In bagging, base estimator could be any model, including Decision Tree, Logistic Regression, Neural Net, etc...\n",
    "* Each estimator is trained on a distinct bootstrap sample of the training set using all available features for training and prediction\n",
    "* **Random Forests** is an ensemble method that uses a decision tree as a base estimator \n",
    "* In Random Forests, each estimator is trained on a different bootstrap sample having the same size as the training set \n",
    "* RF introduces further randomization in the training of individual trees\n",
    "* When each tree is trained, only *d* features can be sampled at each node without replacement, where *d* < total number of features\n",
    "* Note that each tree forming the ensemble is trained on a different bootstrap sample from the training set\n",
    "* Nodes are split using the sample feature that maximizes information gain (*IG*)\n",
    "* In `sklearn` *d* defaults to the squareroot of total features \n",
    "* Once trained, predictions can be made on new instances\n",
    "* When a new instance is fed to the different base estimators, each of them ouputs a prediction\n",
    "* The predictions are then collected by the RF metamodel and the final prediction is made depending on the nature of the problem \n",
    "    * For **classification** the prediction is made by **majority voting**\n",
    "        * The corresponding `sklearn` class is `RandomForestClassifier`\n",
    "    * For **regression** the prediction aggregates all predictions by **averaging** them\n",
    "        * The corresponding `sklearn` class is `RandomForestRegressor`\n",
    "* In general, RF produce lower variance than individual trees. \n",
    "\n",
    "* **Random Forest Regressor in sklearn:**\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "SEED=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state= SEED)\n",
    "rf = RandomForestRegressor(n_estimators = 400, min_samples_leaf=0.12, random_state=SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print(\"Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "```\n",
    "* When a tree-based method is trained, the predictive power of a feature (or its importance) can be assessed\n",
    "* In `sklearn` feature importance is assessed by measuring how much the tree nodes use a particular feature (weighted average) to reduce impurity.\n",
    "* Note that the importance of a feature is expressed as a percentage indicating the weight of that feature in training and prediction\n",
    "    * access using `.feature_importance_` attribute from model\n",
    "#### Visualize feature importance in sklearn\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb55c67",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "* **Boosting:** refers to an ensemble method in which many predictors are trained and each predictor learns from the errors of its predecessor \n",
    "* **Boosting:** ensemble method combining several weak learners to form a strong learner.\n",
    "* **Weak learner:** a model doing slightly better than random guessing\n",
    "    * Example of a weak learner: A dt with a max_depth of 1 (aka a 'Decision Stump')\n",
    "* In boosting, an ensemble of predictors are trained sequentially and each predictor tries to correct its predecessor \n",
    "* Most popular boosting methods:\n",
    "    * **AdaBoost** \n",
    "    * **Gradient Boost** \n",
    "* **AdaBoost:** stands for **Ada**ptive **Boost**ing\n",
    "    * Each predictor pays more attention to the instances wrongly predicted by its predecessor\n",
    "    * Achieved by changing the weights of training instances\n",
    "    * Each predictor is assigned a coefficient $\\alpha$\n",
    "    * $\\alpha$ depends on the predictor's training error\n",
    "    * An important parameter used in training is the **learning rate**, eta, or $\\eta$\n",
    "    * Eta is a number between 0 and 1; it is used to shrink the coefficient alpha of a trained predictor. It's important to note that there's a trade-off between eta and the number of estimators. A smaller value of eta should be compensated by a greater number of estimators.\n",
    "    * Once all the predictors in an ensemble are trained, the label of a new instance can be predicted depending on the nature of the problem\n",
    "    * **Classification:** \n",
    "        * Weighted majority voting\n",
    "        * sklearn: `AdaBoostClassifier`\n",
    "    * **Regression:**\n",
    "        * Weighted average\n",
    "        * sklearn: `AdaBoostRegressor`\n",
    "    * Note: individual predictors do not need to be CARTs, however CARTs are very often used due to their high variance\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.3, stratify = y, random_state = SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d3fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c207da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893ddee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
