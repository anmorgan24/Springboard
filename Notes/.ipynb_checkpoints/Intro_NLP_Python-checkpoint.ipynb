{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d15eff",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing in Python\n",
    "In this course, you'll learn natural language processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a759add",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter 1: Regular expressions & word tokenization\n",
    "This chapter will introduce some basic NLP concepts, such as word tokenization and regular expressions to help parse text. You'll also learn how to handle non-English text and more difficult tokenization you might find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d4d99",
   "metadata": {},
   "source": [
    "#### What is Natural Language Processing?\n",
    "* Massive field of study focused on making sense of language using statistics and computers\n",
    "* Some of the basics of NLP:\n",
    "    * Topic identification\n",
    "    * Text classification\n",
    "* NLP applications include:\n",
    "    * topic identification\n",
    "    * chatbots\n",
    "    * text classification\n",
    "    * translation\n",
    "    * sentiment analysis\n",
    "    * ... many, many more!\n",
    "    \n",
    "#### Regular Expressions\n",
    "* **Regular expressions** are strings you can use that have a special syntax which allows you to match patterns and find other strings\n",
    "* A **pattern** is a series of letters or symbols which can map to an actual text or words or punctuation.\n",
    "* Applications of regular expression:\n",
    "    * Find links in a a webpage or document\n",
    "    * Parse email addresses\n",
    "    * Remove unwanted strings or characters\n",
    "* Regular expressions are often referred to as **regex** and can be used easily with Python via the `re` library\n",
    "* match a substring using the `re.match()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb38909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pycld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e15362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import spacy\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379b25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85151214",
   "metadata": {},
   "source": [
    "* `re.match()` takes the pattern as the first argument, the string as the second argument, and returns a **match object**\n",
    "* We can also use \"special\" patterns that regex understands, like the `\\w+`, which will match a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfe9ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfb3d6",
   "metadata": {},
   "source": [
    "* There are hundreds of characters and patterns you can learn and memorize with regular expressions, but here we get started with a few common patterns:\n",
    "\n",
    "<img src='data/common_regex.png' width=\"300\" height=\"150\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f73e4d",
   "metadata": {},
   "source": [
    "#### Python's re modules\n",
    "* **`re`** module\n",
    "* **`split`**: split a string on a regex\n",
    "* **`findall`**: find all patterns in a string\n",
    "* **`search`**: search for a pattern\n",
    "* **`match`**: match an entire string or substring based on a pattern\n",
    "* Syntax for regex library is always to pass the pattern first and the string second\n",
    "* Depending on the method, it may return an iterator, a new string, or a match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f0ae60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073031b",
   "metadata": {},
   "source": [
    "* This can be used for tokenization, so you can process text using regex while doing NLP\n",
    "\n",
    "#### Exercises: Practicing regular expressions: re.split() and re.findall()\n",
    "\n",
    "```\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n",
    "```\n",
    "\n",
    "### Introduction to tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee938799",
   "metadata": {},
   "source": [
    "* **Tokenization** is the process of transforming a string or document into smaller chunks, which we call tokens.\n",
    "* One step in the process of preparing a text for NLP\n",
    "* Many different theories and rules regarding tokenization\n",
    "    * You can also create your own tokenization rules using regular expressions\n",
    "* Some examples:\n",
    "    * Breaking out words or sentences\n",
    "    * Separating punctuation\n",
    "    * Separating all hashtags in a tweet\n",
    "\n",
    "#### nltk library\n",
    "* One library that is commonly used for simple tokenization is `nltk`, the **natural language took kit** library\n",
    "* `nltk`: natural langauge toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2390a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbe672f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07db31",
   "metadata": {},
   "source": [
    "#### Why tokenize?\n",
    "* Tokenizing can help us with some simple text processing tasks like:\n",
    "    * Mapping parts of speech\n",
    "    * Matching common words\n",
    "    * Removing unwanted tokens\n",
    "    \n",
    "#### Other nltk tokenizers\n",
    "* **`sent_tokenize`:** tokenize a document into sentences\n",
    "* **`regexp_tokenize`:** tokenize a string or document based on a regular expression pattern\n",
    "* **`TweetTokenizer`:** special class just for tweet tokenization, allowing you to separate hashtags, mentions, and lots of exclamation points\n",
    "\n",
    "#### More regex practice\n",
    "* Difference between `re.search()` and `re.match()`:\n",
    "    * When we use `search` and `match` with the same pattern and string when the pattern is at the beginning of the string, we see we find identical matches.\n",
    "    \n",
    "<img src='data/search_vs_match.png' width=\"600\" height=\"300\" align=\"center\"/>    \n",
    "\n",
    "* **Note that `match` will try and match a string from the beginning until it cannot match any longer, while `search` will go through the ENTIRE string to look for match options.**\n",
    "* So, if you need to find a patter that might not be at the beginning of the string, you should use `search`\n",
    "* If you want to be specific about the composition of the entire string, or at least the initial pattern, then you should use `match`\n",
    "\n",
    "#### Exercises: Word tokenization with NLTK\n",
    "\n",
    "```\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n",
    "\n",
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843c22d",
   "metadata": {},
   "source": [
    "### Advanced tokenization with regex\n",
    "\n",
    "#### Regex groups using or \"|\"\n",
    "* OR is represented using **`|`**\n",
    "* Define a group using **`()`**\n",
    "    * Groups can be either a pattern or set of characters you want to match\n",
    "* You can also define explicit character classes using **`[]`**\n",
    "* Example: we want to find all digits and words using tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2373e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "match_digits_and_words = ('(\\d+|\\w+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf2f693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(match_digits_and_words, \"He has 11 cats.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969978f",
   "metadata": {},
   "source": [
    "* Pseudo script: \"find all\" digits and/or words.\n",
    "\n",
    "#### Regex ranges and groups\n",
    "\n",
    "<img src='data/regex_ranges.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f58ade",
   "metadata": {},
   "source": [
    "* **Note** that:\n",
    "    * **ranges** are marked with **`[]`**\n",
    "    * **groups** are marked with **`()`**\n",
    "* We can see in the chart above that we can use square brackets to defne a new character class\n",
    "* Note in the third row of the chart, that because the hyphen and period are special characters in regex, we must tell regex we mean an ACTUAL period or hyphen\n",
    "    * To do so we use what is called an **escape character**: in regex that means to place a backwards slash in front of our character so it knows then to look for a hyphen or period.\n",
    "* On the other hand, with groups which are designated by the parentheses, we can only match what we explicitly define in the group\n",
    "    * For example, see row four in the chart above; this regex only specifies 3 characters to match: `a`, `-`, `z` (and *not* \"all the lowercase letters between a and z).\n",
    "    * **Groups are useful when you want to define an explicit set of characters.**\n",
    "* Final example: spaces or a comma.\n",
    "* In the code example below, use `match` with a character range to match all lowercase ascii, any digits, and spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38bac1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import re\n",
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6eae52",
   "metadata": {},
   "source": [
    "* The above regex is **greedy**, marked by the **`+`** after the range definition, but once it hits the comma, it can't match any more.\n",
    "* This short example demonstrates that thinking about what regex method you use (such as `search` versus `match`) and whether you define a *group* or a *range* can have a large impact on the usefulness and readability of your patterns.\n",
    "\n",
    "#### Exercises: Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. \n",
    "\n",
    "*Unlike the syntax for the regex library, with `nltk_tokenize()` you pass the pattern as the **second** argument.*\n",
    "\n",
    "```\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "# PART 1\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "\n",
    "# PART 2\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([#|@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "\n",
    "# PART 3\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24079b",
   "metadata": {},
   "source": [
    "#### Exercises: Non-ascii Tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called `german_text`, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from `nltk.tokenize`: `regexp_tokenize` and `word_tokenize`.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "`('\\U0001F300'-'\\U0001F5FF')`, `('\\U0001F600-\\U0001F64F')`, `('\\U0001F680-\\U0001F6FF')`, and `('\\u2600'-\\u26FF-\\u2700-\\u27BF')`.\n",
    "\n",
    "```\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n",
    "```\n",
    "\n",
    "### Charting word length with nltk\n",
    "* Using `nltk` with `matplotlib`\n",
    "* Tokenize text and chart word length for a simple sentence\n",
    "\n",
    "#### Combining NLP data extraction with plotting\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the words and punctuation in a short sentence\n",
    "words = word_tokenize(\"this is a pretty cool tool!\")\n",
    "word_lengths = [len(w) for w in words]\n",
    "plt.hist(word_lengths)\n",
    "```\n",
    "* As a brief refresher on list comprehensions, they are a succinct way to write a for loop\n",
    "\n",
    "#### Exercises: Charting practice\n",
    "Try using your new skills to find and chart the number of words per line in the script using `matplotlib`. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: `my_lines = [tokenize(l) for l in lines]` will call a function `tokenize` on each line in the list `lines`. The new transformed list will be saved in the `my_lines` variable.\n",
    "\n",
    "You have access to the entire script in the variable `holy_grail`. Go for it!\n",
    "\n",
    "```\n",
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e145ed",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Simple topic identification\n",
    "This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods: bag-of-words and Tf-idf using NLTK, and a new library `Gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08b240",
   "metadata": {},
   "source": [
    "### Word counts with bag-of-words\n",
    "#### Bag-of-words\n",
    "* Basic method for finding topics in a text\n",
    "* Need to first create tokens using tokenization\n",
    "* ... and then count up all the tokens\n",
    "* Theory: **the more frequent a word or token is, the more central or important it might be to the text.**\n",
    "* Bag-of-words can be a great way to determine the significant words in a text (based on the number of times they are used)\n",
    "\n",
    "#### Bag-of-words in Python\n",
    "\n",
    "```\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d86e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04830102",
   "metadata": {},
   "source": [
    "* The result is a `counter` object, which has a similar structure to a dictionary and allows us to see each token and the frequency of the token.\n",
    "* `counter` objects also have a method called **`most_common()`**, which takes an integer argument, such as `2`, and would then return the top 2 tokens in terms of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c2c40e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45ce7c",
   "metadata": {},
   "source": [
    "* The returned object is a series of tuples inside a list\n",
    "* For each tuple, the first element holds the token and the second element represents the frequency\n",
    "* **Note:** Other than ordering by token frequency, the `most_common` method does not sort the tokens it returns or tell us there are more tokens with that same frequency. \n",
    "    * For example: In the above example, the `most_common()` method doesn't alert us that `'box'` also has a frequency of `3`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e68095",
   "metadata": {},
   "source": [
    "#### Exercises: Building a Counter with bag-of-words\n",
    "\n",
    "```\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7693d",
   "metadata": {},
   "source": [
    "### Simple text preprocessing\n",
    "* Text preprocessing helps make for better input data when performing machine learning or other statisticals methods\n",
    "* Examples:\n",
    "    * Tokenization to create a bag of words\n",
    "    * Lowercasing words/tokens\n",
    "* **Lemmatization** or **Stemming**: shortening the words to their root stems \n",
    "* **Removing stop words**, punctuation, or unwanted tokens: stop words are common words in a language that may not convey much meaning regarding content or topics\n",
    "* Good to experiment with different approaches\n",
    "\n",
    "#### Preprocessing example\n",
    "* Input: Cats, dogs, and birds are common pets. So are fish.\n",
    "* Output: cat, dog, bird, common, pet, fish\n",
    "\n",
    "#### Text preprocessing with Python\n",
    "* Below we use list comprehensions to tokenize the sentences which we first make lowercase using the string `lower()` method.\n",
    "* The **`isalpha()`** method will return `True` if the string has *only* alphabetical characters\n",
    "    * Effectively strips tokens of numbers or punctuation when used as a conditional within list comprehension\n",
    "* We use another list comprehension to remove words that are in the stopwords list\n",
    "    * This **`stopwords` list comes built in with the `nltk` library.**\n",
    "* Finally, we create a counter and check the two most common words, which are now cat and box (\"the\" is no longer included, nor is \"The\")\n",
    "\n",
    "```\n",
    "from ntlk.corpus import stopwords\n",
    "\n",
    "text = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] \n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f476fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] \n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ffaa4",
   "metadata": {},
   "source": [
    "* As demonstrated, preprocessing has already improved our bag of words and made.\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "```\n",
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ca1f1",
   "metadata": {},
   "source": [
    "### Introduction to gensim\n",
    "* **Gensim** is a popular open-source NLP library\n",
    "* It uses top academic models to perform complex tasks like:\n",
    "    * Building document or word vectors\n",
    "    * Building corpora\n",
    "    * Performing topic identification and document comparisons\n",
    "    \n",
    "<img src='data/word_vectors.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5aa910",
   "metadata": {},
   "source": [
    "#### Word vectors\n",
    "* A **word embedding** or **vector** is trained from a larger corpus and is a multi-dimensional representation of a word or document\n",
    "    * Think of it as a multi-dimensional array normally with sparse features \n",
    "    * With these vectors we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find\n",
    "* In the image above, we see that `king minus queen` is approximately equal to `man minus woman`.\n",
    "* Or, that `Spain` is to `Madrid` as `Italy` is to `Rome`.\n",
    "* The deep learning algorithm used to create word vectors has been able to distill this meaning based on how those words are used throughout the text.\n",
    "\n",
    "<img src='data/LDA_viz.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* The image above is an example of LDA visualization\n",
    "* **LDA** stands for **Latent Dirichlet Allocation** and is a statistical model that we can apply to text using Gensim for topic analysis and modeling\n",
    "* Link to above article [HERE](http://tlfvincent.github.io/2015/10/23/presidential-speech-topics)\n",
    "\n",
    "#### Gensim\n",
    "* Gensim allows you to build corpora and dictionaries using simple classes and functions \\\n",
    "* A **corpus** (or, if plural, **corpora**) is a set of texts used to help perform NLP tasks\n",
    "\n",
    "```\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_documents = [\"The movie was about a spaceship and aliens.\",\n",
    "                \"I really liked the movie!\",\n",
    "                \"Awesome action scenes, but boring characters.\",\n",
    "                \"The movie was awful! I hate alien films.\",\n",
    "                \"Space is cool!\" I liked the movie.\",\n",
    "                \"More space films, please!\"]\n",
    "```\n",
    "* In the above example, our \"documents\" are a list of strings that look like movie reviews about space or sci-fi films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69dc96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = [\"The movie was about a spaceship and aliens.\",\n",
    "                \"I really liked the movie!\",\n",
    "                \"Awesome action scenes, but boring characters.\",\n",
    "                \"The movie was awful! I hate alien films.\",\n",
    "                \"Space is cool! I liked the movie.\",\n",
    "                \"More space films, please!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9259b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower-case-it and tokenize\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb901a0c",
   "metadata": {},
   "source": [
    "* Then, pass the tokenized documents to the Gensim Dictionary class:\n",
    "    * This will create a mapping with an id for each token\n",
    "    * This is the beginning of our corpus\n",
    "    * We can now represent whole documents using just a list of their token ids and how often those tokens appear in each document\n",
    "    * We can take a look at the tokens and their ids by looking at the `token2id` attribute, which is a dictionary of all our tokens and their respective ids in our new dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9c0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2356cd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fdb48ed63a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b21652e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dceb941",
   "metadata": {},
   "source": [
    "#### Creating a gensim corpus\n",
    "* Using the dictionary created above, we can then create a Gensim corpus, which is a bit different from a normal corpus (which is just a collection of documents)\n",
    "* Gensim uses a simple bag-of-words model which transforms each document into a bag-of-words using the token ids and the frequency of each token in the document\n",
    "* Below, we can see that the Gensim corpus is a list of list, with each litem item representing one document.\n",
    "* Each document is now a series of tuples, the first item representing the tokenid from the dictionary and the second item representing the token frequency in the document\n",
    "* Unlike our previous Counter-based bow, this Gensim model can be easily save, updated, and reused thanks to the extra tools we have available in Gensim\n",
    "* Our dictionary can also be updated with new texts and extract only words that meet particular thresholds\n",
    "* We are building a more advanced and feature-rich bag-of-words model which can then be used for future exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab359e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a87d9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfa7f7",
   "metadata": {},
   "source": [
    "#### Exercises: Creating and querying a corpus with gensim\n",
    "\n",
    "```\n",
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "```\n",
    "\n",
    "#### Exercises: Gensim bag-of-words\n",
    "Now, you'll use your new `gensim` corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
    "\n",
    "You have access to the `dictionary` and `corpus` objects you created in the previous exercise, as well as the Python `defaultdict` and `itertools` to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "* `defaultdict` allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument `int`, we are able to ensure that any non-existent keys are automatically assigned a default value of `0`. This makes it ideal for storing the counts of words in this exercise.\n",
    "\n",
    "* `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our `corpus` object (which is a list of lists).\n",
    "\n",
    "The fifth document from `corpus` is stored in the variable `doc`, which has been sorted in descending order.\n",
    "\n",
    "```\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d6455",
   "metadata": {},
   "source": [
    "### Tf-idf with gensim\n",
    "* Here we will learn how to use a TFIDF model with Gensim\n",
    "\n",
    "#### What is tf-idf?\n",
    "* **T**erm **f**requency- **i**nverse **d**ocument **f**requency\n",
    "* Allows you to determine the most important words in each documentin the corpus\n",
    "* Underlying theory: Each corpus may have more shared words than just stopwords; these common words are like stopwords and should be removed or at least down-weighted in impotance\n",
    "* Tf-idf ensures most common words don't show up as key words\n",
    "* Keeps document-specific frequent words weighted high\n",
    "    * (And the common words across the entire corpus weighted low)\n",
    "    \n",
    "<img src='data/tfidf_formula.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e2c5f",
   "metadata": {},
   "source": [
    "* The weight will be low if the term doesn't appear often in the document because the tf variable will then be low. However, the weight will also be low if the logarithm is close to zero, meaning the internal equation is low\n",
    "\n",
    "### Tf-idf with gensim\n",
    "* We reference each document by using it like a dictionary key with our new tfidf model \n",
    "\n",
    "```\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260a6e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.1746298276735174),\n",
       " (7, 0.1746298276735174),\n",
       " (9, 0.1746298276735174),\n",
       " (10, 0.29853166221463673),\n",
       " (11, 0.47316148988815415),\n",
       " (12, 0.7716931521027908)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acf486",
   "metadata": {},
   "source": [
    "* For the second document in our corpora, we see the token weights along with the token ids. Notice there are some large differences.\n",
    "* These weights can help you determine good topics and keywords for a corpus with shared vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98192d93",
   "metadata": {},
   "source": [
    "#### Exercises: Tf-idf with Wikipedia\n",
    "\n",
    "```\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b7b2f",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: Named-entity recognition\n",
    "This chapter will introduce a slightly more advanced topic: named-entity recognition. You'll learn how to identify the who, what, and where of your texts using pre-trained models on English and non-English text. You'll also learn how to use some new libraries, polyglot and spaCy, to add to your NLP toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefec22",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "#### What is Named Entity Recognition?\n",
    "* Named Entity Recognition (or **NER**) is an NLP task used to identify important named entities in the text\n",
    "    * People, places, and organizations\n",
    "    * Dates, states, works of art\n",
    "    * ...and other categories (depending on the libraries and notation you use)\n",
    "* NER can be used alongside topic identification.. or on its own\n",
    "* Who? What? When? Where?\n",
    "\n",
    "<img src='data/NER_example.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916aca70",
   "metadata": {},
   "source": [
    "* The text above has been highlighted for different types of named entities that were found using the **Stanford NER library**.\n",
    "* Use NER to solve problems like fact extraction or which entities are related... by using computational language models.\n",
    "\n",
    "### nltk and the Stanford CoreNLP Library\n",
    "* NLTK allows you to interact with NER via its own model, but also the Stanford CoreNLP library\n",
    "* **The Stanford CoreNLP library:**\n",
    "    * Integrated into Python via `nltk`\n",
    "    * Java based\n",
    "    * You can also used the Stanford library on its own without integrating it with NLTK or operate it as an API server\n",
    "    * Great support for **NER** as well as **coreference** and **dependency trees**\n",
    "        * **Coreference:** linking pronouns and entities together\n",
    "        * **Dependency trees:** help with parsing meaning and relationships amongst words or phrases in a sentence\n",
    "* For our simple-use case, we will use the *built-in* named entity recognition with NLTK\n",
    "\n",
    "#### Using nltk for Named Entity Recognition\n",
    "* Take a normal sentence\n",
    "* Preprocess it via tokenization\n",
    "* Then, tag the sentence for parts of speech\n",
    "    * This will add tags for proper nouns, pronouns, adjectives, verbs, and other parts of speech that NLTK uses based on English grammar.\n",
    "\n",
    "```\n",
    "import nltk\n",
    "sentence = '''In New York, I like to rie the Metro to visit MOMA and some restuarants rated well by Ruth Reichl.'''\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "tagged_sent[:3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af238a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9dbedb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''In New York, I like to rie the Metro to visit MOMA and some restuarants rated well by Ruth Reichl.'''\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "tagged_sent[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf39f0",
   "metadata": {},
   "source": [
    "* When we take a look at the tags, we see `New` and `York` are tagged `NNP` which is the tag for a proper noun, singular\n",
    "* Then we pass the tagged sentence (`tagged_sent`) into the **`nltk.ne_chunk()`** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24a8326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  rie/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restuarants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e7b3f",
   "metadata": {},
   "source": [
    "* `ne_chunk` = \"named entity chunk\"\n",
    "* `ne_chunk` will return the sentence as a tree\n",
    "* Though NLTK trees may look a bit different than trees from other libraries, they do still have leaves and subtrees representing more complex grammar\n",
    "* `GPE` - Geopolitical Entity\n",
    "* NTLK classifies each of these words *without consulting a knowledge base* like Wikipedia; instead, it uses *trained statistical and grammatical parsers.*\n",
    "\n",
    "#### Exercises: \n",
    "\n",
    "```\n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n",
    "```            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd534e97",
   "metadata": {},
   "source": [
    "#### Exercise: Charting practice\n",
    "\n",
    "```\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdf75e",
   "metadata": {},
   "source": [
    "### Introduction to SpaCy\n",
    "* **SpaCy** is another great library for NLP\n",
    "\n",
    "#### What is SpaCy?\n",
    "* SpaCy is an NLP library similar to gensim, but with different implementations\n",
    "* Focus on creating NLP pipelines to generate models and corpora\n",
    "* SpaCy is open-source, with extra libraries and tools, including:\n",
    "    * **Displacy**: A visualization tool for viewing parse trees with uses Node-js to create interactive text\n",
    "    * tools to build word and document vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5723d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4199735",
   "metadata": {},
   "source": [
    "* The object `nlp` functions similarly to our gensim dictionary and corpus.\n",
    "* It has several linked objects, including entity , which is an Entity Recognition object from the pipeline module; this is what's used to find entities in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ad5c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.pipes.EntityRecognizer at 0x7fdb4952a3a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31423b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Berlin, Germany, Angela Merkel)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel\"\"\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244f3ca",
   "metadata": {},
   "source": [
    "* When the document (above) is loaded, the named entities are stored as a document attribute called `ents`\n",
    "* We see SpaCy has properly tagged and identified the three main entities in the sentence\n",
    "* We can also investigate the labels of each entity by using the indexing to pick out the first entity and the `label_` attribute to see the label for that particular entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3086698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin GPE\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents[0], doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e643c",
   "metadata": {},
   "source": [
    "* SpaCy has several other language models available, including advanced German and Chinese implementations\n",
    "* It's a great tool especially if you want to build your own extraction and natural language processing pipeline quickly and iteratively.\n",
    "\n",
    "#### Why use SpaCy for NER?\n",
    "* Ability to integrate with the other great SpaCy features\n",
    "* Easy pipeline creation\n",
    "* Different entity types compared to `nltk` (and often labels entities differently than `nltk`)\n",
    "* SpaCy also comes with informal language corpora\n",
    "    * Easily find entities in Tweets and chat messages\n",
    "* Quickly growing! May have more by now!\n",
    "* Some of the *extra* categories that `spacy` uses compared to `nltk` in its NER are:\n",
    "    * NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT\n",
    "\n",
    "#### Exercises: Comparing NLTK with spaCy NER\n",
    "\n",
    "```\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192adcbc",
   "metadata": {},
   "source": [
    "### Multilingual NER with polyglot\n",
    "\n",
    "#### What is polyglot?\n",
    "* Polyglot is yet another NLP library \n",
    "* Uses word vectors to perform simple tasks such as entity recognition\n",
    "* Why `polyglot`?\n",
    "    * The main benefit and diffence of using Polyglot (over `Gensim` or `SpaCy`) is **the wide variety of languages it supports**.\n",
    "    * **More han 130 languages!**\n",
    "    * For this reason you can even use it for tasks like **transliteration**\n",
    "        * [Translation vs Transliteration](https://www.familytreemagazine.com/strategies/now-what-translation-vs-transliteration/#:~:text=A%3A%20A%20translation%20tells%20you,sounding%20characters%20of%20another%20alphabet.)\n",
    "        * **Transliteration** is the ability to translate text by swapping characters from one language to another \n",
    "#### Transliteration from English to Arabic\n",
    "\n",
    "<img src='data/transliteration.png' width=\"200\" height=\"100\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89075bc",
   "metadata": {},
   "source": [
    "* There are usually many issues with translation created by word vectors, but `Polyglot` is a pretty neat open-source tool to have for so many languages\n",
    "\n",
    "#### Spanish NER with polyglot\n",
    "* Similar to SpaCy, you need to have the proper vectors downloaded and installed before you begin\n",
    "    * Once you do, Polyglot does not need to be told which language you are using\n",
    "    * It uses the language detection model to determine the language when the Text object is initialized by passing in the document string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e5694bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"El presidente de la Generalitat de CataluÃ±a, Carles Puigdemont, ha afirmado hot a la alcaldesa de Madrid, Manuela Carmena, que en su etapa de alcalde de Girona (de julio de 2011 a enero de 2016) hizo una gran promocion de Madrid.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3de9755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptext = Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c541602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package embeddings2.ca to\r\n",
      "[polyglot_data]     /Users/abigailmorgan/polyglot_data...\r\n"
     ]
    }
   ],
   "source": [
    "#!polyglot download embeddings2.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5172bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package ner2.ca to\r\n",
      "[polyglot_data]     /Users/abigailmorgan/polyglot_data...\r\n"
     ]
    }
   ],
   "source": [
    "#!polyglot download ner2.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f700454d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I-ORG(['Generalitat']),\n",
       " I-ORG(['CataluÃ±a']),\n",
       " I-PER(['Carles', 'Puigdemont']),\n",
       " I-LOC(['Madrid']),\n",
       " I-PER(['Manuela', 'Carmena']),\n",
       " I-LOC(['Girona']),\n",
       " I-LOC(['enero']),\n",
       " I-LOC(['Madrid'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptext.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297efadc",
   "metadata": {},
   "source": [
    "#### Exercises: French NER with polyglot I\n",
    "\n",
    "```\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))\n",
    "```\n",
    "\n",
    "#### Exercises: French NER with polyglot II\n",
    "\n",
    "```\n",
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)\n",
    "```\n",
    "\n",
    "#### Exercises: Spanish NER with polyglot\n",
    "\n",
    "```\n",
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'MÃ¡rquez' or 'Gabo'\n",
    "    if \"MÃ¡rquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c909ad",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 4: Building a \"fake news\" classifier\n",
    "You'll apply the basics of what you've learned along with some supervised machine learning to build a \"fake news\" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify fake news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e95c82",
   "metadata": {},
   "source": [
    "### Classifying fake news using supervised learning with NLP\n",
    "* Here we'll be learning about supervsed ML with NLP\n",
    "\n",
    "#### Supervised learning with NLP\n",
    "* `scikit-learn`: Powerful open-source library\n",
    "* How to create supervised learning data from text?\n",
    "    * Use bag-of-words models or TFIDF as features\n",
    "    \n",
    "#### Supervised learning steps\n",
    "* Collect and preprocess our data\n",
    "* Determine a label (example: movie genre)\n",
    "* Split data into training and test sets\n",
    "* Extract features from the text to help predict the label\n",
    "    * bag-of-words vector built into `scikit-learn`\n",
    "* Evaluate trained model using the test set\n",
    "\n",
    "### Building word count vectors with scikit-learn\n",
    "\n",
    "#### Count Vectorizer with Python\n",
    "* `CountVectorizer` turns text into a bag of words vectors similar to a Gensim corprs, it will also remove English stop words as a preprocessing step\n",
    "* Each token now acts as a feature for the ML classification problem\n",
    "* `fit_transform` generates a mapping of words with IDs and vectors representing how many times each word appears in the plot\n",
    "* For the `CountVectorizer` class, `fit_transform` will create the bag of words dictionary and vectors for each document using the training data\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = ... # Load data into DataFrame\n",
    "y = df['Sci-Fi']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['plot'],\n",
    "                                                    y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    random_state=53)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "```\n",
    "\n",
    "* **Note:** After calling `fit_transform` on the training data, we call `transform` on the test data to create bag of words vectors using the same dictionary\n",
    "* **The training and test vectors need to use a consistent set of words, so that the trained model can understand the test input.**\n",
    "* **If we don't have much data, there can be an issue with words in the test set which don't appear in the training data.**\n",
    "    * $\\Rightarrow$ This will throw an error, so you will need to either add more training data or remove the unkown words from the test dataset\n",
    "    \n",
    "```\n",
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "```\n",
    "\n",
    "#### Exercises: TfidfVectorizer for text classification\n",
    "\n",
    "```\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n",
    "```\n",
    "\n",
    "#### Exercises: Inspecting the vectors\n",
    "\n",
    "```\n",
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b9a62",
   "metadata": {},
   "source": [
    "### Training and testing a classification model with scikit-learn\n",
    "\n",
    "#### Naive Bayes classifier\n",
    "* Naive Bayes Model\n",
    "    * Commonly used for testing NLP classification problems\n",
    "    * Basis in probability\n",
    "    * Attempts to answer the question: \"Given a particular piece of data, how likely is a particular outcome?\"\n",
    "        * Examples: If the plot has a spaceship, how likely is it to be sci-fi?\n",
    "        * Given a spaceship *and* an alien, how likely *now* is it sci-fi?\n",
    "    * Each word from `CountVectorizer` acts as a feature\n",
    "* Naive Bayes is not always the best tool for the job, but it is a simple and effective one that has stood the test of time (since 1960s)\n",
    "* **Multinomial Naive Bayes** works well with count vectorizers as it expects integer inputs\n",
    "* **Multinomial Naive Bayes** is also used for multiple label classification\n",
    "    * This model may not work as well with floats, such as tfidf weighted inputs.\n",
    "    * Instead, for floats use SVMs or even linear models, but you can also try NB first to determine if it can also work well.\n",
    "* **Pass training count vectorizer first and the training labels second.**\n",
    "\n",
    "```\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import mettrics\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)\n",
    "```\n",
    "\n",
    "#### Confusion matrix\n",
    "* The `confusion_matrix` function, from the `metrics` module, takes the test labels, the predictions, and a list of labels\n",
    "* If a label list is not passed, skklearn will order them using Python ordering.\n",
    "\n",
    "```\n",
    "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "```\n",
    "\n",
    "#### Exercises: Training and testing the \"fake news\" model with CountVectorizer\n",
    "\n",
    "```\n",
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', \"REAL\"])\n",
    "print(cm)\n",
    "```\n",
    "\n",
    "#### Exercises: Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "\n",
    "```\n",
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e62df4",
   "metadata": {},
   "source": [
    "### Simple NLP, complex problems\n",
    "* How to use the skills you've learned to start a longer exploration of working with language in Python\n",
    "* Translation, although it might work well for some languages, still has a long way to go.\n",
    "\n",
    "### Sentiment analysis\n",
    "* Sentiment analysis is far from a solved problem \n",
    "\n",
    "<img src='data/sentiment_analysis1.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff033c7",
   "metadata": {},
   "source": [
    "* Complex issues like snark or **sarcasm** and **difficult problems with negation** (for example; \"I liked it BUT it could have been better\") make sentiment analysis an open field of research\n",
    "* **There is also active research regarding how separate communities use the same words differently.**\n",
    "* Above is a graphic from a project called \"Social Sent\" created by a group of researchers at Stanford.\n",
    "    * The project compares sentiment changes in words over time and from different communities\n",
    "    * Above, the project compares sentiment changes in words over time and from different reddit communities\n",
    "    * **The graphic illustrates the SAME word can be used with very different sentiments depending on the communal understandng of the word.**\n",
    "    \n",
    "#### Language biases\n",
    "* We must also remember that language can contain its own prejudices and unfair treatment towards groups \n",
    "\n",
    "<img src='data/lang_biases.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* **When we then train word vectors on these prejudiced texts, our word vectors will likely reflect those problems.**\n",
    "\n",
    "#### Exercises: Improving your model\n",
    "\n",
    "```\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "```\n",
    "\n",
    "#### Exercises: Inspecting your model \n",
    "\n",
    "```\n",
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1414b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
