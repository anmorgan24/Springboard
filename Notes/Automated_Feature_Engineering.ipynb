{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a82e25",
   "metadata": {},
   "source": [
    "# Automated Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332be753",
   "metadata": {},
   "source": [
    "* Often a predictive model's performance is limited by its features — you can tune the model for the best parameters and still not have the best performing model. \n",
    "* Identifying and engineering features that clearly demonstrate the predictive signal is paramount to model performance.\n",
    "* The single biggest technical hurdle that machine learning algorithms must overcome is their need for processed data in order to work — they can only make predictions from numeric data. \n",
    "* The process for extracting these numeric features is called “feature engineering.\n",
    "\n",
    "## Deep Feature Synthesis (DFS)\n",
    "* Developed at MIT in 2014\n",
    "* generates many of the same features that a human data scientist would create.\n",
    "\n",
    "* There are three key concepts in understanding Deep Feature Synthesis:\n",
    "\n",
    "* **1) Features are derived from relationships between the data points in a dataset.**\n",
    "    * DFS performs feature engineering for multi-table and transactional datasets commonly found in databases or log files.\n",
    "\n",
    "* **2) Across datasets, many features are derived by using similar mathematical operations.**\n",
    "    * Dataset-agnostic operations are called “primitives.”\n",
    "    \n",
    "* **3) New features are often composed from utilizing previously derived features.**\n",
    "    * Primitives are the building blocks of DFS. \n",
    "    * Because primitives define their input and output types, we can stack them to construct complex features that mimic the ones that humans create today.\n",
    "    * DFS can apply primitives across relationships between entities, so features can be created from datasets with many tables. \n",
    "    * We can control the complexity of the features we create by setting a maximum depth for our search.\n",
    "\n",
    "* A second advantage of primitives: they can be used to quickly enumerate many interesting features in a parameterized fashion\n",
    "    \n",
    "*  Since primitives are defined independently of a specific dataset, any new primitive added to Featuretools can be incorporated into any other dataset that contains the same variable data types. In some cases, this might be a dataset in the same domain, but it could also be for a completely different use case.\n",
    "\n",
    "* It’s easy to accidentally leak information about what you’re trying to predict into a model.\n",
    "* DFS can be used to develop baseline models with little human intervention.\n",
    "* the automation of feature engineering should be thought of as a complement to critical human expertise — it enables data scientists to be more precise and productive.\n",
    "\n",
    "* Deep Feature Synthesis vs. Deep Learning\n",
    "* Deep Learning automates feature engineering for images, text, and audio where a large training set is typically required, whereas DFS targets the structured transactional and relational datasets that companies work with.\n",
    "* The features that DFS generates are more explainable to humans because they are based on combinations of primitives that are easily described in natural language. \n",
    "* The transformations in deep learning must be possible through matrix multiplication, while the primitives in DFS can be mapped to any function that a domain expert can describe.\n",
    "* This increases the accessibility of the technology and offers more opportunities for those who are not experienced machine learning professionals to contribute their own expertise.\n",
    "* Additionally, while deep learning often requires many training examples to train the complex architectures it needs to work, DFS can start creating potential features based only on the schema of a dataset.\n",
    "* For many enterprise use cases, enough training examples for deep learning are not available.\n",
    "* DFS offers a way to begin creating interpretable features for smaller datasets that humans can manually validate.\n",
    "* Automating feature engineering offers the potential to accelerate the process of applying machine learning to the valuable datasets collected by data science teams today. \n",
    "* It will help data scientists to quickly address new problems as they arise and, more importantly, make it easier for those new to data science to develop the skills necessary to apply their own domain expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e343b",
   "metadata": {},
   "source": [
    "# Automated Feature Engineering pt 2\n",
    "\n",
    "* Data is like the crude oil of machine learning which means it has to be refined into features — predictor variables — to be useful for training a model. Without relevant features, you can’t train an accurate model, no matter how complex the machine learning algorithm. The process of extracting features from a raw dataset is called feature engineering.\n",
    "* Feature engineering means building features for each label while filtering the data used for the feature based on the label’s cutoff time to make valid features. These features and labels are then passed to modeling where they will be used for training a machine learning algorithm.\n",
    "* While feature engineering requires label times, in our general-purpose framework, it is not hard-coded for specific labels corresponding to only one prediction problem.\n",
    "* Instead, we use APIs like Featuretools that can build features for any set of labels without requiring changes to the code.\n",
    "* This fits with the principles of our machine learning approach: we segment each step of the pipeline while standardizing inputs and outputs. This independence means we can change the problem in prediction engineering without needing to alter the downstream feature engineering and machine learning code.\n",
    "* The key to making this step of the machine learning process repeatable across prediction problems is automated feature engineering.\n",
    "* Traditionally, feature engineering is done by hand, building features one at a time using domain knowledge. However, this manual process is error-prone, tedious, must be started from scratch for each dataset, and ultimately is limited by constraints on human creativity and time. Furthermore, in time-dependent problems where we have to filter every feature based on a cutoff time, it’s hard to avoid errors that can invalidate an entire machine learning solution.\n",
    "* After solving a few problems with machine learning, it becomes clear that many of the operations used to build features are repeated across datasets.\n",
    "* We can apply the same basic building blocks — called feature primitives — to different relational datasets to build predictor variables.\n",
    "* Ultimately, automated feature engineering makes us more efficient as data scientists by removing the need to repeat tedious operations across problems.\n",
    "* Currently, the only open-source Python library for automated feature engineering using multiple tables is Featuretools, developed and maintained by Feature Labs. \n",
    "* Featuretools requires some background code to link together the tables through relationships, but then we can automatically make features for customer churn using the following code (see notebook for complete details):\n",
    "\n",
    "```\n",
    "import featuretools as ft\n",
    "\n",
    "# Primitives for deep feature synthesis\n",
    "trans_primitives = ['weekend', 'cum_sum', 'day', 'month', 'diff', 'time_since_previous']\n",
    "agg_primitives = ['sum', 'time_since_last', 'avg_time_between', 'all', 'mode', \n",
    "                  'num_unique', 'min', 'last', 'mean', 'percent_true', \n",
    "                  'max', 'std', 'count']\n",
    "\n",
    "# Perform deep feature synthesis \n",
    "feature_matrix, feature_names = ft.dfs(entityset=es, \n",
    "                                       trans_primitives = trans_primitives,\n",
    "                                       agg_primitives = agg_primitives,\n",
    "                                       target_entity='customers',\n",
    "                                       cutoff_times=cutoff_times)\n",
    "```\n",
    "* This one line of code gives us over 200 features for each label in cutoff_times. Each feature is a combination of feature primitives and is built with only data from before the associated cutoff time.\n",
    "\n",
    "* To solve a different problem, rather than rewrite the entire pipeline, we:\n",
    "* 1) Tweak the prediction engineering code to create new label times\n",
    "* 2) Input the label times to feature engineering and output features\n",
    "* 3) Use the features to train and a supervised machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68b1e9",
   "metadata": {},
   "source": [
    "# Case Study Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20aec66",
   "metadata": {},
   "source": [
    "* The first step in using Featuretools is to make an EntitySet and add all the entitys - tables - to it. An EntitySet is a data structure that holds the tables and the relationships between them. This makes it easier to keep track of all the data in a problem with multiple relational tables.\n",
    "\n",
    "## Entities\n",
    "\n",
    "When creating entities from a dataframe, we need to make sure to include:\n",
    "\n",
    "* The index if there is one or a name for the created index. This is a unique identifier for each observation.\n",
    "* make_index = True if there is no index, we need to supply a name under index and set this to True.\n",
    "* A time_index if present. This is the time at which the information in the row becomes known. Featuretools will use the time_index and the cutoff_time to make valid features for each label.\n",
    "variable_types. In some cases our data will have variables for which we should specify the type. An example would be a boolean that is represented as a float. This prevents Featuretools from making features such as the min or max of a True/False varaibles.\n",
    "* For this problem these are the only arguments we'll need. There are additional arguments that can be used as shown in the documentation.\n",
    "\n",
    "* primitives are data agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c2aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad821ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2d2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd076269",
   "metadata": {},
   "source": [
    "# you're one project away from your next job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
