{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03c4017",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509c480",
   "metadata": {},
   "source": [
    "# Remember to scale data in COV_weather project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368db22",
   "metadata": {},
   "source": [
    "#### Categorical, Text, and Image Features\n",
    "* Data scientists regularly work with categorical, text, and image data. However, to execute machine learning algorithms on these data types, it's necessary to perform transformations first. \n",
    "* Categorical data, such as the neighborhood in which a property is located, does not always work well with the machine learning algorithm you're most interested in using. \n",
    "* Linear regression, for example, requires numerical inputs.\n",
    "* Options include one-hot encoding of categorical data and text and image data feature engineering (important for processes like NLP, which has applications in social media and data mining).\n",
    "* Featuer engineering with images can be very complex: the simplest of which is just using the pixel values themselves\n",
    "* HOG: Histogram of Oriented Gradients\n",
    "   \n",
    "Feature Engineering: understand how best to preprocess and engineer features from categorical, continuous, and unstructured data. \n",
    "\n",
    "* **Feature Engineering:** the act of taking raw data and extracting features for machine learning \n",
    "* Most machine learning algorithms work with tabular data.\n",
    "* Most ML algorithms require their imput data to be represented as a vector or a matrix and many assume that the data is distributed normally\n",
    "\n",
    "* **Different Types of Data:**\n",
    "    * **Continuous:** either integers (whole numbers) or floats (decimal values)\n",
    "    * **Categorical:** one of a limited set of values, e.g. gender, country of birth\n",
    "    * **Ordinal:** ranked values, often with no detail of distance between them\n",
    "    * **Boolean:** True/False values\n",
    "    * **Datetime:** dates and times\n",
    "* in pandas, \"objects\" are columns that contain strings\n",
    "* knowing the types of each column can be very useful if you are performing analysis based on a subset of specific data types. To do this, use: `.select_dtypes()` method and pass a list of relevant data types: `only_ints = df.select_dtypes(include=['int'])`\n",
    "\n",
    "#### Categorical Variables\n",
    "* Categorical variables are used to represent groups that are qualitative in nature, like colors, country of birth\n",
    "* You will need to encode categorical values as numeric values to use them in your machine learning models \n",
    "* When categories are unordered (like colors, country of birth), assigned ordered numerical values to them may greatly penalize the effectiveness of your model.\n",
    "* Thus, you cannot allocate arbitrary numbers to each category, as that would imply some form of ordering to the categories\n",
    "* $\\Rightarrow$ **One Hot Encoding**\n",
    "* $\\Rightarrow$ **Dummy Encoding**\n",
    "    * Very similar, and often confused\n",
    "    * by default, pandas performs one hot encoding when you use the get_dummies() function\n",
    "    * difference:\n",
    "        * **One Hot Encoding:** converts *n* categories into *n* features\n",
    "            * `pd.get_dummies(df, columns=['Country'], prefix ='C')`\n",
    "            * note that specifying a prefix argument can improve readability, especially if the list of column names passed to `columns` contains more than one column.\n",
    "            * **Use for: generally creating more explainable features**\n",
    "            * **Note: one must be aware that one-hot encoding may create features that are entirely colinear due to the same information being represented multiple times. \n",
    "        * **Dummy Encoding:** creates *n* - 1 features for *n* categories\n",
    "            * `pd.get_dummies(df, columns=['Coutnry'], drop_first=True, prefix = 'C')`\n",
    "            * the dropped column (referred to as the *base column* is encoded by the absence of all other features and it's value is represented by the intercept\n",
    "            * **Use for: Necessary information without duplication.**\n",
    "            \n",
    "        * Both one-hot encoding and dummy encoding may result in a **huge** number of columns being created if there are too many different categories in a column \n",
    "        * In these cases, you may only want to create columns for the most common values:\n",
    "            * `counts = df['Country'].value_counts()` # to check occurences of a category value \n",
    "            * once you have your counts of column category occurences, you can use it to limit what values you will include by first creating a mask of values that occur less than *n* times:\n",
    "            * `mask = df['Country'].isin(counts[counts<5].index)`\n",
    "            * use the mask to replace these categories that occur less frequently with a value of your choice (for example: an umbrella category like 'Other')\n",
    "            * `df['Country'][mask] = 'Other'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223aae0",
   "metadata": {},
   "source": [
    "#### Numeric variables\n",
    "* Even if your raw data is all numeric, there is still a lot you can do to improve your features \n",
    "* Types of numeric features:\n",
    "    * Age\n",
    "    * Price\n",
    "    * Counts\n",
    "    * Geospacial data (such as coordinates) \n",
    "* A few of the considerations and possible feature engineering steps to keep in mind when dealing with numeric data:\n",
    "\n",
    "* **Is the magnitude of the feature its most important trait, or just its direction?**\n",
    "    * Can you turn numeric values (for example, number of restaurant health code violations) into binary values (has restaurant ever violated a health code before? yes/no)\n",
    "    * **Binarizing numeric data:**\n",
    "    * #Create new column: Binary Violation\n",
    "    * `df['Binary_Violation'] = 0`\n",
    "    * `df.loc[df['Number_of_Violations'] > 0, 'Binary_Violation'] = 1`\n",
    "    \n",
    "    * **Binning numeric variables:** \n",
    "    * Similar to binarizing, but using more than just 2 bins\n",
    "    * Often useful for variables such as age brackets, income brackets, etc where exact numbers are less relevant than general magnitude of the value \n",
    "    * `df['Binned_Group'] = pd.cut(df['Number_of_Violations'], bins=[-np.inf, 0, 2, np.inf], labels =[1,2,3])`\n",
    "    * note in above code: `bin` arguments represent cut-off points; so, for 3 bins, 4 values are needed\n",
    "    * bins created using `pd.cut()`\n",
    "    * **Note:** A new column can be created using `df[column_name] = default_value`\n",
    "\n",
    "```\n",
    "# Create the Paid_Job column filled with zeros\n",
    "so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
    "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\n",
    "```\n",
    "    \n",
    "* Bins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\n",
    "\n",
    "```\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Specify the boundaries of the bins\n",
    "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary using these boundaries\n",
    "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], \n",
    "                                         bins=bins, labels=labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61f807",
   "metadata": {},
   "source": [
    "## Dealing with messy data and missing values \n",
    "\n",
    "* Real world data often has noise and ommissions. \n",
    "* Many machine learning models cannot work with missing values \n",
    "    * for example, if you are performing linear regression, you need a value for every row and column used in your dataset\n",
    "* Missing data may also be a sign of a wider data issue\n",
    "* Missing data may also provide data in and of itself and can sometimes be a useful feature \n",
    "* Use `.info()` method to have a preliminary look at how complete the data set is\n",
    "* To find where these missing values exist, `.isnull()`\n",
    "* Find non-missing values with `.notnull()`\n",
    "\n",
    "#### Dealing with missing values \n",
    "* If you are confident that the missing values in your dataset are occurring at random (and not being intentionally omitted), the most effective and statistically sound approach to dealing with them is called \"complete case analysis\" or \"listwise deletion\" \n",
    "    * In this method, a record is fully excluded from your model if any of its values are missing\n",
    "    * **List-wise deletion in Python:**\n",
    "    * To delete *all* rows with at least one NaN:\n",
    "        * `df.dropna(how = any)`\n",
    "    * On the other hand, if you want to delete rows with missing values in only a specific column, use `subset` argument:\n",
    "        * `df.dropna(subset=['VersionControl'])`\n",
    "        * pass a *list* of arguments/columns\n",
    "        \n",
    "* Drawbacks to list-wise deletion:\n",
    "    * It deletes valid data points \n",
    "    * It relies on randomness\n",
    "    * It reduces information (specifically, degrees of freedom)\n",
    "\n",
    "* The most common way to deal with missing values is to replace them:\n",
    "    * **`.fillna()`**: `df['Version_Control'].fillna(value='None Given', inplace=True)`\n",
    "    * To use the fillna() method on a specific column, you need to provide the value you want to replace the missing values with\n",
    "    * In the case of categorical columns, it is common to replace missing values with strings like \"Other\", \"Not Given\", etc.\n",
    "    \n",
    "* In situations where you believe that the absence or presence of data is more important than the values themselves, you can create a new column that records the absence of data (and then drop the original column). \n",
    "\n",
    "```\n",
    "df['SalaryGiven'] = df['Converted Salary'].notnull()\n",
    "df.drop(columns=['ConvertedSalary'])\n",
    "```\n",
    "\n",
    "```\n",
    "# Drop all rows where Gender (column) value is missing\n",
    "no_gender = so_survey_df.dropna(subset = ['Gender'], axis=0)\n",
    "```\n",
    "\n",
    "#### Fill continuous missing values\n",
    "* one of the major issues with list-wise deletion is apparent when you are building a predictive model: you can't delete rows with missing values in the test set.\n",
    "* **Replacing missing values:** \n",
    "    * **Categorical columns:** replace missing values with the most common occurring value or a string that flags missimg values such as 'None.'\n",
    "    * **Numeric columns:** replace missing values with a \"suitable value\"\n",
    "        * A \"suitable value\" would be a measure of central tendency like mean or median.\n",
    "        * However, remember: this can lead to biased estimates of the variances and covariances of the features \n",
    "        * Similarly, the standard error and test statistics can be incorrectly estimated: So, if these metrics are needed, they should be calculated before the missing values have been filled\n",
    "        * directly fill missing values:\n",
    "            * `df['ConvertedSalary']=df['ConvertedSalary'].fillna(df['ConvertedSalary'].mean())`\n",
    "\n",
    "#### Dealing with other data issues:\n",
    "* **Dealing with bad characters:**\n",
    "    * `df['RawSalary'] = df['RawSalary'].str.replace(',','')`\n",
    "    * `df['RawSalary'] = df['RawSalary'].astype('float')`\n",
    "    * If attempting to change the data type results in an error, this may indicate that there are additional stray characters which you didn't account for.\n",
    "    * Instead of manually searching for values with other stray characters, use the `pd.to_numeric()` function:\n",
    "        * `pd.tonumeric(errors=coerce)`: pandas will convert the column to numeric, but all values that can't be converted to numeric be changed to NaNs\n",
    "        * You can now use `isna()` as follows:\n",
    "            * `print(df[coreced_vals.isna()].head())`\n",
    "* **Chaining methods:**\n",
    "`so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',','').str.replace('dolsign','').str.replace('£', '').astype('float')`\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57de19",
   "metadata": {},
   "source": [
    "#### Data distributions\n",
    "* An important consideration before building a machine learning model is to understand what the distribution of your underlying data looks like.\n",
    "* A lot of algorithms make assumptions about how your data is distributed or how different features interact with each other \n",
    "* For example, almost all models, besides tree-based models, require your features to be on the same scale\n",
    "* Feature engineering can be used to manipulate your data so that it can fit the assumptions of the distribution or at least fit it as closely as possible \n",
    "* Almost every model (besides tree-based models) assume that your data is normally distributed\n",
    "    * 68% of the data lies within 1 standard deviation of the mean\n",
    "    * 95% lies within 2 standard deviations of the mean\n",
    "    * 99.7% of the data lies within 3 standard deviations of the mean\n",
    "* To understand the shape of your own data, you can create histograms of each of the continuous features: `df.hist()`\n",
    "* To create boxplots: `df[['column1']].boxplot()`\n",
    "* Pairing distributions: `sns.pairplot(df)`\n",
    "* Summary statistics: `df.describe()`\n",
    "\n",
    "#### Scalar and transformations\n",
    "* Most ML models and algorithms require your data to be on the same scale for them to be effective\n",
    "* There are many different approaches to scaling data, but the most popular are: **Standardization** and **Min-Max Scaling** (aka **Normalization**)\n",
    "\n",
    "* **Min-Max Scaling** is when your data is scaled linearly between a minimum and maximum value, often 0 and 1, with 0 corresponding with the lowest value in the column and 1 corresponding with the highest value in the column\n",
    "    * As it is a linear scaling, while the values will change, the distribution will not\n",
    "    * To implement min-max scaling in Python:\n",
    "    \n",
    "```\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[['Age']])\n",
    "df['normalized_age'] = scaler.transform(df[['Age']])\n",
    "```\n",
    "* **Note:** Since this scaler assumes the max value of your data to be the upper boundary of values, new data may produce unforeseen results.\n",
    "\n",
    "* **Standardization:** As opposed to finding an outer boundary and squeezing everything within it, Standardization instead finds the mean of your data and centers your distribution around it, calculating the number of stds away from the mean each point is. These values (the number of standard deviations) are then used as your new values. This centers your data around zero but technically has no limit to the maximum and minimum values \n",
    "    * Standardization in Python:\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[['Age']])\n",
    "df['standardized_col'] = scaler.transform(df[['Age']])\n",
    "```\n",
    "\n",
    "* Both normalizaton and standardization are types of scalers (ie the data remained in the same shape)\n",
    "* A **log transformation**, on the other hand, can be used to make highly skewed distributions less skewed (for example if your data has a long tail)\n",
    "    * Log transformation in Python:\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "log = PowerTransformer()\n",
    "log.fit(df[['ConvertedSalary']])\n",
    "df['log_ConvertedSalary'] = log.transform(df[['ConvertedSalary']])\n",
    "```\n",
    "* Also: **PowerTransformer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82f704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e4dd234",
   "metadata": {},
   "source": [
    "## Text mining in Python\n",
    "* Text Mining is the process of deriving meaningful information from natural language text.\n",
    "* The overall goal is to turn the texts into data for analysis, via application of Natural Language Processing.\n",
    "\n",
    "## Natural Language Processing (NLP)\n",
    "* NLP is a part of computer science and artificial intelligence which deals with human languages\n",
    "* In other words, NLP is a coponent of text mining that performs a special kind of linguistic analysis that essentially helps a machine \"read\" text.\n",
    "* It uses a different methodology to decipher the ambiguities in human language, including:\n",
    "    * automatic summarization\n",
    "    * part-of-speech tagging\n",
    "    * disambiguation\n",
    "    * chunking\n",
    "    * natural language understanding and recognition\n",
    "**First, we need to install the NLTK library that is the natural language toolkit for building Python programs to work with human language data**\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* **Tokenization:**\n",
    "    * the first step in NLP\n",
    "    * it is the process of breaking strings into tokens which in turn are small structures or units.\n",
    "    * involves three steps:\n",
    "        * 1) breaking a complex sentence into words\n",
    "        * 2) understanding the importance of each word with respect to the sentence\n",
    "        * 3) produce structural description on an input sentence\n",
    "* Example input:\n",
    "\n",
    "```\n",
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "# sample text for performing tokenization\n",
    "text = “In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern\n",
    "side of South America\"\n",
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "token\n",
    "```\n",
    "* Output:\n",
    "\n",
    "```\n",
    "['In','Brazil','they','drive', 'on','the', 'right-hand', 'side', 'of', 'the', 'road', '.', 'Brazil', 'has', 'a', 'large', 'coastline', 'on', 'the', 'eastern', 'side', 'of', 'South', 'America']\n",
    "```\n",
    "\n",
    "#### Finding frequency of distinct tokens in the text\n",
    "* **2** methods:\n",
    "\n",
    "* Example input, **method 1**:\n",
    "\n",
    "```\n",
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist\n",
    "```\n",
    "* Output: `FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})`\n",
    "\n",
    "* Example input, **method 2**:\n",
    "\n",
    "```\n",
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1\n",
    "```\n",
    "* Output:\n",
    "\n",
    "```\n",
    "[('the', 3),\n",
    " ('Brazil', 2),\n",
    " ('on', 2),\n",
    " ('side', 2),\n",
    " ('of', 2),\n",
    " ('In', 1),\n",
    " ('they', 1),\n",
    " ('drive', 1),\n",
    " ('right-hand', 1),\n",
    " ('road', 1)]\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b0de1",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "* Stemming usually refers to normalizing words into its base form or root form.\n",
    "* For example: **waiting**, **waited**, **waits** $\\Rightarrow$ **wait**\n",
    "* There are two methods in stemming, namely, **1) Porter Stemming** (removes common morphological and inflectional endings from words) and **2) Lancaster Stemming** (a more aggressive stemming algorithm).\n",
    "\n",
    "* **method 1:**\n",
    "\n",
    "```\n",
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(“waiting”)\n",
    "```\n",
    "* output: `wait`\n",
    "\n",
    "* **method 2:**\n",
    "\n",
    "```\n",
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word+ \":\" +pst.stem(word))\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "waited:wait\n",
    "waiting:wait\n",
    "waits:wait\n",
    "```\n",
    "* **method 3:**\n",
    "\n",
    "```\n",
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [“giving”, “given”, “given”, “gave”]\n",
    "for word in stm :\n",
    " print(word+ “:” +lst.stem(word))\n",
    "``` \n",
    "* output:\n",
    "\n",
    "```\n",
    "giving:giv\n",
    "given:giv\n",
    "given:giv\n",
    "gave:gav\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab20694",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "* groups together different inflected forms of a word, called Lemma\n",
    "* SOmehow similar to Stemming, as it maps several words into one common root\n",
    "* Output of Lemmatization is a proper word\n",
    "* For example, a Lemmatize should map 'gone', 'going', and 'went' into 'go'\n",
    "\n",
    "* **Lemmatization**, in simpler terms is the process of converting a word to it's base form. \n",
    "* the difference between stemming and lemmatization is that lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "* For example, lemmatization would correctly identify the base form of 'caring' to 'care', whereas stemming would cutoff the 'ing' part and convert it to 'car'\n",
    "* Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP\n",
    "\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(“rocks :”, lemmatizer.lemmatize(“rocks”)) \n",
    "print(“corpora :”, lemmatizer.lemmatize(“corpora”))\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "rocks : rock\n",
    "corpora : corpus\n",
    "```\n",
    "\n",
    "#### Stop words\n",
    "* “Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library\n",
    "\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "# importing stopwors from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words(‘english’))\n",
    "text = “Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.”\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
    "Output of stopwords:\n",
    "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
    "```\n",
    "\n",
    "#### Part of speech tagging (POS)\n",
    "* Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection)\n",
    "*  There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "text = “vote to choose a particular man or a group (party) to represent them in parliament”\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "print(nltk.pos_tag([token]))\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "[('vote', 'NN')]\n",
    "[('to', 'TO')]\n",
    "[('choose', 'NN')]\n",
    "[('a', 'DT')]\n",
    "[('particular', 'JJ')]\n",
    "[('man', 'NN')]\n",
    "[('or', 'CC')]\n",
    "[('a', 'DT')]\n",
    "[('group', 'NN')]\n",
    "[('(', '(')]\n",
    "[('party', 'NN')]\n",
    "[(')', ')')]\n",
    "[('to', 'TO')]\n",
    "[('represent', 'NN')]\n",
    "[('them', 'PRP')]\n",
    "[('in', 'IN')]\n",
    "[('parliament', 'NN')]\n",
    "```\n",
    "\n",
    "#### Named Entity Recognition\n",
    "* is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value.\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "text = “Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event”\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "Tree('S', [Tree('GPE', [('Google', 'NNP')]), (\"'s\", 'POS'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])\n",
    "```\n",
    "\n",
    "#### Chunking\n",
    "* Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "text = “We saw the yellow dog”\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = “NP: {<DT>?<JJ>*<NN>}” \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)\n",
    "```\n",
    "* output:\n",
    "`(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9766",
   "metadata": {},
   "source": [
    "#### Encoding Text\n",
    "* When you are faced with text data, it will often **not** be tabular\n",
    "* Data that is not in a predefined form is called **unstructured data**\n",
    "    * One example of unstructured data is: **free text**\n",
    "* Before you can leverage text data in an ML model, you must first transform it into a series of columns of numbers or vectors \n",
    "* There are many different approaches to achieving this (above): we'll go through some of the most common\n",
    "* Using Inaugural Speeches dataset:\n",
    "    * before any text analytics can be performed, you must ensure that the text data is in a format that can be used\n",
    "    * the body of the text of these speeches contained in 'text' column as single observation per speech\n",
    "    \n",
    "* **1)** Most bodies of text will have **non-letter characters**, such as punctuation, that**will need to be removed before analysis.**\n",
    "    * Use: `.replace()`\n",
    "    * regex:\n",
    "        * `[a-zA-Z]` : All letter characters\n",
    "        * `[^a-zA-Z]` : All non-letter characters\n",
    "    * `speech_df['text'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')`\n",
    " \n",
    "* **2)** Once you have removed all unwanted characters, you will then want to **standardize the remaining characters in your text so that they are all lowercase.**\n",
    "    * `speech_df['text'] = speech_df['text'].str.lower()`\n",
    "        \n",
    "* Often there is value in the fundamental characteristics of a text\n",
    "    * **Length of a text**\n",
    "        * `speech_df['char_cnt'] = speech_df['text'].str.len()`\n",
    "        * calculate the number of characters in each speech\n",
    "    * **Word counts**\n",
    "        * `speech_df['word_cnt'] = speech_df['text'].str.split().str.len()`\n",
    "    * **Average length of word**\n",
    "        * `speech_df['avg_word_len'] = speech_df['char_cnt'] / speech_df['word_cnt']`\n",
    "\n",
    "#### Word Count Representation\n",
    "* The most common approach to creating features from transformed & cleaned text data, is to create a column for each word and record the number of times each particular word appears in each text.\n",
    "* this results in a set of columns equal in width to the number of unique words in the dataset with counts filling each entry.\n",
    "* sklearn already has a built-in function for this with: `CountVectorizer()`\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "```\n",
    "* creating a column for every word will result in far too many values for analysis. Thankfully, you can specify arguments when initializing your CountVectorizer to limit this.\n",
    "* for example, you can specify the minimum number of texts that a word must be contained in: `min_df`\n",
    "    * if a float is given (for example `min_df = 0.1`) then the word must appear in at least this percent of documents\n",
    "    * this threshold eliminates words that occur so rarely that they would not be useful when generalizing to new texts\n",
    "    * conversely, `max_df` limits words to only ones that occur below a certain percentage of the data\n",
    "    * this can be useful to remove words that occur too frequently to be of any value \n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction import CountVectorizer\n",
    "cv = CountVectorizer(min_df = 0.1, max_df = 0.9)\n",
    "cv.fit(speech_df['text_clean'])\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "```\n",
    "* The above code outputs a *sparse array* with a row for each text and a column for every word\n",
    "* to transform this to a *non-sparse array*:\n",
    "`cv_transformed.toarray()`\n",
    "* output will be an array with no concept of column names\n",
    "* to get the names of the features (words) that have been generated, call:\n",
    "`feature_names = cv.get_feature_names()`\n",
    "* returns a list of the features (words) generated, in the same order that the columns of the converted array are in\n",
    "\n",
    "* **Note:** While fitting and transforming separately can be useful, particularly when you need to transform a different dataset than the one that you used to fit the vectorizer, you can accomplish both steps at once using the `.fit_transform()` method\n",
    "* **Putting it all together:**\n",
    "\n",
    "`cv_df = pd.DataFrame(cv_transformed.toarray(), columns =cv.get_feature_names())\\.add_prefix('Counts_')`\n",
    "\n",
    "* you can now combine this dataframe with your original dataframe so that they can be used to generate future analytical models \n",
    "\n",
    "`speech_df = pd.concat([speech_df, cv_df, axis=1, sort=False)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72ad94",
   "metadata": {},
   "source": [
    "#### Tf-Idf (Term Frequency- Inverse Document Frequency)\n",
    "* While counts of occurences of words can be a good first step towards encoding your text to build models, it has some limitations\n",
    "* The main issue, is counts will be much higher for very common words that occur across all texts, providing very little value as a distinguishing feature.\n",
    "* to limit common words like \"the\" from overpowering your model, some form of normalization can be used \n",
    "* one of the most effective approaches to do this is called: **Term Frequency- Inverse Document Frequency**, or **TF-IDF**\n",
    "\n",
    "\\begin{equation}\n",
    "TF-IDF =\n",
    "\\frac{\\frac{count-of-word-occurences}{total-words-in-document}}{log\\frac{number-of-docs-word-is-in}{total-number-of-docs}}\n",
    "\\end{equation}\n",
    "\n",
    "* the effect: reduces the value of common words, while increasing the weights of words that do not occur in many documents \n",
    "* to use the TF-IDF Vectorizer\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "```\n",
    "* silimarly to when you worked with the CountVectorizer, where you could limit the number of features created by specifying arguments \n",
    "* set `max_features` argument = 100, will only use the top 100 most common words\n",
    "* `stop_words` are a predefined list of the most common [insert language here] words, such as \"and\" and \"the\"\n",
    "    * you can use sklearn's built-in list, load your own, or use lists provided by other Python libraries\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tv.fit(train_speech_df['text'])\n",
    "train_tv_transformed = tv.transform(train_speech_df['text'])\n",
    "```\n",
    "* **Note** that here we are fitting and transforming the training data as a subset of the original data\n",
    "\n",
    "```\n",
    "train_tv_df = pd.DataFrame(train_tv_transformed.toarray(), columns= tv.get_feature_names()).add_prefix(TFIDF_')\n",
    "train_speech_df = pd.concat([train_speech_df, train_tv_df], axis = 1, sort = False)\n",
    "```\n",
    "* Inspect your transformation:\n",
    "\n",
    "```\n",
    "examine_row = train_tv_df.iloc[0]\n",
    "print(examine_row.sort_values(ascending=False)\n",
    "```\n",
    "* **Applying a vectorizer to new data (like the test set):**\n",
    "* **Preprocess** your test data using the transformations made on the train data **only**.\n",
    "\n",
    "```\n",
    "test_tv_transformed = tv.tranform(test_df['text_clean'])\n",
    "\n",
    "test_tv_diff = pd.DataFrame(test_tv_transformed.toarray(), columns=tv.get_feature_names()).add_prefix('TDIDF_')\n",
    "\n",
    "test_speech_df = pd.concat([test_speech_df, test_tv_df], axis = 1, sort=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6badb28",
   "metadata": {},
   "source": [
    "#### Bag of words and N-grams\n",
    "* So far we've looked at individual words on their own, without any context\n",
    "* This approach is called a **bag of words model**, as the words are being treated as if they are being drawn from a bag at random, with no concept of order or grammar\n",
    "* Individual words *can* lose all their context or meaning when viewed independently \n",
    "* Issues with bag of words:\n",
    "    * Positive meaning: happy\n",
    "    * Negative meaning: not happy\n",
    "    * Positive meandin: never not happy\n",
    "* $\\Uparrow$ different meanings of happy depending on context \n",
    "* One common method to retain at least some concept of word order in a text is to instead use multiple consecutive words, like pairs (**bi-grams**), or three consecutive words (**tri-grams**).\n",
    "* This maintains at least some ordering information, while at the same time allowing for the creation of a reasonable set of features\n",
    "* To leverage N-grams in your own models:\n",
    "* `ngram_range` parameter = values equal the minimum and maximum length of the ngrams to be included\n",
    "\n",
    "```\n",
    "tv_bi_gram_vec = TfidfVectorizer(ngram_range =(2,2))\n",
    "\n",
    "tv_bi_gram = tv_bi-gram_vec.fit_transform(speech_df['text'])\n",
    "```\n",
    "* Create a DataFrame with the Counts features \n",
    "\n",
    "```\n",
    "tv_df = pd.DataFrame(tv_bi_gram.toarray(), columns=tv_bi_gram_vec.get_feature_names().add_prefix('Counts')\n",
    "tv_sums = tv_df.sum()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884cfa8",
   "metadata": {},
   "source": [
    "#### Image Processing and scikit-image\n",
    "* image pre-processing has become a highly valuable skill, applicable in many use cases.\n",
    "* learn to process, transform, and manipulate images at your will, even when they come in thousands. \n",
    "* restore damaged images, perform noise reduction, smart-resize images, count the number of dots on a dice, apply facial detection, and much more, using scikit-image. \n",
    "* Extract data, transform and analyze images using NumPy and Scikit-image\n",
    "* With just a few lines of code, you will convert RGB images to grayscale, get data from them, obtain histograms containing very useful information, and separate objects from the background\n",
    "\n",
    "#### Scikit-image\n",
    "* **Image processing** is a method to perform operations on images and videos in order to:\n",
    "    * Enhance them\n",
    "    * Extract useful information\n",
    "    * Analyze it and make decisions\n",
    "    \n",
    "* By quantifying the information in images, we can make calculations\n",
    "* Image processing is a subset of computer vision\n",
    "* **Wide range of applications:**\n",
    "    * Medical image analysis\n",
    "    * Artificial intelligence\n",
    "    * Image restoration and enhancement \n",
    "    * Geospatial computing\n",
    "    * Surveillance\n",
    "    * Robotic vision\n",
    "    * Automotive safety\n",
    "    * And many more...\n",
    "* **Purposes** of image processing:\n",
    "    * 1) **Visualization:**\n",
    "        * Observe objects that are not visible\n",
    "    * 2) **Image sharpening and restoration:**\n",
    "        * Create a better image\n",
    "    * 3) **Image retrieval:**\n",
    "        * Seek an image of interest\n",
    "    * 4) **Measurement of pattern:**\n",
    "        * Measures various objects\n",
    "    * 5) **Image recognition:**\n",
    "        * Distinguish objects in an image.\n",
    "\n",
    "* **scikit-image** is an image-processing library in python:\n",
    "    * easy to use\n",
    "    * makes use of ML\n",
    "    * out of the box algorithms\n",
    "* What is an image?\n",
    "    * A digital image is an array or matrix of square pixels (picture elements) arranged in columns and rows: in other words, a 2-dimensional matrix.\n",
    "    * These pixels contain information about color and intensity. \n",
    "* There are some testing-purpose images provided by scikit-image, in a module called data. \n",
    "* 2-dimensional color images are often represented in RGB—3 layers of 2-dimensional arrays, where the three layers represent Red, Green and Blue channels of the image.\n",
    "* Grayscale images only have shades of black and white. Often, the grayscale intensity is stored as an 8-bit integer giving 256 possible different shades of gray. Grayscale images don't have any color information.\n",
    "* RGB images have three color channels, while grayscaled ones have a single channel.\n",
    "\n",
    "* Convert RGB to Grayscale or Grayscale to RGB with:\n",
    "\n",
    "```\n",
    "from skimage import color\n",
    "grayscale = color.rgb2gray(original)\n",
    "rgb = color.gray2rgb(grayscale)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f10c1",
   "metadata": {},
   "source": [
    "To display images using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262e63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title='Image', cmap_type='gray'):\n",
    "    plt.imshow(image, cmap= cmap_type)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5db7fe",
   "metadata": {},
   "source": [
    "#### Numpy for images\n",
    "* With NumPy, we can practice simple image processing techniques, such as flipping images, extracting features, and analyzing them.\n",
    "* Because images can be represented by NumPy multi-dimensional arrays (or \"NdArrays\"), NumPy methods for manipulating arrays work well on these images.\n",
    "* Remember that a color image is a NumPy array with athird dimension for color channels. We can slice the multidimensional array and obtain these channels separately.\n",
    "* To see the individual color intensities along the image:\n",
    "\n",
    "```\n",
    "# Obtain the ____ [red, blue, or green] values of the image \n",
    "# Keep the values of the height and width of the pixels, select only the desired layer of color:\n",
    "red = image[:, :, 0]\n",
    "green = image[:, :, 1]\n",
    "blue = image[:, :, 2]\n",
    "```\n",
    "* Just like with NumPy arrays, we can get the shape of images. This Madrid picture is 426 pixels high and 640 pixels wide.\n",
    "* It has three layers for color representation: it's an RGB-3 image. So it has shape of (426, 640, 3): `madrid_image.shape`, and a total number of pixels of 817920: `madrid_image.size`\n",
    "* We can flip the image vertically by using the `np.flipud()` method.\n",
    "` seville_vertical_flip = np.flipud(flipped_seville)`\n",
    "* You can flip the image horizontally using the `np.fliplr()` method.\n",
    "`seville_horizontal_flip = np.fliplr(seville_vertical_flip)`\n",
    "* Use the `show_image()` function to display an image.\n",
    "`show_image(seville_horizontal_flip, 'Seville')`\n",
    "* The histogram of an image is a graphical representation of the amount of pixels of each intensity value. From 0 (pure black) to 255(pure white). \n",
    "* We can also create histograms from RGB-3 colored images. In this case each channel: red, green and blue will have a corresponding histogram.\n",
    "* We can learn a lot about an image by just looking at its histogram. Histograms are used to **threshold images**, to **alter brightness and contrast**, and to **equalize** an image\n",
    "* Matplotlib has a histogram method. It takes an input array (frequency) and bins as parameters. The successive elements in bin array act as the boundary of each bin. We obtain the red color channel of the image by slicing it:\n",
    "`red = image[:, :, 0]` \n",
    "* We then use the histogram function:\n",
    "`plt.hist(red.ravel(), bins=256)`\n",
    "* Use `.ravel()` to return a continuous flattened array from the color values of the image, in this case red. And pass this ravel and the bins as parameters. We set bins to 256 because we'll show the number of pixels for every pixel value, that is, from 0 to 255. Meaning you need 256 values to show the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3895619",
   "metadata": {},
   "source": [
    "#### Getting started with thresholding\n",
    "* Thresholding is used to partition the background and foreground of grayscale images, by essentially making them black and white. \n",
    "* We compare each pixel to a given threshold value. \n",
    "* If the pixel is less than that value, we turn it white. If it's greater, we turn it black.\n",
    "* **Thresholding** is the simplest method of image segmentation.\n",
    "* Thresholding lets us isolate elements and is used in: object detection, facial recognition, etc\n",
    "    * works best in high contrast grayscale images\n",
    "* To threshold color images, we must first convert them to grayscale:\n",
    "\n",
    "```\n",
    "# Set optimal threshold value:\n",
    "thresh = 127 # midpoint between 0 and 255\n",
    "# Apply thresholding to the image\n",
    "binary = image > thresh\n",
    "# Show original and thresholded\n",
    "show_image(image, 'Original')\n",
    "show_image(binary, 'Thresholded')\n",
    "```\n",
    "* **Inverted thresholding = inverting the color**\n",
    "\n",
    "```\n",
    "# Set optimal threshold value:\n",
    "thresh = 127 # midpoint between 0 and 255\n",
    "# Apply thresholding to the image\n",
    "inverted_binary = image <= thresh\n",
    "# Show original and thresholded\n",
    "show_image(image, 'Original')\n",
    "show_image(inverted_binary, 'Inverted thresholded')\n",
    "```\n",
    "* There are 2 categories of thresholding in scikit-image:\n",
    "    * **Global or Histogram-based:** good for uniform backgrounds\n",
    "    * **Local or adaptive:** for uneven background illumination (background is not easily differentiated)\n",
    "        * Note that local is slower than global thresholding.\n",
    "* scikit-image includes a function that evaluates several global algorithms, so that you can choose the one that gives you best results: the `try_all_threshold()` function from `filters` module.\n",
    "    \n",
    "```\n",
    "from skimage.filters import try_all_threshold\n",
    "# Obtain all the resulting images\n",
    "fig, ax = try_all_threshold(image, verbose = False) #v=False so it doesn't print function name for each method\n",
    "# Show resulting plots\n",
    "show_plt(fig, ax)\n",
    "```\n",
    "* It will use seven global algorithms\n",
    "* When the background of an image seems uniform, global thresholding works best. Previously, we arbitrarily set the thresh value, but we can also calculate the optimal value. \n",
    "* For that we import the `threshold_otsu()` function from filters module. Then obtain the optimal global thresh value by calling this function. Apply the local thresh to the image.\n",
    "\n",
    "```\n",
    "# Import the otsu threshold function\n",
    "from skimage.filters import threshold_otsu\n",
    "# Obtain the optimal threshold value\n",
    "thresh = threshold_otsu(image)\n",
    "# Apply thresholding to the image \n",
    "binary_global = image > thresh\n",
    "```\n",
    "* the optimal thresh is spotted by a red line in the histogram of the image.\n",
    "* **Local threshold:**\n",
    "* If the image doesn't have high contrast or the background is uneven, local thresholding produces better results. \n",
    "* Import `threshold_local()`, also from filters. \n",
    "* With this function, we calculate thresholds in small pixel regions surrounding each pixel we are binarizing. So we need to specify a block_size to surround each pixel; also known as local neighborhoods. \n",
    "* And an optional offset, that's a constant subtracted from the mean of blocks to calculate the local threshold value.\n",
    "* Here in the threshold_local function we set a block_size of 35 pixels and an offset of 10. Then apply that local thresh.\n",
    "\n",
    "```\n",
    "#Import the local threshold function\n",
    "from skimage.filters import threshold_local \n",
    "#Set the block size to 35\n",
    "block_size = 35\n",
    "#Obtain the optimal local thresholding\n",
    "local_thresh = threshold_local(text_image, block_size, offset = 10)\n",
    "#Apply local thresholding and obtain the binary image\n",
    "binary_local = text_image > local_thresh\n",
    "```\n",
    "\n",
    "```\n",
    "# Import the otsu threshold function\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "# Make the image grayscale using rgb2gray\n",
    "chess_pieces_image_gray = rgb2gray(chess_pieces_image)\n",
    "\n",
    "# Obtain the optimal threshold value with otsu\n",
    "thresh = threshold_otsu(chess_pieces_image_gray)\n",
    "\n",
    "# Apply thresholding to the image\n",
    "binary = chess_pieces_image_gray > thresh\n",
    "\n",
    "# Show the image\n",
    "show_image(binary, 'Binary image')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461b859",
   "metadata": {},
   "source": [
    "### HOG- Histogram of Oriented Gradients\n",
    "* What is a **feature descriptor**?\n",
    "    * It is a simplified representation of the image that contains only the most important information about the image.\n",
    "* There are a number of feature descriptors out there:\n",
    "    * **HOG: Histogram of Oriented Gradients**\n",
    "    * **SIFT: Scale Invariant Feature Transform**\n",
    "    * **SURF: Speeded-Up Robust Feature**\n",
    "    \n",
    "* **HOG**, or **Histogram of Oriented Gradients**, is a feature descriptor that is often used to extract features from image data. It is widely used in computer vision tasks for object detection. Some important aspects of HOG that makes it different from other feature descriptors:\n",
    "    * The HOG descriptor focuses on the structure or the shape of an object. Now you might ask, how is this different from the edge features we extract for images? In the case of edge features, we only identify if the pixel is an edge or not. HOG is able to provide the edge direction as well. This is done by extracting the gradient and orientation (or you can say magnitude and direction) of the edges\n",
    "    * Additionally, these orientations are calculated in ‘localized’ portions. This means that the complete image is broken down into smaller regions and for each region, the gradients and orientation are calculated.\n",
    "    * Finally the HOG would generate a Histogram for each of these regions separately. The histograms are created using the gradients and orientations of the pixel values, hence the name ‘Histogram of Oriented Gradients’\n",
    "* To put a formal definition to this: The HOG feature descriptor counts the occurrences of gradient orientation in localized portions of an image.\n",
    "\n",
    "* **Process of Calculating the Histogram of Oriented Gradients (HOG):**\n",
    "    * **Step 1: Preprocess the Data:**\n",
    "        * We need to preprocess the image and bring down the width to height ratio to 1:2. The image size should preferably be 64 x 128. \n",
    "        * This is because we will be dividing the image into 8*8 and 16*16 patches to extract the features. \n",
    "        * Having the specified size (64 x 128) will make all our calculations pretty simple.\n",
    "    * **Step 2: Calculating Gradients (direction x and y):**\n",
    "        * The next step is to calculate the gradient for every pixel in the image. \n",
    "        * Gradients are the small change in the x and y directions.\n",
    "        * Take a small patch from the image and calculate the gradients on that\n",
    "        * get the pixel values for this patch.\n",
    "        * generate a pixel matrix for the given patch\n",
    "        * to determine the gradient (or change) in the x-direction, we need to subtract the value on the left from the pixel value on the right.\n",
    "        * Similarly, to calculate the gradient in the y-direction, we will subtract the pixel value below from the pixel value above the selected pixel.\n",
    "        * This process will give us two new matrices – one storing gradients in the x-direction and the other storing gradients in the y direction. This is similar to using a Sobel Kernel of size 1. The magnitude would be higher when there is a sharp change in intensity, such as around the edges.\n",
    "        * We have calculated the gradients in both x and y direction separately. The same process is repeated for all the pixels in the image. \n",
    "    * **Step 3: Calculate the Magnitude and Orientation:**\n",
    "        * Using the gradients we calculated in the last step, we will now determine the magnitude and direction for each pixel value. For this step, we will be using the Pythagoras theorem\n",
    "        * The orientation comes out to be 36 when we plug in the values. So now, for every pixel value, we have the total gradient (magnitude) and the orientation (direction). We need to generate the histogram using these gradients and orientations.\n",
    "    * **Step 4: Calculate Histogram of Gradients in 8×8 cells (9×1):**\n",
    "        * The histograms created in the HOG feature descriptor are not generated for the whole image. Instead, the image is divided into 8×8 cells, and the histogram of oriented gradients is computed for each cell.\n",
    "        * By doing so, we get the features (or histogram) for the smaller patches which in turn represent the whole image. We can certainly change this value here from 8 x 8 to 16 x 16 or 32 x 32.\n",
    "        * Once we have generated the HOG for the 8×8 patches in the image, the next step is to normalize the histogram.\n",
    "    * **Step 5: Normalize gradients in 16×16 cell (36×1):**\n",
    "        * Although we already have the HOG features created for the 8×8 cells of the image, the gradients of the image are sensitive to the overall lighting. This means that for a particular picture, some portion of the image would be very bright as compared to the other portions.\n",
    "        * We cannot completely eliminate this from the image. But we can reduce this lighting variation by normalizing the gradients by taking 16×16 blocks.\n",
    "        * To normalize this matrix, we will divide each of these values by the square root of the sum of squares of the values.\n",
    "    * **Step 6: Step 6: Features for the complete image:**\n",
    "```\n",
    "#importing required libraries\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#reading the image\n",
    "img = imread('puppy.jpeg')\n",
    "imshow(img)\n",
    "print(img.shape)\n",
    "```\n",
    "* We can see that the shape of the image is 663 x 459. We will have to resize this image into 64 x 128. Note that we are using skimage which takes the input as height x width.\n",
    "\n",
    "```\n",
    "#resizing image \n",
    "resized_img = resize(img, (128,64)) \n",
    "imshow(resized_img) \n",
    "print(resized_img.shape)\n",
    "\n",
    "#creating hog features \n",
    "fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "```\n",
    "* a basic idea of what each of these hyperparameters represents:\n",
    "    * The `orientations` are the number of buckets we want to create. Since I want to have a 9 x 1 matrix, I will set the orientations to 9 \n",
    "    * `pixels_per_cell` defines the size of the cell for which we create the histograms. In the example we covered in this article, we used 8 x 8 cells and here I will set the same value. As mentioned previously, you can choose to change this value \n",
    "    * We have another hyperparameter `cells_per_block` which is the size of the block over which we normalize the histogram. Here, we mention the cells per blocks and not the number of pixels. So, instead of writing 16 x 16, we will use 2 x 2 here\n",
    "\n",
    "* As expected, we have 3,780 features for the image and this verifies the calculations we did in step 7 earlier. You can choose to change the values of the hyperparameters and that will give you a feature matrix of different sizes.\n",
    "\n",
    "```\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True) \n",
    "\n",
    "ax1.imshow(resized_img, cmap=plt.cm.gray) \n",
    "ax1.set_title('Input image') \n",
    "\n",
    "# Rescale histogram for better display \n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10)) \n",
    "\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) \n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b3ec2",
   "metadata": {},
   "source": [
    "#### Different Methods to Create Histograms using Gradients and Orientation:\n",
    "* **Method 1:**\n",
    "    * Let us start with the simplest way to generate histograms. We will take each pixel value, find the orientation of the pixel and update the frequency table.\n",
    "    * Here is the process for the highlighted pixel (85). Since the orientation for this pixel is 36, we will add a number against angle value 36, denoting the frequency.\n",
    "    * The same process is repeated for all the pixel values, and we end up with a frequency table that denotes angles and the occurrence of these angles in the image. This frequency table can be used to generate a histogram with angle values on the x-axis and the frequency on the y-axis.\n",
    "* **Method 2:**\n",
    "    * This method is similar to the previous method, except that here we have a bin size of 20. So, the number of buckets we would get here is 9.\n",
    "* **Method 3:**\n",
    "    * The above two methods use only the orientation values to generate histograms and do not take the gradient value into account. \n",
    "    * Here is another way in which we can generate the histogram – instead of using the frequency, we can use the gradient magnitude to fill the values in the matrix. \n",
    "* **Method 4:**\n",
    "    * Let’s make a small modification to the above method. Here, we will add the contribution of a pixel’s gradient to the bins on either side of the pixel gradient. Remember, the higher contribution should be to the bin value which is closer to the orientation.\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d82ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "872c804b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\frac{first}{second}}{\\frac{third}{fourth}}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
