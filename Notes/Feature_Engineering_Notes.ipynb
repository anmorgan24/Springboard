{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03c4017",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368db22",
   "metadata": {},
   "source": [
    "#### Categorical, Text, and Image Features\n",
    "* Data scientists regularly work with categorical, text, and image data. However, to execute machine learning algorithms on these data types, it's necessary to perform transformations first. \n",
    "* Categorical data, such as the neighborhood in which a property is located, does not always work well with the machine learning algorithm you're most interested in using. \n",
    "* Linear regression, for example, requires numerical inputs.\n",
    "* Options include one-hot encoding of categorical data and text and image data feature engineering (important for processes like NLP, which has applications in social media and data mining).\n",
    "* Featuer engineering with images can be very complex: the simplest of which is just using the pixel values themselves\n",
    "* HOG: Histogram of Oriented Gradients\n",
    "   \n",
    "Feature Engineering: understand how best to preprocess and engineer features from categorical, continuous, and unstructured data. \n",
    "\n",
    "* **Feature Engineering:** the act of taking raw data and extracting features for machine learning \n",
    "* Most machine learning algorithms work with tabular data.\n",
    "* Most ML algorithms require their imput data to be represented as a vector or a matrix and many assume that the data is distributed normally\n",
    "\n",
    "* **Different Types of Data:**\n",
    "    * **Continuous:** either integers (whole numbers) or floats (decimal values)\n",
    "    * **Categorical:** one of a limited set of values, e.g. gender, country of birth\n",
    "    * **Ordinal:** ranked values, often with no detail of distance between them\n",
    "    * **Boolean:** True/False values\n",
    "    * **Datetime:** dates and times\n",
    "* in pandas, \"objects\" are columns that contain strings\n",
    "* knowing the types of each column can be very useful if you are performing analysis based on a subset of specific data types. To do this, use: `.select_dtypes()` method and pass a list of relevant data types: `only_ints = df.select_dtypes(include=['int'])`\n",
    "\n",
    "#### Categorical Variables\n",
    "* Categorical variables are used to represent groups that are qualitative in nature, like colors, country of birth\n",
    "* You will need to encode categorical values as numeric values to use them in your machine learning models \n",
    "* When categories are unordered (like colors, country of birth), assigned ordered numerical values to them may greatly penalize the effectiveness of your model.\n",
    "* Thus, you cannot allocate arbitrary numbers to each category, as that would imply some form of ordering to the categories\n",
    "* $\\Rightarrow$ **One Hot Encoding**\n",
    "* $\\Rightarrow$ **Dummy Encoding**\n",
    "    * Very similar, and often confused\n",
    "    * by default, pandas performs one hot encoding when you use the get_dummies() function\n",
    "    * difference:\n",
    "        * **One Hot Encoding:** converts *n* categories into *n* features\n",
    "            * `pd.get_dummies(df, columns=['Country'], prefix ='C')`\n",
    "            * note that specifying a prefix argument can improve readability, especially if the list of column names passed to `columns` contains more than one column.\n",
    "            * **Use for: generally creating more explainable features**\n",
    "            * **Note: one must be aware that one-hot encoding may create features that are entirely colinear due to the same information being represented multiple times. \n",
    "        * **Dummy Encoding:** creates *n* - 1 features for *n* categories\n",
    "            * `pd.get_dummies(df, columns=['Coutnry'], drop_first=True, prefix = 'C')`\n",
    "            * the dropped column (referred to as the *base column* is encoded by the absence of all other features and it's value is represented by the intercept\n",
    "            * **Use for: Necessary information without duplication.**\n",
    "            \n",
    "        * Both one-hot encoding and dummy encoding may result in a **huge** number of columns being created if there are too many different categories in a column \n",
    "        * In these cases, you may only want to create columns for the most common values:\n",
    "            * `counts = df['Country'].value_counts()` # to check occurences of a category value \n",
    "            * once you have your counts of column category occurences, you can use it to limit what values you will include by first creating a mask of values that occur less than *n* times:\n",
    "            * `mask = df['Country'].isin(counts[counts<5].index)`\n",
    "            * use the mask to replace these categories that occur less frequently with a value of your choice (for example: an umbrella category like 'Other')\n",
    "            * `df['Country'][mask] = 'Other'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd500195",
   "metadata": {},
   "source": [
    "#### Numeric variables\n",
    "* Even if your raw data is all numeric, there is still a lot you can do to improve your features \n",
    "* Types of numeric features:\n",
    "    * Age\n",
    "    * Price\n",
    "    * Counts\n",
    "    * Geospacial data (such as coordinates) \n",
    "* A few of the considerations and possible feature engineering steps to keep in mind when dealing with numeric data:\n",
    "\n",
    "* **Is the magnitude of the feature its most important trait, or just its direction?**\n",
    "    * Can you turn numeric values (for example, number of restaurant health code violations) into binary values (has restaurant ever violated a health code before? yes/no)\n",
    "    * **Binarizing numeric data:**\n",
    "    * #Create new column: Binary Violation\n",
    "    * `df['Binary_Violation'] = 0`\n",
    "    * `df.loc[df['Number_of_Violations'] > 0, 'Binary_Violation'] = 1`\n",
    "    \n",
    "    * **Binning numeric variables:** \n",
    "    * Similar to binarizing, but using more than just 2 bins\n",
    "    * Often useful for variables such as age brackets, income brackets, etc where exact numbers are less relevant than general magnitude of the value \n",
    "    * `df['Binned_Group'] = pd.cut(df['Number_of_Violations'], bins=[-np.inf, 0, 2, np.inf], labels =[1,2,3])`\n",
    "    * note in above code: `bin` arguments represent cut-off points; so, for 3 bins, 4 values are needed\n",
    "    * bins created using `pd.cut()`\n",
    "    * **Note:** A new column can be created using `df[column_name] = default_value`\n",
    "\n",
    "```\n",
    "# Create the Paid_Job column filled with zeros\n",
    "so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
    "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\n",
    "```\n",
    "    \n",
    "* Bins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\n",
    "\n",
    "```\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Specify the boundaries of the bins\n",
    "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary using these boundaries\n",
    "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], \n",
    "                                         bins=bins, labels=labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8562ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e718389d",
   "metadata": {},
   "source": [
    "## Text mining in Python\n",
    "* Text Mining is the process of deriving meaningful information from natural language text.\n",
    "* The overall goal is to turn the texts into data for analysis, via application of Natural Language Processing.\n",
    "\n",
    "## Natural Language Processing (NLP)\n",
    "* NLP is a part of computer science and artificial intelligence which deals with human languages\n",
    "* In other words, NLP is a coponent of text mining that performs a special kind of linguistic analysis that essentially helps a machine \"read\" text.\n",
    "* It uses a different methodology to decipher the ambiguities in human language, including:\n",
    "    * automatic summarization\n",
    "    * part-of-speech tagging\n",
    "    * disambiguation\n",
    "    * chunking\n",
    "    * natural language understanding and recognition\n",
    "**First, we need to install the NLTK library that is the natural language toolkit for building Python programs to work with human language data**\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* **Tokenization:**\n",
    "    * the first step in NLP\n",
    "    * it is the process of breaking strings into tokens which in turn are small structures or units.\n",
    "    * involves three steps:\n",
    "        * 1) breaking a complex sentence into words\n",
    "        * 2) understanding the importance of each word with respect to the sentence\n",
    "        * 3) produce structural description on an input sentence\n",
    "* Example input:\n",
    "\n",
    "```\n",
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "# sample text for performing tokenization\n",
    "text = “In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern\n",
    "side of South America\"\n",
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "token\n",
    "```\n",
    "* Output:\n",
    "\n",
    "```\n",
    "['In','Brazil','they','drive', 'on','the', 'right-hand', 'side', 'of', 'the', 'road', '.', 'Brazil', 'has', 'a', 'large', 'coastline', 'on', 'the', 'eastern', 'side', 'of', 'South', 'America']\n",
    "```\n",
    "\n",
    "#### Finding frequency of distinct tokens in the text\n",
    "* **2** methods:\n",
    "\n",
    "* Example input, **method 1**:\n",
    "\n",
    "```\n",
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist\n",
    "```\n",
    "* Output: `FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})`\n",
    "\n",
    "* Example input, **method 2**:\n",
    "\n",
    "```\n",
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1\n",
    "```\n",
    "* Output:\n",
    "\n",
    "```\n",
    "[('the', 3),\n",
    " ('Brazil', 2),\n",
    " ('on', 2),\n",
    " ('side', 2),\n",
    " ('of', 2),\n",
    " ('In', 1),\n",
    " ('they', 1),\n",
    " ('drive', 1),\n",
    " ('right-hand', 1),\n",
    " ('road', 1)]\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f8fac",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "* Stemming usually refers to normalizing words into its base form or root form.\n",
    "* For example: **waiting**, **waited**, **waits** $\\Rightarrow$ **wait**\n",
    "* There are two methods in stemming, namely, **1) Porter Stemming** (removes common morphological and inflectional endings from words) and **2) Lancaster Stemming** (a more aggressive stemming algorithm).\n",
    "\n",
    "* **method 1:**\n",
    "\n",
    "```\n",
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(“waiting”)\n",
    "```\n",
    "* output: `wait`\n",
    "\n",
    "* **method 2:**\n",
    "\n",
    "```\n",
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word+ \":\" +pst.stem(word))\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "waited:wait\n",
    "waiting:wait\n",
    "waits:wait\n",
    "```\n",
    "* **method 3:**\n",
    "\n",
    "```\n",
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [“giving”, “given”, “given”, “gave”]\n",
    "for word in stm :\n",
    " print(word+ “:” +lst.stem(word))\n",
    "``` \n",
    "* output:\n",
    "\n",
    "```\n",
    "giving:giv\n",
    "given:giv\n",
    "given:giv\n",
    "gave:gav\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c607a1",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "* groups together different inflected forms of a word, called Lemma\n",
    "* SOmehow similar to Stemming, as it maps several words into one common root\n",
    "* Output of Lemmatization is a proper word\n",
    "* For example, a Lemmatize should map 'gone', 'going', and 'went' into 'go'\n",
    "\n",
    "* **Lemmatization**, in simpler terms is the process of converting a word to it's base form. \n",
    "* the difference between stemming and lemmatization is that lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "* For example, lemmatization would correctly identify the base form of 'caring' to 'care', whereas stemming would cutoff the 'ing' part and convert it to 'car'\n",
    "* Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP\n",
    "\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(“rocks :”, lemmatizer.lemmatize(“rocks”)) \n",
    "print(“corpora :”, lemmatizer.lemmatize(“corpora”))\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "rocks : rock\n",
    "corpora : corpus\n",
    "```\n",
    "\n",
    "#### Stop words\n",
    "* “Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library\n",
    "\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "# importing stopwors from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words(‘english’))\n",
    "text = “Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.”\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
    "Output of stopwords:\n",
    "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
    "```\n",
    "\n",
    "#### Part of speech tagging (POS)\n",
    "* Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection)\n",
    "*  There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "text = “vote to choose a particular man or a group (party) to represent them in parliament”\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "print(nltk.pos_tag([token]))\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "[('vote', 'NN')]\n",
    "[('to', 'TO')]\n",
    "[('choose', 'NN')]\n",
    "[('a', 'DT')]\n",
    "[('particular', 'JJ')]\n",
    "[('man', 'NN')]\n",
    "[('or', 'CC')]\n",
    "[('a', 'DT')]\n",
    "[('group', 'NN')]\n",
    "[('(', '(')]\n",
    "[('party', 'NN')]\n",
    "[(')', ')')]\n",
    "[('to', 'TO')]\n",
    "[('represent', 'NN')]\n",
    "[('them', 'PRP')]\n",
    "[('in', 'IN')]\n",
    "[('parliament', 'NN')]\n",
    "```\n",
    "\n",
    "#### Named Entity Recognition\n",
    "* is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value.\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "text = “Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event”\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk\n",
    "```\n",
    "* output:\n",
    "\n",
    "```\n",
    "Tree('S', [Tree('GPE', [('Google', 'NNP')]), (\"'s\", 'POS'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])\n",
    "```\n",
    "\n",
    "#### Chunking\n",
    "* Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.\n",
    "* **method:**\n",
    "\n",
    "```\n",
    "text = “We saw the yellow dog”\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = “NP: {<DT>?<JJ>*<NN>}” \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)\n",
    "```\n",
    "* output:\n",
    "`(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a6b62",
   "metadata": {},
   "source": [
    "#### Encoding Text\n",
    "* When you are faced with text data, it will often **not** be tabular\n",
    "* Data that is not in a predefined form is called **unstructured data**\n",
    "    * One example of unstructured data is: **free text**\n",
    "* Before you can leverage text data in an ML model, you must first transform it into a series of columns of numbers or vectors \n",
    "* There are many different approaches to achieving this (above): we'll go through some of the most common\n",
    "* Using Inaugural Speeches dataset:\n",
    "    * before any text analytics can be performed, you must ensure that the text data is in a format that can be used\n",
    "    * the body of the text of these speeches contained in 'text' column as single observation per speech\n",
    "    \n",
    "* **1)** Most bodies of text will have **non-letter characters**, such as punctuation, that**will need to be removed before analysis.**\n",
    "    * Use: `.replace()`\n",
    "    * regex:\n",
    "        * `[a-zA-Z]` : All letter characters\n",
    "        * `[^a-zA-Z]` : All non-letter characters\n",
    "    * `speech_df['text'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')`\n",
    " \n",
    "* **2)** Once you have removed all unwanted characters, you will then want to **standardize the remaining characters in your text so that they are all lowercase.**\n",
    "    * `speech_df['text'] = speech_df['text'].str.lower()`\n",
    "        \n",
    "* Often there is value in the fundamental characteristics of a text\n",
    "    * **Length of a text**\n",
    "        * `speech_df['char_cnt'] = speech_df['text'].str.len()`\n",
    "        * calculate the number of characters in each speech\n",
    "    * **Word counts**\n",
    "        * `speech_df['word_cnt'] = speech_df['text'].str.split().str.len()`\n",
    "    * **Average length of word**\n",
    "        * `speech_df['avg_word_len'] = speech_df['char_cnt'] / speech_df['word_cnt']`\n",
    "\n",
    "#### Word Count Representation\n",
    "* The most common approach to creating features from transformed & cleaned text data, is to create a column for each word and record the number of times each particular word appears in each text.\n",
    "* this results in a set of columns equal in width to the number of unique words in the dataset with counts filling each entry.\n",
    "* sklearn already has a built-in function for this with: `CountVectorizer()`\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "```\n",
    "* creating a column for every word will result in far too many values for analysis. Thankfully, you can specify arguments when initializing your CountVectorizer to limit this.\n",
    "* for example, you can specify the minimum number of texts that a word must be contained in: `min_df`\n",
    "    * if a float is given (for example `min_df = 0.1`) then the word must appear in at least this percent of documents\n",
    "    * this threshold eliminates words that occur so rarely that they would not be useful when generalizing to new texts\n",
    "    * conversely, `max_df` limits words to only ones that occur below a certain percentage of the data\n",
    "    * this can be useful to remove words that occur too frequently to be of any value \n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction import CountVectorizer\n",
    "cv = CountVectorizer(min_df = 0.1, max_df = 0.9)\n",
    "cv.fit(speech_df['text_clean'])\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "```\n",
    "* The above code outputs a *sparse array* with a row for each text and a column for every word\n",
    "* to transform this to a *non-sparse array*:\n",
    "`cv_transformed.toarray()`\n",
    "* output will be an array with no concept of column names\n",
    "* to get the names of the features (words) that have been generated, call:\n",
    "`feature_names = cv.get_feature_names()`\n",
    "* returns a list of the features (words) generated, in the same order that the columns of the converted array are in\n",
    "\n",
    "* **Note:** While fitting and transforming separately can be useful, particularly when you need to transform a different dataset than the one that you used to fit the vectorizer, you can accomplish both steps at once using the `.fit_transform()` method\n",
    "* **Putting it all together:**\n",
    "\n",
    "`cv_df = pd.DataFrame(cv_transformed.toarray(), columns =cv.get_feature_names())\\.add_prefix('Counts_')`\n",
    "\n",
    "* you can now combine this dataframe with your original dataframe so that they can be used to generate future analytical models \n",
    "\n",
    "`speech_df = pd.concat([speech_df, cv_df, axis=1, sort=False)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be986f",
   "metadata": {},
   "source": [
    "#### Tf-Idf (Term Frequency- Inverse Document Frequency)\n",
    "* While counts of occurences of words can be a good first step towards encoding your text to build models, it has some limitations\n",
    "* The main issue, is counts will be much higher for very common words that occur across all texts, providing very little value as a distinguishing feature.\n",
    "* to limit common words like \"the\" from overpowering your model, some form of normalization can be used \n",
    "* one of the most effective approaches to do this is called: **Term Frequency- Inverse Document Frequency**, or **TF-IDF**\n",
    "\n",
    "\\begin{equation}\n",
    "TF-IDF =\n",
    "\\frac{\\frac{count-of-word-occurences}{total-words-in-document}}{log\\frac{number-of-docs-word-is-in}{total-number-of-docs}}\n",
    "\\end{equation}\n",
    "\n",
    "* the effect: reduces the value of common words, while increasing the weights of words that do not occur in many documents \n",
    "* to use the TF-IDF Vectorizer\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "```\n",
    "* silimarly to when you worked with the CountVectorizer, where you could limit the number of features created by specifying arguments \n",
    "* set `max_features` argument = 100, will only use the top 100 most common words\n",
    "* `stop_words` are a predefined list of the most common [insert language here] words, such as \"and\" and \"the\"\n",
    "    * you can use sklearn's built-in list, load your own, or use lists provided by other Python libraries\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tv.fit(train_speech_df['text'])\n",
    "train_tv_transformed = tv.transform(train_speech_df['text'])\n",
    "```\n",
    "* **Note** that here we are fitting and transforming the training data as a subset of the original data\n",
    "\n",
    "```\n",
    "train_tv_df = pd.DataFrame(train_tv_transformed.toarray(), columns= tv.get_feature_names()).add_prefix(TFIDF_')\n",
    "train_speech_df = pd.concat([train_speech_df, train_tv_df], axis = 1, sort = False)\n",
    "```\n",
    "* Inspect your transformation:\n",
    "\n",
    "```\n",
    "examine_row = train_tv_df.iloc[0]\n",
    "print(examine_row.sort_values(ascending=False)\n",
    "```\n",
    "* **Applying a vectorizer to new data (like the test set):**\n",
    "* **Preprocess** your test data using the transformations made on the train data **only**.\n",
    "\n",
    "```\n",
    "test_tv_transformed = tv.tranform(test_df['text_clean'])\n",
    "\n",
    "test_tv_diff = pd.DataFrame(test_tv_transformed.toarray(), columns=tv.get_feature_names()).add_prefix('TDIDF_')\n",
    "\n",
    "test_speech_df = pd.concat([test_speech_df, test_tv_df], axis = 1, sort=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4835a5",
   "metadata": {},
   "source": [
    "#### Bag of words and N-grams\n",
    "* So far we've looked at individual words on their own, without any context\n",
    "* This approach is called a **bag of words model**, as the words are being treated as if they are being drawn from a bag at random, with no concept of order or grammar\n",
    "* Individual words *can* lose all their context or meaning when viewed independently \n",
    "* Issues with bag of words:\n",
    "    * Positive meaning: happy\n",
    "    * Negative meaning: not happy\n",
    "    * Positive meandin: never not happy\n",
    "* $\\Uparrow$ different meanings of happy depending on context \n",
    "* One common method to retain at least some concept of word order in a text is to instead use multiple consecutive words, like pairs (**bi-grams**), or three consecutive words (**tri-grams**).\n",
    "* This maintains at least some ordering information, while at the same time allowing for the creation of a reasonable set of features\n",
    "* To leverage N-grams in your own models:\n",
    "* `ngram_range` parameter = values equal the minimum and maximum length of the ngrams to be included\n",
    "\n",
    "```\n",
    "tv_bi_gram_vec = TfidfVectorizer(ngram_range =(2,2))\n",
    "\n",
    "tv_bi_gram = tv_bi-gram_vec.fit_transform(speech_df['text'])\n",
    "```\n",
    "* Create a DataFrame with the Counts features \n",
    "\n",
    "```\n",
    "tv_df = pd.DataFrame(tv_bi_gram.toarray(), columns=tv_bi_gram_vec.get_feature_names().add_prefix('Counts')\n",
    "tv_sums = tv_df.sum()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e09ed",
   "metadata": {},
   "source": [
    "#### Image Processing and scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0209c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fece03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f69db5e",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\frac{first}{second}}{\\frac{third}{fourth}}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
