{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d71283f",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6a6a4",
   "metadata": {},
   "source": [
    "High-dimensional datasets can be overwhelming and leave you not knowing where to start. Typically, you’d visually explore a new dataset first, but when you have too many dimensions the classical approaches will seem insufficient. Fortunately, there are visualization techniques designed specifically for high dimensional data. After exploring the data, you’ll often find that many features hold little information because they don’t show any variance or because they are duplicates of other features; detect these features and drop them from the dataset so that you can focus on the informative ones.  In a next step, you might want to build a model on these features, and it may turn out that some don’t have any effect on the thing you’re trying to predict. You’ll learn how to detect and drop these irrelevant features too, in order to reduce dimensionality and thus complexity. Finally, you’ll learn how feature extraction techniques can reduce dimensionality for you through the calculation of uncorrelated principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525787f",
   "metadata": {},
   "source": [
    "### I. Exploring High Dimensional Data\n",
    "Learn the difference between feature selection and feature extraction and will apply both techniques for data exploration.\n",
    "* **Dimensionality:** the number of columns in your dataset (assuming that you have a tidy dataset)\n",
    "* **Tidy data set:** Every column represents a variable or feature and every row represents an observation or instance of each variable.\n",
    "* **High-dimensional:** When you have many columns, or features, in your dataset; high-dimensionality indicates complexity.\n",
    "* **Note:** by default, `.describe()` ignores the non-numeric columns in a dataset; we can tell describe to do the opposite, by passing the argument `exclude='number'`; or, `df.describe(exclude='number')`; we will then get summary statistics adapted to non-numeric data\n",
    "\n",
    "* Becoming familiar with the shape of your dataset and the properties of the features within it, is a crucial step you should take before you make the decision to reduce dimensionality\n",
    "\n",
    "#### Methods for reducing dimensionality:\n",
    "* Drop columns with little to no variance (when you are looking to determine differences among observations in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc34589",
   "metadata": {},
   "source": [
    "#### Feature selection vs Feature Extraction\n",
    "* Reducing the number of dimensions in your dataset has multiple benefits. Your dataset will become:\n",
    "    * less complex\n",
    "    * require less disk space\n",
    "    * require less computation time\n",
    "    * have lower chance of model overfitting\n",
    "    \n",
    "* The simplest way to reduce dimensionality is to only select the features or columns that are important to you from a larger dataset\n",
    "    * If you're new to a dataset or have little background knowledge of a dataset topic, you'll likely have to do some exploring to determine which features are both relevant and useful.\n",
    "    * Seaborn's **pairplot** is excellent to visually explore small to medium sized datasets\n",
    "    \n",
    "```\n",
    "sns.pairplot(ansur_df, hue = 'gender', diag_kind='hist')\n",
    "```\n",
    "#### Pairplots\n",
    "* **sns pairplot** provides a 1x1 comparison of each numeric feature in the dataset in the form of a scatterplot. Plus, diagonally, a view of the distribution of each feature (for example, with a histogram, as specified in the above code).\n",
    "    * Pairplots make it very easy to visually spot duplicated features (such as a weight column of different units- kilogramsa and pounds), as well as unvarying features (such as a constant); both of these types of columns can typically be dropped for dimensionality reduction\n",
    "\n",
    "* Always try to minimize information loss by only removing features that are irrelevant or hold little unique information (if possible)\n",
    "\n",
    "#### Feature extraction\n",
    "* Compared to feature selection, **feature extraction** is a completely different approach but with the same goal of reducing dimensionality\n",
    "* Instead of selecting a subset of features from our initial dataset, we calculate or extract new features from the original ones (for example: PCA).\n",
    "    * These new features have as little redundant information as possible and are therefore fewer in number\n",
    "    * One downside: the newly created features are often less intuitive to understand than the original ones\n",
    "* Dimensionality of datasets with a lot of strong correlations between the different features in it, can be reduced a lot with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978bbe4",
   "metadata": {},
   "source": [
    "### t-SNE visualization of high-dimensional data\n",
    "* **t-SNE** = **t-Distributed Stochastic Neighbor Embedding**\n",
    "* A powerful technique to visualize high-dimensional data using feature extraction\n",
    "* t-SNE will maximize the distance in 2-D space between observations that are mmost different in a high-dimensional space\n",
    "    * Because of this, observations that are similar together will be close to one another and may become clustered\n",
    "    * **t-SNE doesn't work with non-numeric data** (though you can always use one-hot-encoding to get around this if necessary).\n",
    "\n",
    "```\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "m = TSNE(learning_rate=50)\n",
    "tsne_features = m.fit_transform(df_numeric)\n",
    "tsne_features[1:4,:]\n",
    "\n",
    "df['x'] = tsne_features[:, 0]\n",
    "df['y'] = tsne_features[:, 1]\n",
    "\n",
    "```\n",
    "* `.fit_transform()` will project our high-dimensional dataset onto a NumPy array with two dimensions\n",
    "* Assign these two dimensions back to our original dataset, naming them 'x' and 'y.'\n",
    "\n",
    "* **High learning** rates will cause the algorithm to be **more adventurous** in the configurations it tries out\n",
    "* **Low learning** rates will cause the algorithm to be **conservative**.\n",
    "* Usually, learning rates fall in the 10 to 1000 range\n",
    "\n",
    "* Plot t-SNE using seaborn's scatterplot:\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x = 'x', y = 'y', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'BMI_class', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* t-SNE helps us to visually explore our dataset and identify the most importnat drivers of variance in body shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bcd6a",
   "metadata": {},
   "source": [
    "## II. Feature selection I, selecting for feature information \n",
    "#### The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1531b",
   "metadata": {},
   "source": [
    "* Models tend to overfit badly on high-dimensional data\n",
    "* How to detect low quality features and how to remove them?\n",
    "* With each feature you add to a dataset, you should also be ready to increase the number of observations in your dataset\n",
    "    * If you don't, you'll end up with a lot of unique combinations of features that models can easily memorize and thus overfit to \n",
    "* The solution to the curse of high dimensionality is to apply dimensionality reduction.\n",
    "\n",
    "```\n",
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06f67d",
   "metadata": {},
   "source": [
    "#### Features with missing values or little variance\n",
    "* Automate the selection of features that have sufficient variance and not too many missing values \n",
    "* Creating a feature selector:\n",
    "\n",
    "#### VarianceThreshold()\n",
    "* built-in sklearn feature selection tool\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=1)\n",
    "sel.fit(ansur_df)\n",
    "\n",
    "mask = sel.get_support()\n",
    "print(mask)\n",
    "\n",
    "reduced_df = ansur_df.loc[:, mask]\n",
    "print(reduced_df.shape)\n",
    "```\n",
    "* The `.get_support()` method will give us a `True` or `False` value on whether each feature's variance is above the threshold or not\n",
    "* We call this type of Boolean array a mask, and we can use this mask to reduce the number of dimensions in our dataset\n",
    "\n",
    "#### Variance selector caveats\n",
    "* One problem with variance thresholds is that the variance values aren't always easy to interpret or compare between features\n",
    "* `buttock_df.boxplot()`\n",
    "* If, for example, higher values have higher variances, we should normalize the variances before using for feature selection\n",
    "    * to do so, divide each column by its mean value before fitting the selector\n",
    "    * After normalization, the variance in the dataset will be lower (so we can therefore reduce the variance threshold- but make sure to inspect your data visually while setting this value)\n",
    "    \n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.005)\n",
    "sel.fit(ansur_df / ansur_df.mean())\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold = 0.005)\n",
    "sel.fit(ansur_df / ansur_df.mean())\n",
    "mask = sel.get_support()\n",
    "reduced_df = ansur_df.loc[:, mask]\n",
    "print(reduced_df.shape)\n",
    "```\n",
    "\n",
    "* Another reason that you might want to drop a feature is if it contains a lot of missing values.\n",
    "* `.isna()`\n",
    "* Get ration of missing values between 0 and 1 for each column/feature in dataframe:\n",
    "* **`pokemon_df.isna().sum() / len(pokemon_df)`**\n",
    "* Based on this $\\Uparrow$ ratio, we can create a mask for features that have fewer missing values than a certain threshold:\n",
    "\n",
    "```\n",
    "mask = pokemon_df.isna().sum() / len(pokemon_df) < 0.3\n",
    "reduced_df = pokemon_df.loc[:, mask]\n",
    "```\n",
    "* When features have some missing values, but not *too* much, we could apply imputation to fill in the blanks\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df/ head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d7548",
   "metadata": {},
   "source": [
    "#### Pairwise correlation\n",
    "* Look at how features relate to one another to decide if they are worth keeping.\n",
    "* `sns.pairplot(ansur, hue = 'gender')` allows us visually identify strongly correlated features; however, if we want to quanity the correlation between features, this method would fall short\n",
    "* To solve this, we use **correlation coefficient** $\\rho$\n",
    "* The value of $\\rho$ always lies between minus one and plus one;\n",
    "    * -1 desscribes a perfectly negative correlation \n",
    "    * 1 describes a perfectly positive correlation\n",
    "    * 0 describes no correlation \n",
    "* Calculate **correlation matrix:**\n",
    "    * `weights_df.corr()`\n",
    "    * correlation matrix shows the correlation coefficient for each pairwise combination of features in the dataset\n",
    "    * By definition, the diagonal in our correlation matrix shows a series of ones, telling us, not surprisingly, that each each feature is perfectly correlated to itself.\n",
    "* Visualizing the correlation matrix:\n",
    "\n",
    "```\n",
    "cmap = sns.diverging_palette(h_neg = 10,\n",
    "                             h_pos = 240,\n",
    "                             as_cmap = True)\n",
    "\n",
    "sns.heatmap(weights_df.corr(), center = 0, cmap= cmap, linewidths= 1, annot= True, fmt= \".2f\")\n",
    "```\n",
    "* We can improve this plot further by removing duplicate and unnecessary information like the correlation coefficients of 1 on the diagonal; to do so, we'll create a boolean mask.\n",
    "\n",
    "```\n",
    "corr = weights_df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "```\n",
    "* NumPy's `ones_like()` function creates a matrix filled with True values (or 1's) with the same dimensions as our correlation matrix \n",
    "* We then pass this to NumPy's `triu()` (\"triangle upper\") function, to set all non-upper triangle values to False.\n",
    "* When we pass this mask to the heatmap() function, it will ignore the upper triangle, allowing us to focus on the interesting part of the plot:\n",
    "* `sns.heatmap(weights_df.corr(), mask=mask, center=0, cmap=cmap, linewidths=1, annot= True, fmt=\".2f\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774d0a8",
   "metadata": {},
   "source": [
    "#### Removing highly correlated features \n",
    "* Features that are perfectly correlated with each other, with a correlation coefficient of 1 or -1 bring no new information to a dataset, but do add to the comlexity.\n",
    "    * So, naturally, we'd want to drop one of the duplicated features that holds the same information\n",
    "* In addition to this, we might want to drop features that have correlation coefficients close to 1 or -1 if they are measurements of the same or similar things \n",
    "* If you are confident that dropping highly correlated features will not cause you to lose too much information, you filter them out using a threshold value.\n",
    "\n",
    "```\n",
    "# Create positive correlation matrix\n",
    "corr_df = chest_df.corr().abs()\n",
    "# Create and apply mask\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "tri_df\n",
    "```\n",
    "* When we pass this mask to the pd dataframe `mask()` method, it will replace all positions in the dataframe where the mask has a True value with NA\n",
    "* Then use a list comprehension to find all columns that have a correlation to any feature stronger than the threshold value.\n",
    "\n",
    "```\n",
    "# Find columns that meet threshold\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "# Drop those columns \n",
    "reduced_df = chest_df.drop(to_drop, axis=1)\n",
    "```\n",
    "* The reason we use a mask to set half of the matrix to NA values is that we want to avoid removing *both* features when they have a strong correlation\n",
    "* This method is a bit of a brute force approach that *should only be applied if you have a good understanding of the dataset.*\n",
    "* **Note:** Correlation coefficients can produce weird results when the relation between two features is non-linear or when outliers are involved.\n",
    "* **Strong correlations do not imply causation.**\n",
    "\n",
    "```\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871afb4f",
   "metadata": {},
   "source": [
    "### III. Feature selection II, selecting for model accuracy\n",
    "* Learn how to let models help you find the most important features in a dataset for predicting a particular target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b71e5",
   "metadata": {},
   "source": [
    "#### Selecting features for model performance\n",
    "* A more pragmatic approach to dimension reduction is to select features based on how they affect model performance\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "X_test_std = scaler.transform(X_test)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(lr.coef_)\n",
    "```\n",
    "* Use above method to check if any coefficient values are close to zero. \n",
    "* Since these coefficients will be multiplied with the feature values when the model makes a prediction, features with coefficients close to zero will contribute little to the end result\n",
    "* We can use the zip function to transform the output into a dictionary that shows which feature has which coefficient.\n",
    "* `print(dict(zip(X.columns, abs(lr.coef_[0]))))`\n",
    "* If we want to remove a feature from the initial dataset with as little effect on the predictions as possible, we should pick the one(s) with the lowest coefficient(s).\n",
    "* **The fact what we standardized the data first makes sure that we can compare the coefficients to one another.**\n",
    "* We could repeat the above process until we have the desired number of features, but there is a sklearn function for that: **RFE** for **Recursive Feature Elimination** is a feature selection algorithm that can be wrapped around any model that produces feature coefficients of feature importance values.\n",
    "* We can pass it the model we want to use and the number of features we want to select\n",
    "* While fitting to our data, it will repeat a process where it first fits the internal model and then drops the feature with the weakest coefficient.\n",
    "    * It will keep doing this until the desired number of features is reached.\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator= LogisticRegression(), n_features_to_select=2, verbose=1)\n",
    "rfe.fit(X_train_std, y_train)\n",
    "```\n",
    "* If we set RFE's verbose parameter higher than zero, we'll be able to see that features are dropped one by one while fitting\n",
    "* Another pro: this recursive method is much safer than many other methods, as dropping one feature will cause all of the other coefficients to change (and so the task must be done recursively).\n",
    "\n",
    "```\n",
    "X.columns[rfe.support_]\n",
    "```\n",
    "* `.support_` feature containts True/False values indicating which features were kept in the dataset\n",
    "* Using the zip function once more, we can also check out rfe's `.ranking_` attribute to see in which iteration a feature was dropped (values of 1 mean that a feature was kept in the dataset until the end, while high values mean the feature was dropped early on):\n",
    "\n",
    "```\n",
    "X.columns[rfe.support_]\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "print(accuracy_score(y_test, rfe.predict(X_test_std)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db7f75",
   "metadata": {},
   "source": [
    "#### Tree-based feature selection\n",
    "* Some models will perform feature selection by design to avoid overfitting \n",
    "    * For example: Random Forest Classifier\n",
    "* Random Forest algorithm naturally calculates feature importance values\n",
    "    * These values can be extracted from a trained model using the **`feature_importances_`** attribute:\n",
    "    \n",
    "```\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.feature_importances_)\n",
    "```\n",
    "* Just like the coefficients produced by the logistic regressor, these feature importance values can be used to perform feature selection, as unimportant features will be closer to zero.\n",
    "* An advantage of these feature importance values over coefficients is that they are comparable between features by default, since they always sum up to one (meaning we don't have to scale out input data first)\n",
    "* We can also use the feature importance values to create a True/False (boolean) mask for features that meet a certain importance threshold:\n",
    "\n",
    "```\n",
    "mask = rf.feature_importancces_ > 0.1\n",
    "X_reduced = X.loc[:, mask]\n",
    "print(X_reduced.columns)\n",
    "```\n",
    "* **Remember** that dropping one weak feature can make other features relatively more or less important; if you want to play it safe and minimize the risk of dropping the wrong features, you should not drop all least important features at once, but rather one by one; to do so, once again wrap a Recursive Feature Eliminator (or `RFE()`) around the model:\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe= RFE(estimator= RandomForestClassifier(), \n",
    "                    n_features_to_select= 6,\n",
    "                    verbose=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "```\n",
    "* However, training the model once for each feature we want to drop can result in too much computational overhead; to speed up the process, we can pass the `step` parameter to `RFE()`:\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe= RFE(estimator= RandomForestClassifier(), \n",
    "                    n_features_to_select= 6,\n",
    "                    step = 10,\n",
    "                    verbose=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "```\n",
    "* With `step` set to 10, on each iteration, the 10 least important features are dropped.\n",
    "* Once the final model is trained, we can use the feature eliminator's `.support_` attribute as a mask to print the remaining column names.\n",
    "* `print(X.columns[rfe.support_])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f99c32",
   "metadata": {},
   "source": [
    "```\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "```\n",
    "***\n",
    "***\n",
    "```\n",
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "```\n",
    "***\n",
    "*** \n",
    "\n",
    "```\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f568c2",
   "metadata": {},
   "source": [
    "#### Regularized linear regression\n",
    "* Linear regressions: build a model that derives the linear function between three input values and a target\n",
    "\n",
    "```\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Calculate R-squared\n",
    "print(lr.score(X_test, y_test)\n",
    "```\n",
    "* With **regularization**, the model will not only try to be as accurate as possible by minimizing the loss function, but it will also **try to keep the model simple** by keeping the coefficients low. \n",
    "* The strength of regularization can be tweaked with **alpha** ($\\alpha$).\n",
    "* When alpha is too low, the model might overfit.\n",
    "* When alpha is too high, the model might become too simple and inaccurate (the model might be underfitted).\n",
    "* **LASSO regularization:** **L**east **A**bsolute **S**hrinkage and **S**election **O**perator\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "la = Lasso(alpha = 0.05)\n",
    "la.fit(X_train, y_train)\n",
    "print(la.coef_)\n",
    "```\n",
    "\n",
    "```\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "```\n",
    "* **NOTE TO SELF:** Always `.fit_transform` scaler on `Xtrain` $\\Rightarrow$ and then fit regularizing object (`Lasso()`, etc) on `X_train_scaled`, `y_train` $\\Rightarrow$ then use pre-fitted scaler to `.transform` **ONLY** `X_test`.\n",
    "\n",
    "```\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7347b",
   "metadata": {},
   "source": [
    "#### Combining feature selectors \n",
    "* Above we manually found an optimal alpha value, but this can be very tedious.\n",
    "* In order to automate this: `LassoCV()` class regressor\n",
    "* **`LassoCV()`** will **use cross validation to try out different alpha settings and select the best one.**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print(lcv.alpha_)\n",
    "```\n",
    "* When we fit this model to our training data, it will get an `.alpha_` attribute with the optimal value\n",
    "* To actually remove the features to which the Lasso regressor assigned a zero, we create a mask for all non-zero coefficients:\n",
    "\n",
    "```\n",
    "mask = lcv.coef_ != 0\n",
    "reduced_X = X.loc[:, mask]\n",
    "```\n",
    "\n",
    "**Recap: Random Forests as a type of ensemble model**\n",
    "* Random forest is a combination of decision trees\n",
    "* We can use combination of models for feature selection too.\n",
    "* Designed on the idea that a lot of weak predictors can combine to form a strong one\n",
    "* Instead of trusting a single model to tell us which features are important, we could have multiple models each cast their vote on whether we should keep a feature or not\n",
    "\n",
    "* **Feature selection with LassoCV:**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "lcv.score(X_test, y_test)\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "sum(lcv_mask)\n",
    "```\n",
    "* Output: `66`\n",
    "\n",
    "* **Feature selection with random forest:**\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "             n_features_to_select=66,\n",
    "             step=5,\n",
    "             verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "rf_mask= rfe_rf.support_\n",
    "```\n",
    "* Note that here (above) we've wrapped a RFE around the model to have it select the same number of features as the LassoCV() regressor (66).\n",
    "\n",
    "* **Feature selection with a gradient boosting regressor:**\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "rfe_gb = RFE(estimator= GradientBoostingRegressor(),\n",
    "             n_features_to_select=66, step=5, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "gb_mask = rfe_gb.support_\n",
    "``` \n",
    "\n",
    "* **Combining the (3) feature selectors:**\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "print(votes)\n",
    "```\n",
    "* **The output will be an array with the number of votes that each feature got.** For example:\n",
    "* `array([3, 2, 2, ..., 3, 0, 1])`\n",
    "* What we do with this vote then depends on how conservative we want to be\n",
    "    * If we want to make sure we don't lose any information, we could select all features with at least one vote\n",
    "    * For majority voting (in this case of using 3 estimators in the vote), we could use 2:\n",
    "\n",
    "```\n",
    "mask = votes >= 2\n",
    "reduced_X = X.loc[:, mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a305148",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_\n",
    "\n",
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b57eb5",
   "metadata": {},
   "source": [
    "### IV. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cd55b",
   "metadata": {},
   "source": [
    "* A deep-dive on the most frequently used dimensionality reduction algorithm, **Principal Component Analysis (PCA)**.\n",
    "* Apply it both to data exploration and data pre-processing in a modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7f9f6",
   "metadata": {},
   "source": [
    "* **Feature extraction** comes down to calculating new features based on the existing ones while trying to lose as little information as possible\n",
    "* **Feature extraction** creates new features, which are in fact combinations of the original ones.\n",
    "* There are powerful algorithms that will calculate the new features in a way that as much information as possible is preserved. There are also more simple methods of feature extraction as well.\n",
    "\n",
    "* **Creation of new features (BMI):**\n",
    "* When you have a good understanding of the features in your dataset, you can sometimes combine multiple features into a new feature that makes the original ones obsolete.\n",
    "* For example: *BMI* (or Body Mass Index) is a combination of *height* and *weight*.\n",
    "* If we build a model based on these features, the height and weight features themselves might become obsolete once we have the BMI (and we could drop them from the dataset with the drop method to reduce dimensionality\n",
    "\n",
    "* **Feature generation: Averages**\n",
    "* Imagine we have a body measurement dataset with measurements for both left and right leg lengths\n",
    "* For most applications, it would be sufficient to reduce these two features into a single leg length feature\n",
    "\n",
    "```\n",
    "leg_df['leg mm'] = leg_df[['right leg mm',, 'left leg mm']].mean(axis=1)\n",
    "leg_df.drop(['right leg mm', 'left leg mm'], axis=1)\n",
    "```\n",
    "* Taking the average of two features comes with the cost of losing some information\n",
    "\n",
    "#### Intro to PCA\n",
    "* **For PCA, it's important to scale the features first, so that their values are easier to compare.**\n",
    "\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "df_std = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d895fe",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "* describe a datapoint in terms of multiplication of principal vectors\n",
    "* Remember that PCA can really underperform if you don't scale your data first.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "std_df = scaler.fit_transform(df)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "print(pca.fit_transform(std_df))\n",
    "```\n",
    "* When we plot these values for all points in the dataset, we'll see that our resulting point cloud no longer shows any correlation and, therefore, no more duplicate information (plot PC_1 on x and PC_2 on y)\n",
    "* If we were to have a third feature in the original dataset, we would also have to add a third principal component if we don't want to lose any information\n",
    "* This remains true as you keep adding features.\n",
    "* You could describe a 100 feature dataset with 100 principal components, however the components are much harder to understand than the original features.\n",
    "* **The components share no duplicate information and are ranked from most to least important**\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(std_df)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "* Access the **explained variance ratio** of each principal component after fitting the algorithm to the data using the `.explained_variance_ratio_` attribute\n",
    "* When you're dealing with a dataset with a lot of correlation, the explained variance typically becomes concentrated in the first few components\n",
    "* The remaining components then explain so little variance that they can be dropped\n",
    "\n",
    "```\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(ansur_std_df)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "```\n",
    "* Use NumPy's cumulative sum method on the explained_variance_ratio_ attribute to see how much variance we can explaine in total by using a certain number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22907436",
   "metadata": {},
   "source": [
    "```\n",
    "sns.pairplot(ansur_df)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler and standardize the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "\n",
    "# This changes the numpy array output back to a dataframe\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e1331",
   "metadata": {},
   "source": [
    "#### PCA applications\n",
    "* When you use PCA for dimensionality reduction, you decide how much of the explained variance you're willing to sacrifice\n",
    "* However, one downside of PCA is that the remaining components can be hard to interpret\n",
    "* To improve your understanding of the components it can help to look at the `.components_` attribute.\n",
    "* The **`.components_`** attribute tells us to what extent each component's vector is affected by a particular feature.\n",
    "* The features that have the biggest positive or negative effects on a component can then be used to add a meaning to that component.\n",
    "\n",
    "#### PCA in a pipeline\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reducer', PCA())])\n",
    "pc = pipe.fit_transform(ansur_df)\n",
    "\n",
    "print(pc[:, :2])\n",
    "```\n",
    "* **Note:** Since we always scale the data before applying PCA, we can combine both operations in a pipeline!\n",
    "    * We pass the two operations to the Pipeline() calss in the form of two tuples inside a list.\n",
    "    * Within each tuple we give out operation a name, 'scaler' and 'reducer.'\n",
    "    * Then fit and transform the data in one go.\n",
    "\n",
    "* **PCA is not the preferred algorithm to reduce the dimensionality of categorical datasets.** But, we can check whether they align with the most important sources of variance in the dataset\n",
    "* Add first two principal components to our dataframe and plot them with Seaborn's scatterplot():\n",
    "\n",
    "```\n",
    "ansur_categories['PC_1'] = pc[:,0]\n",
    "ansur_categories['PC_2'] = pc[:,1]\n",
    "sns.scatterplot(data=ansur_categories,\n",
    "                x='PC 1',\n",
    "                y='PC 2',\n",
    "                hue='Height_class', alpha=0.4)\n",
    "                \n",
    "sns.scatterplot(data=ansur_categories,\n",
    "                x='PC 1',\n",
    "                y='PC 2',\n",
    "                hue='Gender',\n",
    "                alpha=0.4)\n",
    "                \n",
    "sns.scatterplot(data=ansur_categories,\n",
    "                x='PC 1',\n",
    "                y='PC 2',\n",
    "                hue= 'BMI_class',\n",
    "                alpha=0.4)\n",
    "```\n",
    "* To go beyond data exploration, we can add a model to the pipeline:\n",
    "#### PCA in a model pipeline\n",
    "\n",
    "```\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=3)),\n",
    "        ('classifier', RandomForestClassifier())])\n",
    "pipe.fit(X_train, y_train)\n",
    "```\n",
    "* Once the pipeline is fit to the training data, we can access the steps within by indexing over the `.steps_` attribute: **`print(pipe.steps[1])`**\n",
    "* PCA is the second step, and therefore has index 1.\n",
    "* This returns a tule with the name of the step first and the actual algorithm second.\n",
    "* We can then access the `.explained_variance_ratio_` attribute like so:\n",
    "\n",
    "* `pipe.steps[1][1].explained_variance_ratio_.cumsum()`\n",
    "\n",
    "* When we take its cumulative sum, we see that these three components explain 74% of the variance in the dataset.\n",
    "\n",
    "```\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20fa76",
   "metadata": {},
   "source": [
    "#### Principal Component selection\n",
    "* Above we set the number of components that the PCA algorithm should calculate\n",
    "* An alternative technique is telling PCA the minimal proportion of variance we want to keep and let the algorithm decide on the number of components it needs to achieve that.\n",
    "* We can do this by **passing a number between 0 and 1 to the n_components parameter of PCA**.\n",
    "* For example, when we pass it 0.9, it will make sure to select enough components to explain 90% of the variance\n",
    "\n",
    "```\n",
    "pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reducer', PCA(n_components=0.9))])\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(poke_df)\n",
    "\n",
    "print(len(pipe.steps[1][1].components_))\n",
    "```\n",
    "* One problem is that, whether we set the number of components as an integer or ratio, we're still just making these numbers up using our gut feeling.\n",
    "* There is no single right answer to the question, \"how many components should I keep?\" because it depends on how much information you are willing to sacrifice to reduce complexity\n",
    "\n",
    "#### Plot to determine optimal number of components\n",
    "\n",
    "```\n",
    "pipe.fit(poke_df)\n",
    "\n",
    "var = pipe.steps[1][1].explained_variance_ratio_\n",
    "\n",
    "plt.plot(var)\n",
    "\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Exlplained variance ratio')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* As you go from left to right in this type of plot, you'll often see that the explained variance ratio per component starts to level out quite abruptly \n",
    "* The location of where this shift happens is known as **the 'elbow' in the plot.**\n",
    "* This typicaly gives you a good starting point for the number of components to keep.\n",
    "\n",
    "* **Trick: how to go back from the pricipal components to the original feature space.**\n",
    "    * This can be done by calling the **`.inverse_transform()`** method on the principle component array:\n",
    "        * `pca.inverse_transform(pc)`\n",
    "\n",
    "* Because you typically lose information going from left to right, you'll see that the result from going back to the original feature space will have changedd somewhat\n",
    "    * An application where this is very relevant is **image compression**\n",
    "    \n",
    "```\n",
    "pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reducer', PCA(n_components=290))])\n",
    "pipe.fit(X_train)\n",
    "pc = pipe,fit_transform(X_test)\n",
    "\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "img_plotter(X_rebuilt)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd41322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307de63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875594ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
