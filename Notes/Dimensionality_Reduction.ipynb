{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d71283f",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6a6a4",
   "metadata": {},
   "source": [
    "High-dimensional datasets can be overwhelming and leave you not knowing where to start. Typically, you’d visually explore a new dataset first, but when you have too many dimensions the classical approaches will seem insufficient. Fortunately, there are visualization techniques designed specifically for high dimensional data. After exploring the data, you’ll often find that many features hold little information because they don’t show any variance or because they are duplicates of other features; detect these features and drop them from the dataset so that you can focus on the informative ones.  In a next step, you might want to build a model on these features, and it may turn out that some don’t have any effect on the thing you’re trying to predict. You’ll learn how to detect and drop these irrelevant features too, in order to reduce dimensionality and thus complexity. Finally, you’ll learn how feature extraction techniques can reduce dimensionality for you through the calculation of uncorrelated principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525787f",
   "metadata": {},
   "source": [
    "### Exploring High Dimensional Data\n",
    "Learn the difference between feature selection and feature extraction and will apply both techniques for data exploration.\n",
    "* **Dimensionality:** the number of columns in your dataset (assuming that you have a tidy dataset)\n",
    "* **Tidy data set:** Every column represents a variable or feature and every row represents an observation or instance of each variable.\n",
    "* **High-dimensional:** When you have many columns, or features, in your dataset; high-dimensionality indicates complexity.\n",
    "* **Note:** by default, `.describe()` ignores the non-numeric columns in a dataset; we can tell describe to do the opposite, by passing the argument `exclude='number'`; or, `df.describe(exclude='number')`; we will then get summary statistics adapted to non-numeric data\n",
    "\n",
    "* Becoming familiar with the shape of your dataset and the properties of the features within it, is a crucial step you should take before you make the decision to reduce dimensionality\n",
    "\n",
    "#### Methods for reducing dimensionality:\n",
    "* Drop columns with little to no variance (when you are looking to determine differences among observations in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc34589",
   "metadata": {},
   "source": [
    "#### Feature selection vs Feature Extraction\n",
    "* Reducing the number of dimensions in your dataset has multiple benefits. Your dataset will become:\n",
    "    * less complex\n",
    "    * require less disk space\n",
    "    * require less computation time\n",
    "    * have lower chance of model overfitting\n",
    "    \n",
    "* The simplest way to reduce dimensionality is to only select the features or columns that are important to you from a larger dataset\n",
    "    * If you're new to a dataset or have little background knowledge of a dataset topic, you'll likely have to do some exploring to determine which features are both relevant and useful.\n",
    "    * Seaborn's **pairplot** is excellent to visually explore small to medium sized datasets\n",
    "    \n",
    "```\n",
    "sns.pairplot(ansur_df, hue = 'gender', diag_kind='hist')\n",
    "```\n",
    "#### Pairplots\n",
    "* **sns pairplot** provides a 1x1 comparison of each numeric feature in the dataset in the form of a scatterplot. Plus, diagonally, a view of the distribution of each feature (for example, with a histogram, as specified in the above code).\n",
    "    * Pairplots make it very easy to visually spot duplicated features (such as a weight column of different units- kilogramsa and pounds), as well as unvarying features (such as a constant); both of these types of columns can typically be dropped for dimensionality reduction\n",
    "\n",
    "* Always try to minimize information loss by only removing features that are irrelevant or hold little unique information (if possible)\n",
    "\n",
    "#### Feature extraction\n",
    "* Compared to feature selection, **feature extraction** is a completely different approach but with the same goal of reducing dimensionality\n",
    "* Instead of selecting a subset of features from our initial dataset, we calculate or extract new features from the original ones (for example: PCA).\n",
    "    * These new features have as little redundant information as possible and are therefore fewer in number\n",
    "    * One downside: the newly created features are often less intuitive to understand than the original ones\n",
    "* Dimensionality of datasets with a lot of strong correlations between the different features in it, can be reduced a lot with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978bbe4",
   "metadata": {},
   "source": [
    "### t-SNE visualization of high-dimensional data\n",
    "* **t-SNE** = **t-Distributed Stochastic Neighbor Embedding**\n",
    "* A powerful technique to visualize high-dimensional data using feature extraction\n",
    "* t-SNE will maximize the distance in 2-D space between observations that are mmost different in a high-dimensional space\n",
    "    * Because of this, observations that are similar together will be close to one another and may become clustered\n",
    "    * **t-SNE doesn't work with non-numeric data** (though you can always use one-hot-encoding to get around this if necessary).\n",
    "\n",
    "```\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "m = TSNE(learning_rate=50)\n",
    "tsne_features = m.fit_transform(df_numeric)\n",
    "tsne_features[1:4,:]\n",
    "\n",
    "df['x'] = tsne_features[:, 0]\n",
    "df['y'] = tsne_features[:, 1]\n",
    "\n",
    "```\n",
    "* `.fit_transform()` will project our high-dimensional dataset onto a NumPy array with two dimensions\n",
    "* Assign these two dimensions back to our original dataset, naming them 'x' and 'y.'\n",
    "\n",
    "* **High learning** rates will cause the algorithm to be **more adventurous** in the configurations it tries out\n",
    "* **Low learning** rates will cause the algorithm to be **conservative**.\n",
    "* Usually, learning rates fall in the 10 to 1000 range\n",
    "\n",
    "* Plot t-SNE using seaborn's scatterplot:\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x = 'x', y = 'y', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'BMI_class', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* t-SNE helps us to visually explore our dataset and identify the most importnat drivers of variance in body shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bcd6a",
   "metadata": {},
   "source": [
    "## Feature selection I, selecting for feature information \n",
    "#### The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1531b",
   "metadata": {},
   "source": [
    "* Models tend to overfit badly on high-dimensional data\n",
    "* How to detect low quality features and how to remove them?\n",
    "* With each feature you add to a dataset, you should also be ready to increase the number of observations in your dataset\n",
    "    * If you don't, you'll end up with a lot of unique combinations of features that models can easily memorize and thus overfit to \n",
    "* The solution to the curse of high dimensionality is to apply dimensionality reduction.\n",
    "\n",
    "```\n",
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06f67d",
   "metadata": {},
   "source": [
    "#### Features with missing values or little variance\n",
    "* Automate the selection of features that have sufficient variance and not too many missing values \n",
    "* Creating a feature selector:\n",
    "\n",
    "#### VarianceThreshold()\n",
    "* built-in sklearn feature selection tool\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=1)\n",
    "sel.fit(ansur_df)\n",
    "\n",
    "mask = sel.get_support()\n",
    "print(mask)\n",
    "\n",
    "reduced_df = ansur_df.loc[:, mask]\n",
    "print(reduced_df.shape)\n",
    "```\n",
    "* The `.get_support()` method will give us a `True` or `False` value on whether each feature's variance is above the threshold or not\n",
    "* We call this type of Boolean array a mask, and we can use this mask to reduce the number of dimensions in our dataset\n",
    "\n",
    "#### Variance selector caveats\n",
    "* One problem with variance thresholds is that the variance values aren't always easy to interpret or compare between features\n",
    "* `buttock_df.boxplot()`\n",
    "* If, for example, higher values have higher variances, we should normalize the variances before using for feature selection\n",
    "    * to do so, divide each column by its mean value before fitting the selector\n",
    "    * After normalization, the variance in the dataset will be lower (so we can therefore reduce the variance threshold- but make sure to inspect your data visually while setting this value)\n",
    "    \n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.005)\n",
    "sel.fit(ansur_df / ansur_df.mean())\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold = 0.005)\n",
    "sel.fit(ansur_df / ansur_df.mean())\n",
    "mask = sel.get_support()\n",
    "reduced_df = ansur_df.loc[:, mask]\n",
    "print(reduced_df.shape)\n",
    "```\n",
    "\n",
    "* Another reason that you might want to drop a feature is if it contains a lot of missing values.\n",
    "* `.isna()`\n",
    "* Get ration of missing values between 0 and 1 for each column/feature in dataframe:\n",
    "* **`pokemon_df.isna().sum() / len(pokemon_df)`**\n",
    "* Based on this $\\Uparrow$ ratio, we can create a mask for features that have fewer missing values than a certain threshold:\n",
    "\n",
    "```\n",
    "mask = pokemon_df.isna().sum() / len(pokemon_df) < 0.3\n",
    "reduced_df = pokemon_df.loc[:, mask]\n",
    "```\n",
    "* When features have some missing values, but not *too* much, we could apply imputation to fill in the blanks\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df/ head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d7548",
   "metadata": {},
   "source": [
    "#### Pairwise correlation\n",
    "* Look at how features relate to one another to decide if they are worth keeping.\n",
    "* `sns.pairplot(ansur, hue = 'gender')` allows us visually identify strongly correlated features; however, if we want to quanity the correlation between features, this method would fall short\n",
    "* To solve this, we use **correlation coefficient** $\\rho$\n",
    "* The value of $\\rho$ always lies between minus one and plus one;\n",
    "    * -1 desscribes a perfectly negative correlation \n",
    "    * 1 describes a perfectly positive correlation\n",
    "    * 0 describes no correlation \n",
    "* Calculate **correlation matrix:**\n",
    "    * `weights_df.corr()`\n",
    "    * correlation matrix shows the correlation coefficient for each pairwise combination of features in the dataset\n",
    "    * By definition, the diagonal in our correlation matrix shows a series of ones, telling us, not surprisingly, that each each feature is perfectly correlated to itself.\n",
    "* Visualizing the correlation matrix:\n",
    "\n",
    "```\n",
    "cmap = sns.diverging_palette(h_neg = 10,\n",
    "                             h_pos = 240,\n",
    "                             as_cmap = True)\n",
    "\n",
    "sns.heatmap(weights_df.corr(), center = 0, cmap= cmap, linewidths= 1, annot= True, fmt= \".2f\")\n",
    "```\n",
    "* We can improve this plot further by removing duplicate and unnecessary information like the correlation coefficients of 1 on the diagonal; to do so, we'll create a boolean mask.\n",
    "\n",
    "```\n",
    "corr = weights_df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "```\n",
    "* NumPy's `ones_like()` function creates a matrix filled with True values (or 1's) with the same dimensions as our correlation matrix \n",
    "* We then pass this to NumPy's `triu()` (\"triangle upper\") function, to set all non-upper triangle values to False.\n",
    "* When we pass this mask to the heatmap() function, it will ignore the upper triangle, allowing us to focus on the interesting part of the plot:\n",
    "* `sns.heatmap(weights_df.corr(), mask=mask, center=0, cmap=cmap, linewidths=1, annot= True, fmt=\".2f\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774d0a8",
   "metadata": {},
   "source": [
    "#### Removing highly correlated features \n",
    "* Features that are perfectly correlated with each other, with a correlation coefficient of 1 or -1 bring no new information to a dataset, but do add to the comlexity.\n",
    "    * So, naturally, we'd want to drop one of the duplicated features that holds the same information\n",
    "* In addition to this, we might want to drop features that have correlation coefficients close to 1 or -1 if they are measurements of the same or similar things \n",
    "* If you are confident that dropping highly correlated features will not cause you to lose too much information, you filter them out using a threshold value.\n",
    "\n",
    "```\n",
    "# Create positive correlation matrix\n",
    "corr_df = chest_df.corr().abs()\n",
    "# Create and apply mask\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "tri_df\n",
    "```\n",
    "* When we pass this mask to the pd dataframe `mask()` method, it will replace all positions in the dataframe where the mask has a True value with NA\n",
    "* Then use a list comprehension to find all columns that have a correlation to any feature stronger than the threshold value.\n",
    "\n",
    "```\n",
    "# Find columns that meet threshold\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "# Drop those columns \n",
    "reduced_df = chest_df.drop(to_drop, axis=1)\n",
    "```\n",
    "* The reason we use a mask to set half of the matrix to NA values is that we want to avoid removing *both* features when they have a strong correlation\n",
    "* This method is a bit of a brute force approach that *should only be applied if you have a good understanding of the dataset.*\n",
    "* **Note:** Correlation coefficients can produce weird results when the relation between two features is non-linear or when outliers are involved.\n",
    "* **Strong correlations do not imply causation.**\n",
    "\n",
    "```\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871afb4f",
   "metadata": {},
   "source": [
    "### Feature selection II, selecting for model accuracy\n",
    "* Learn how to let models help you find the most important features in a dataset for predicting a particular target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b71e5",
   "metadata": {},
   "source": [
    "# Selecting features for model performance\n",
    "* A more pragmatic approach to dimension reduction is to select features based on how they affect model performance\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "X_test_std = scaler.transform(X_test)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(lr.coef_)\n",
    "```\n",
    "* Use above method to check if any coefficient values are close to zero. \n",
    "* Since these coefficients will be multiplied with the feature values when the model makes a prediction, features with coefficients close to zero will contribute little to the end result\n",
    "* We can use the zip function to transform the output into a dictionary that shows which feature has which coefficient.\n",
    "* `print(dict(zip(X.columns, abs(lr.coef_[0]))))`\n",
    "* If we want to remove a feature from the initial dataset with as little effect on the predictions as possible, we should pick the one(s) with the lowest coefficient(s).\n",
    "* **The fact what we standardized the data first makes sure that we can compare the coefficients to one another.**\n",
    "* We could repeat the above process until we have the desired number of features, but there is a sklearn function for that: **RFE** for **Recursive Feature Elimination** is a feature selection algorithm that can be wrapped around any model that produces feature coefficients of feature importance values.\n",
    "* We can pass it the model we want to use and the number of features we want to select\n",
    "* While fitting to our data, it will repeat a process where it first fits the internal model and then drops the feature with the weakest coefficient.\n",
    "    * It will keep doing this until the desired number of features is reached.\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator= LogisticRegression(), n_features_to_select=2, verbose=1)\n",
    "rfe.fit(X_train_std, y_train)\n",
    "```\n",
    "* If we set RFE's verbose parameter higher than zero, we'll be able to see that features are dropped one by one while fitting\n",
    "* Another pro: this recursive method is much safer than many other methods, as dropping one feature will cause all of the other coefficients to change (and so the task must be done recursively).\n",
    "\n",
    "```\n",
    "X.columns[rfe.support_]\n",
    "```\n",
    "* `.support_` feature containts True/False values indicating which features were kept in the dataset\n",
    "* Using the zip function once more, we can also check out rfe's `.ranking_` attribute to see in which iteration a feature was dropped (values of 1 mean that a feature was kept in the dataset until the end, while high values mean the feature was dropped early on):\n",
    "\n",
    "```\n",
    "X.columns[rfe.support_]\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "print(accuracy_score(y_test, rfe.predict(X_test_std)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db7f75",
   "metadata": {},
   "source": [
    "#### Tree-based feature selection\n",
    "* Some models will perform feature selection by design to avoid overfitting \n",
    "    * For example: Random Forest Classifier\n",
    "* Random Forest algorithm naturally calculates feature importance values\n",
    "    * These values can be extracted from a trained model using the **`feature_importances_`** attribute:\n",
    "    \n",
    "```\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.feature_importances_)\n",
    "```\n",
    "* Just like the coefficients produced by the logistic regressor, these feature importance values can be used to perform feature selection, as unimportant features will be closer to zero.\n",
    "* An advantage of these feature importance values over coefficients is that they are comparable between features by default, since they always sum up to one (meaning we don't have to scale out input data first)\n",
    "* We can also use the feature importance values to create a True/False (boolean) mask for features that meet a certain importance threshold:\n",
    "\n",
    "```\n",
    "mask = rf.feature_importancces_ > 0.1\n",
    "X_reduced = X.loc[:, mask]\n",
    "print(X_reduced.columns)\n",
    "```\n",
    "* **Remember** that dropping one weak feature can make other features relatively more or less important; if you want to play it safe and minimize the risk of dropping the wrong features, you should not drop all least important features at once, but rather one by one; to do so, once again wrap a Recursive Feature Eliminator (or `RFE()`) around the model:\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe= RFE(estimator= RandomForestClassifier(), \n",
    "                    n_features_to_select= 6,\n",
    "                    verbose=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "```\n",
    "* However, training the model once for each feature we want to drop can result in too much computational overhead; to speed up the process, we can pass the `step` parameter to `RFE()`:\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe= RFE(estimator= RandomForestClassifier(), \n",
    "                    n_features_to_select= 6,\n",
    "                    step = 10,\n",
    "                    verbose=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "```\n",
    "* With `step` set to 10, on each iteration, the 10 least important features are dropped.\n",
    "* Once the final model is trained, we can use the feature eliminator's `.support_` attribute as a mask to print the remaining column names.\n",
    "* `print(X.columns[rfe.support_])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f99c32",
   "metadata": {},
   "source": [
    "```\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "```\n",
    "***\n",
    "***\n",
    "```\n",
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "```\n",
    "***\n",
    "*** \n",
    "\n",
    "```\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6250d5d",
   "metadata": {},
   "source": [
    "#### Regularized linear regression\n",
    "* Linear regressions: build a model that derives the linear function between three input values and a target\n",
    "\n",
    "```\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Calculate R-squared\n",
    "print(lr.score(X_test, y_test)\n",
    "```\n",
    "* With **regularization**, the model will not only try to be as accurate as possible by minimizing the loss function, but it will also **try to keep the model simple** by keeping the coefficients low. \n",
    "* The strength of regularization can be tweaked with **alpha** ($\\alpha$).\n",
    "* When alpha is too low, the model might overfit.\n",
    "* When alpha is too high, the model might become too simple and inaccurate (the model might be underfitted).\n",
    "* **LASSO regularization:** **L**east **A**bsolute **S**hrinkage and **S**election **O**perator\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "la = Lasso(alpha = 0.05)\n",
    "la.fit(X_train, y_train)\n",
    "print(la.coef_)\n",
    "```\n",
    "\n",
    "```\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "```\n",
    "* **NOTE TO SELF:** Always `.fit_transform` scaler on `Xtrain` $\\Rightarrow$ and then fit regularizing object (`Lasso()`, etc) on `X_train_scaled`, `y_train` $\\Rightarrow$ then use pre-fitted scaler to `.transform` **ONLY** `X_test`.\n",
    "\n",
    "```\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132e07d",
   "metadata": {},
   "source": [
    "#### Combining feature selectors \n",
    "* Above we manually found an optimal alpha value, but this can be very tedious.\n",
    "* In order to automate this: `LassoCV()` class regressor\n",
    "* **`LassoCV()`** will **use cross validation to try out different alpha settings and select the best one.**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print(lcv.alpha_)\n",
    "```\n",
    "* When we fit this model to our training data, it will get an `.alpha_` attribute with the optimal value\n",
    "* To actually remove the features to which the Lasso regressor assigned a zero, we create a mask for all non-zero coefficients:\n",
    "\n",
    "```\n",
    "mask = lcv.coef_ != 0\n",
    "reduced_X = X.loc[:, mask]\n",
    "```\n",
    "\n",
    "**Recap: Random Forests as a type of ensemble model**\n",
    "* Random forest is a combination of decision trees\n",
    "* We can use combination of models for feature selection too.\n",
    "* Designed on the idea that a lot of weak predictors can combine to form a strong one\n",
    "* Instead of trusting a single model to tell us which features are important, we could have multiple models each cast their vote on whether we should keep a feature or not\n",
    "\n",
    "* **Feature selection with LassoCV:**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "lcv.score(X_test, y_test)\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "sum(lcv_mask)\n",
    "```\n",
    "* Output: `66`\n",
    "\n",
    "* **Feature selection with random forest:**\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "             n_features_to_select=66,\n",
    "             step=5,\n",
    "             verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "rf_mask= rfe_rf.support_\n",
    "```\n",
    "* Note that here (above) we've wrapped a RFE around the model to have it select the same number of features as the LassoCV() regressor (66).\n",
    "\n",
    "* **Feature selection with a gradient boosting regressor:**\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "rfe_bg = RFE(estimator= GradientBoostingRegressor(),\n",
    "             n_features_to_select=66, step=5, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "gb_mask = rfe+gb.support_\n",
    "``` \n",
    "\n",
    "* **Combining the (3) feature selectors:**\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "print(votes)\n",
    "```\n",
    "* **The output will be an array with the number of votes that each feature got.** For example:\n",
    "* `array([3, 2, 2, ..., 3, 0, 1])`\n",
    "* What we do with this vote then depends on how conservative we want to be\n",
    "    * If we want to make sure we don't lose any information, we could select all features with at least one vote\n",
    "    * For majority voting (in this case of using 3 estimators in the vote), we could use 2:\n",
    "\n",
    "```\n",
    "mask = votes >= 2\n",
    "reduced_X = X.loc[:, mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c25014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
