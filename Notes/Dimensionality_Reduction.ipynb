{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f2376d",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462de0a",
   "metadata": {},
   "source": [
    "High-dimensional datasets can be overwhelming and leave you not knowing where to start. Typically, you’d visually explore a new dataset first, but when you have too many dimensions the classical approaches will seem insufficient. Fortunately, there are visualization techniques designed specifically for high dimensional data. After exploring the data, you’ll often find that many features hold little information because they don’t show any variance or because they are duplicates of other features; detect these features and drop them from the dataset so that you can focus on the informative ones.  In a next step, you might want to build a model on these features, and it may turn out that some don’t have any effect on the thing you’re trying to predict. You’ll learn how to detect and drop these irrelevant features too, in order to reduce dimensionality and thus complexity. Finally, you’ll learn how feature extraction techniques can reduce dimensionality for you through the calculation of uncorrelated principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484c349",
   "metadata": {},
   "source": [
    "### Exploring High Dimensional Data\n",
    "Learn the difference between feature selection and feature extraction and will apply both techniques for data exploration.\n",
    "* **Dimensionality:** the number of columns in your dataset (assuming that you have a tidy dataset)\n",
    "* **Tidy data set:** Every column represents a variable or feature and every row represents an observation or instance of each variable.\n",
    "* **High-dimensional:** When you have many columns, or features, in your dataset; high-dimensionality indicates complexity.\n",
    "* **Note:** by default, `.describe()` ignores the non-numeric columns in a dataset; we can tell describe to do the opposite, by passing the argument `exclude='number'`; or, `df.describe(exclude='number')`; we will then get summary statistics adapted to non-numeric data\n",
    "\n",
    "* Becoming familiar with the shape of your dataset and the properties of the features within it, is a crucial step you should take before you make the decision to reduce dimensionality\n",
    "\n",
    "#### Methods for reducing dimensionality:\n",
    "* Drop columns with little to no variance (when you are looking to determine differences among observations in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a381717",
   "metadata": {},
   "source": [
    "#### Feature selection vs Feature Extraction\n",
    "* Reducing the number of dimensions in your dataset has multiple benefits. Your dataset will become:\n",
    "    * less complex\n",
    "    * require less disk space\n",
    "    * require less computation time\n",
    "    * have lower chance of model overfitting\n",
    "    \n",
    "* The simplest way to reduce dimensionality is to only select the features or columns that are important to you from a larger dataset\n",
    "    * If you're new to a dataset or have little background knowledge of a dataset topic, you'll likely have to do some exploring to determine which features are both relevant and useful.\n",
    "    * Seaborn's **pairplot** is excellent to visually explore small to medium sized datasets\n",
    "    \n",
    "```\n",
    "sns.pairplot(ansur_df, hue = 'gender', diag_kind='hist')\n",
    "```\n",
    "#### Pairplots\n",
    "* **sns pairplot** provides a 1x1 comparison of each numeric feature in the dataset in the form of a scatterplot. Plus, diagonally, a view of the distribution of each feature (for example, with a histogram, as specified in the above code).\n",
    "    * Pairplots make it very easy to visually spot duplicated features (such as a weight column of different units- kilogramsa and pounds), as well as unvarying features (such as a constant); both of these types of columns can typically be dropped for dimensionality reduction\n",
    "\n",
    "* Always try to minimize information loss by only removing features that are irrelevant or hold little unique information (if possible)\n",
    "\n",
    "#### Feature extraction\n",
    "* Compared to feature selection, **feature extraction** is a completely different approach but with the same goal of reducing dimensionality\n",
    "* Instead of selecting a subset of features from our initial dataset, we calculate or extract new features from the original ones (for example: PCA).\n",
    "    * These new features have as little redundant information as possible and are therefore fewer in number\n",
    "    * One downside: the newly created features are often less intuitive to understand than the original ones\n",
    "* Dimensionality of datasets with a lot of strong correlations between the different features in it, can be reduced a lot with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2ccb6",
   "metadata": {},
   "source": [
    "### t-SNE visualization of high-dimensional data\n",
    "* **t-SNE** = **t-Distributed Stochastic Neighbor Embedding**\n",
    "* A powerful technique to visualize high-dimensional data using feature extraction\n",
    "* t-SNE will maximize the distance in 2-D space between observations that are mmost different in a high-dimensional space\n",
    "    * Because of this, observations that are similar together will be close to one another and may become clustered\n",
    "    * **t-SNE doesn't work with non-numeric data** (though you can always use one-hot-encoding to get around this if necessary).\n",
    "\n",
    "```\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "m = TSNE(learning_rate=50)\n",
    "tsne_features = m.fit_transform(df_numeric)\n",
    "tsne_features[1:4,:]\n",
    "\n",
    "df['x'] = tsne_features[:, 0]\n",
    "df['y'] = tsne_features[:, 1]\n",
    "\n",
    "```\n",
    "* `.fit_transform()` will project our high-dimensional dataset onto a NumPy array with two dimensions\n",
    "* Assign these two dimensions back to our original dataset, naming them 'x' and 'y.'\n",
    "\n",
    "* **High learning** rates will cause the algorithm to be **more adventurous** in the configurations it tries out\n",
    "* **Low learning** rates will cause the algorithm to be **conservative**.\n",
    "* Usually, learning rates fall in the 10 to 1000 range\n",
    "\n",
    "* Plot t-SNE using seaborn's scatterplot:\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x = 'x', y = 'y', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'BMI_class', data = df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* t-SNE helps us to visually explore our dataset and identify the most importnat drivers of variance in body shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcc6fb",
   "metadata": {},
   "source": [
    "## Feature selection I, selecting for feature information \n",
    "#### The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5d0d1",
   "metadata": {},
   "source": [
    "* Models tend to overfit badly on high-dimensional data\n",
    "* How to detect low quality features and how to remove them?\n",
    "* With each feature you add to a dataset, you should also be ready to increase the number of observations in your dataset\n",
    "    * If you don't, you'll end up with a lot of unique combinations of features that models can easily memorize and thus overfit to \n",
    "* The solution to the curse of high dimensionality is to apply dimensionality reduction.\n",
    "\n",
    "```\n",
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b62d8e",
   "metadata": {},
   "source": [
    "#### Features with missing values or little variance\n",
    "* Automate the selection of features that have sufficient variance and not too many missing values \n",
    "* Creating a feature selector:\n",
    "\n",
    "#### VarianceThreshold()\n",
    "* built-in sklearn feature selection tool\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=1)\n",
    "sel.fit(ansur_df)\n",
    "\n",
    "mask = sel.get_support()\n",
    "print(mask)\n",
    "\n",
    "reduced_df = ansur_df.loc[:, mask]\n",
    "print(reduced_df.shape)\n",
    "```\n",
    "* The `.get_support()` method will give us a `True` or `False` value on whether each feature's variance is above the threshold or not\n",
    "* We call this type of Boolean array a mask, and we can use this mask to reduce the number of dimensions in our dataset\n",
    "\n",
    "#### Variance selector caveats\n",
    "* One problem with variance thresholds is that the variance values aren't always easy to interpret or compare between features\n",
    "* `buttock_df.boxplot()`\n",
    "* If, for example, higher values have higher variances, we should normalize the variances before using for feature selection\n",
    "    * to do so, divide each column by its mean value before fitting the selector\n",
    "    * After normalization, the variance in the dataset will be lower (so we can therefore reduce the variance threshold- but make sure to inspect your data visually while setting this value)\n",
    "    \n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.005)\n",
    "sel.fit(ansur_df / ansur_df.mean())\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold = 0.005)\n",
    "sel.fit(ansur_df / ansur_df.mean())\n",
    "mask = sel.get_support()\n",
    "reduced_df = ansur_df.loc[:, mask]\n",
    "print(reduced_df.shape)\n",
    "```\n",
    "\n",
    "* Another reason that you might want to drop a feature is if it contains a lot of missing values.\n",
    "* `.isna()`\n",
    "* Get ration of missing values between 0 and 1 for each column/feature in dataframe:\n",
    "* **`pokemon_df.isna().sum() / len(pokemon_df)`**\n",
    "* Based on this $\\Uparrow$ ratio, we can create a mask for features that have fewer missing values than a certain threshold:\n",
    "\n",
    "```\n",
    "mask = pokemon_df.isna().sum() / len(pokemon_df) < 0.3\n",
    "reduced_df = pokemon_df.loc[:, mask]\n",
    "```\n",
    "* When features have some missing values, but not *too* much, we could apply imputation to fill in the blanks\n",
    "\n",
    "```\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df/ head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18133965",
   "metadata": {},
   "source": [
    "#### Pairwise correlation\n",
    "* Look at how features relate to one another to decide if they are worth keeping.\n",
    "* `sns.pairplot(ansur, hue = 'gender')` allows us visually identify strongly correlated features; however, if we want to quanity the correlation between features, this method would fall short\n",
    "* To solve this, we use **correlation coefficient** $\\rho$\n",
    "* The value of $\\rho$ always lies between minus one and plus one;\n",
    "    * -1 desscribes a perfectly negative correlation \n",
    "    * 1 describes a perfectly positive correlation\n",
    "    * 0 describes no correlation \n",
    "* Calculate **correlation matrix:**\n",
    "    * `weights_df.corr()`\n",
    "    * correlation matrix shows the correlation coefficient for each pairwise combination of features in the dataset\n",
    "    * By definition, the diagonal in our correlation matrix shows a series of ones, telling us, not surprisingly, that each each feature is perfectly correlated to itself.\n",
    "* Visualizing the correlation matrix:\n",
    "\n",
    "```\n",
    "cmap = sns.diverging_palette(h_neg = 10,\n",
    "                             h_pos = 240,\n",
    "                             as_cmap = True)\n",
    "\n",
    "sns.heatmap(weights_df.corr(), center = 0, cmap= cmap, linewidths= 1, annot= True, fmt= \".2f\")\n",
    "```\n",
    "* We can improve this plot further by removing duplicate and unnecessary information like the correlation coefficients of 1 on the diagonal; to do so, we'll create a boolean mask.\n",
    "\n",
    "```\n",
    "corr = weights_df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "```\n",
    "* NumPy's `ones_like()` function creates a matrix filled with True values (or 1's) with the same dimensions as our correlation matrix \n",
    "* We then pass this to NumPy's `triu()` (\"triangle upper\") function, to set all non-upper triangle values to False.\n",
    "* When we pass this mask to the heatmap() function, it will ignore the upper triangle, allowing us to focus on the interesting part of the plot:\n",
    "* `sns.heatmap(weights_df.corr(), mask=mask, center=0, cmap=cmap, linewidths=1, annot= True, fmt=\".2f\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555984e",
   "metadata": {},
   "source": [
    "#### Removing highly correlated features \n",
    "* Features that are perfectly correlated with each other, with a correlation coefficient of 1 or -1 bring no new information to a dataset, but do add to the comlexity.\n",
    "    * So, naturally, we'd want to drop one of the duplicated features that holds the same information\n",
    "* In addition to this, we might want to drop features that have correlation coefficients close to 1 or -1 if they are measurements of the same or similar things \n",
    "* If you are confident that dropping highly correlated features will not cause you to lose too much information, you filter them out using a threshold value.\n",
    "\n",
    "```\n",
    "# Create positive correlation matrix\n",
    "corr_df = chest_df.corr().abs()\n",
    "# Create and apply mask\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "tri_df\n",
    "```\n",
    "* When we pass this mask to the pd dataframe `mask()` method, it will replace all positions in the dataframe where the mask has a True value with NA\n",
    "* Then use a list comprehension to find all columns that have a correlation to any feature stronger than the threshold value.\n",
    "\n",
    "```\n",
    "# Find columns that meet threshold\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "# Drop those columns \n",
    "reduced_df = chest_df.drop(to_drop, axis=1)\n",
    "```\n",
    "* The reason we use a mask to set half of the matrix to NA values is that we want to avoid removing *both* features when they have a strong correlation\n",
    "* This method is a bit of a brute force approach that *should only be applied if you have a good understanding of the dataset.*\n",
    "* **Note:** Correlation coefficients can produce weird results when the relation between two features is non-linear or when outliers are involved.\n",
    "* **Strong correlations do not imply causation.**\n",
    "\n",
    "```\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15eae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
