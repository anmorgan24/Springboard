{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f31b383",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. Youâ€™ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\n",
    "\n",
    "Instructor: Upendra Devisetty, Science Analyst at CyVerse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1650a3",
   "metadata": {},
   "source": [
    "## $\\star$ Introduction to Big Data Analysis with Spark\n",
    "This chapter introduces the exciting world of Big Data, as well as the various concepts and different frameworks for processing Big Data. You will understand why Apache Spark is considered the best framework for BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9881eb1",
   "metadata": {},
   "source": [
    "#### The 3 Vs of Big Data\n",
    "* The 3 Vs are used to describe big data's characteristics\n",
    "* **Volume:** Size of the data \n",
    "* **Variety:** Different sources and formats of data\n",
    "* **Velocity:** Speed at which the data is generated and available for processing\n",
    "\n",
    "#### Big Data concepts and Terminology\n",
    "* **Clustered computing:** collection of resources of multiple machines\n",
    "* **Parallel computing:** a type of computation in which many calculations are carried out simultaneously\n",
    "* **Distributed computing:** Collection of nodes (networked computers) that run in parallel\n",
    "* **Batch processing:** Breaking the job into small piece and running them on individual machines\n",
    "* **Real-time processing:** Immediate processing of data\n",
    "\n",
    "#### Big Data processing systems\n",
    "* **Hadoop/MapReduce:** Scalable and fault-tolerant framework; written in Java\n",
    "    * Open source\n",
    "    * Batch processing\n",
    "* **Apache Spark:** General purpose and lightning fast cluster computing system\n",
    "    * Open source\n",
    "    * Suited for both batch and real-tine data processing\n",
    "\n",
    "#### Features of Apache Spark framework\n",
    "* Distributed cluster computing framework\n",
    "* Efficient in-memory computations for large scale data sets\n",
    "* Lightning-fast data processing framework\n",
    "* Provides support for Java, Scala, Python, R, and SQL\n",
    "\n",
    "#### Spark modes of deployment\n",
    "* **Local mode:** Single machine such as your laptop\n",
    "    * Convenient for testing, debugging, and demonstration\n",
    "* **Cluster mode:** Set of pre-defined machines\n",
    "    * Good for production\n",
    "* Typical workflow: Local $\\Rightarrow$ clusters\n",
    "    * During this transition, no code change is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a62dcd",
   "metadata": {},
   "source": [
    "### PySpark: Spark with Python\n",
    "#### What is Spark shell?\n",
    "* Interactive environment for running Spark jobs\n",
    "* Helpful for fast interactive prototyping\n",
    "* Spark's shells allow interacting with data on disk or in memory across many machines or one, and Spark takes care of automatically distributing this processing\n",
    "* Three different Spark shells:\n",
    "    * Spark-shell for Scala\n",
    "    * PySpark-shell for Python\n",
    "    * SparkR for R\n",
    "    \n",
    "#### PySpark shell\n",
    "* PySpark shell is the Python-based command line tool\n",
    "* PySpark shell sllows data scientists to interface with Spark data structures\n",
    "* PySpark shell supports connecting to a cluster\n",
    "\n",
    "#### Understanding SparkContext\n",
    "* SparkContext is an entry point into the world of Spark\n",
    "* An **entry point** is where control is transferred from the Operating system to the provided program.\n",
    "    * An entry point is a way of connecting to Spark cluster\n",
    "    * An entry point is \"like a key to the house.\" \n",
    "* Access the SparkContext in the PySpark shell as a variable named `sc`\n",
    "\n",
    "#### Inspecting SparkContext\n",
    "* **Version:** to retrieve SparkContext version that you are currently running:\n",
    "    * `sc.version`\n",
    "* **Python Version:** to retrieve Python version *that SparkContext is currently using*\n",
    "    * `sc.pythonVer`\n",
    "* **Master:** URL of the cluster of \"local\" string to run in local mode of SparkContext\n",
    "    * `sc.master`\n",
    "    * If returns: `local[*]`, means SparkContext acts as a master on a local node using all available threads on the computer where it is running.\n",
    "    \n",
    "#### Loading data in PySpark\n",
    "* SparkContext's **`parallelize()`** method (used on a list)\n",
    "    * For example, to create parallelized collections holding the numbers 1 to 5:\n",
    "    * `rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "* SparkContext's **`textFile()`** method (used on a file)\n",
    "    * For example, to load a text file named `test.txt` using this method:\n",
    "    * `rdd2 = sc.textFile(\"test.txt\")`\n",
    "    \n",
    "```\n",
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create a Python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f59c6",
   "metadata": {},
   "source": [
    "### Use of lambda function in python- filter()\n",
    "* Understanding PySpark becomes a lot easier if we understand functional programming principles in Python:\n",
    "    * `lambda`\n",
    "    * `map`\n",
    "    * `filter`\n",
    "* Python supports the creation of anonymous functions.\n",
    "    * **Anonymous functions** are functions that are not bound to a name at runtime, using a construct called `lambda`\n",
    "    * Lambda functions are very powerful and well-integrated into Python\n",
    "    * Lambda is especially efficient with `map()` and `filter()`\n",
    "    * Like `def`, Python creates a function to be called later in the program. However, it returns the function instead of assigning it to a name (ie **anonymous**).\n",
    "    * In practice, they are used as a way to inline a function definition, or to defer execution of a code. \n",
    "    \n",
    "#### Lambda function syntax\n",
    "* Lambda function can be used whenever function objects are required. \n",
    "* They can have any number of arguments, but only one expression, and the expression is evaluated and returned.\n",
    "* **The general syntax of the lambda function is:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9cea5",
   "metadata": {},
   "source": [
    "**`lambda arguments: expression`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3772ed0",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "```\n",
    "double = lambda x: x * 2\n",
    "print(double(3))\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "g = lambda x: X**3\n",
    "print(g(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737f30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "double = lambda x: x * 2\n",
    "print(double(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4c5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "g = lambda x: x**3\n",
    "print(g(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bedea",
   "metadata": {},
   "source": [
    "* No return statement for lambda\n",
    "* Can put lambda function anywhere, without ever assigning it to a variable\n",
    "* We use lambda functions when we require a nameless function for a short period of time\n",
    "\n",
    "#### Use of Lambda function in Python - map()\n",
    "* `map()` function takes a function and a list and returns a new list which contains items returned by that function for each item\n",
    "* General syntax of `map()`: \n",
    "    * **`map(function, list)`**\n",
    "* Example of `map` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))\n",
    "```\n",
    "\n",
    "result:\n",
    "\n",
    "**`[3, 4, 5, 6]`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a4f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63664aa3",
   "metadata": {},
   "source": [
    "#### Use of lambda function in python- filter()\n",
    "* `filter()` function takes a function and a list and returns a new list for which the function evaluates as true\n",
    "* General syntax of filter():\n",
    "    * **`filter(function, list`**\n",
    "* Example of `filter()` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88b1759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee15d2",
   "metadata": {},
   "source": [
    "```\n",
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842c13f",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter  2: Programming in PySpark RDDs\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This chapter introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4abef88",
   "metadata": {},
   "source": [
    "#### Introduction to PySpark RDD\n",
    "In this chapter, we will start working with RDDs which are Spark's core abstraction for working with data. \n",
    "\n",
    "* **RDD** = **Resilient Distributed Datasets**\n",
    "* RDDs are a collection of data distributed across the cluster\n",
    "* RDD is the fundamental and backbone data type in PySpark\n",
    "\n",
    "#### Decomposing RDDs\n",
    "* Resilient Distributed Datasets\n",
    "    * **Reslient:** ability to withstand failures\n",
    "    * **Distributed:** spanning across multiple machines\n",
    "    * **Datasets:** collection of partitioned data e.g., arrays, tables, tuples, etc. ...\n",
    "    \n",
    "#### Creating RDDs\n",
    "* Three methods for creating RDDs\n",
    "* **Parallelize:**\n",
    "    * The **simplest method** to create RDDs is to take an existing collection of objects (for example a list, array, or set) and pass it to SparkContext's parallelize method.\n",
    "* **External datasets:**\n",
    "    * A **more common** way to create RDDs is to load data from external datasets such as:\n",
    "        * files stored in HDFS\n",
    "        * Objects in Amazon S3 bucket\n",
    "        * lines in a text file\n",
    "* **From existing RDDs**\n",
    "\n",
    "#### Parallelized collection (parallelizing)\n",
    "* RDDs are created from a list or a set using the SparkContext's `parallelize` method.\n",
    "\n",
    "```\n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "type(helloRDD)\n",
    "```\n",
    "\n",
    "#### From external datasets\n",
    "* Creating RDDs from external datasets is by far the most common method in PySpark\n",
    "* `textFile()` for creating RDDs from external datasets\n",
    "* For file README stored locally:\n",
    "* `fileRDD = sc.textFile(\"README.md\")`\n",
    "* `type(fileRDD)`\n",
    "\n",
    "#### Understanding Partitioning in PySpark\n",
    "* Data partitioning is an important concept in Spark and understanding how Spark deals with partitions allows one to control parallelism.\n",
    "* A **partition** is a logical division of a large distributed data set. \n",
    "* By default, Spark partitions the data at the time of creating RDD based on several factors such as available resources, external datasets, etc. but this behavior can also be controlled by passing a second argument called `minPartitions`, which defines the minimum number of partitions to be created for an RDD\n",
    "* `parallelize()` method:\n",
    "    * `numRDD = sc.parallelize(range(10), minPartitions = 6)`\n",
    "* `textFile()` method:\n",
    "    * `fileRDD = sc.textFile(\"README.md\", minPartitions = 6)`\n",
    "* The number of partitions in an RDD can always be found by using the `getNumPartitions()` method\n",
    "\n",
    "```\n",
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e321f",
   "metadata": {},
   "source": [
    "#### RDD Operations in PySpark\n",
    "* RDDs in PySpark supports two different types of operations:\n",
    "    * Transformations\n",
    "    * Actions\n",
    "* **Transformations** are operations on RDDs that return a new RDD\n",
    "    * follow lazy evaluation\n",
    "    * basic RDD transformations:\n",
    "        * `map()`\n",
    "        * `filter()`\n",
    "        * `flatMap()`\n",
    "        * `union()`\n",
    "* **Actions** are operations that perfomr some computation on the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48d1fb",
   "metadata": {},
   "source": [
    "#### map() Transformation \n",
    "* The `map()` transformation applies a function to all elements in the RDD\n",
    "* Example:\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "```\n",
    "\n",
    "#### filter() Transformation\n",
    "* The `filter()` transformation takes in a function and returns an RDD that only has elements that pass the condition.\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "```\n",
    "\n",
    "#### flatmap() Transformation \n",
    "* The `flatMap()` transformation is similar to `map()` transformation, except that it returns multiple values for each element in the source RDD.\n",
    "* A simple usage of `flatMap()` is splitting up an input string into words\n",
    "* Even thought input RDD has 2 elements, for example, the output RDD now contains 5 elements:\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "```\n",
    "\n",
    "#### union() Transformation\n",
    "* The `union()` transformation returns the union of one RDD with another RDD\n",
    "* Similar to pandas' `concat()`\n",
    "\n",
    "```\n",
    "inputRDD = sc.textFile(\"logs.txt\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda c: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)\n",
    "```\n",
    "### RDD Actions\n",
    "* Operation return a value after running a computation on the RDD\n",
    "* Basic RDD Actions\n",
    "    * **`collect()`:** returns all the elements of the dataset as an array\n",
    "        * `RDD_map.collect()`\n",
    "    * **`take(n)`:** returns an array with the first N elements of the dataset\n",
    "        * `RDD_map.take(2)`\n",
    "    * **`first()`:** prints the first element of the RDD\n",
    "        * `RDD_map.first()`\n",
    "    * **`count()`:** return the number of elements in the RDD\n",
    "        * `RDD_flatmap.count()`\n",
    "        \n",
    "```\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "\tprint(numb)\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "  print(line)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c700d",
   "metadata": {},
   "source": [
    "#### Working with Pair RDDs in PySpark\n",
    "* Real life datasets are usually keyvaue pairs \n",
    "* Eah row is a key and maps to one or more values\n",
    "* **Pair RDD** is a special data structure to work with this kind of dataset\n",
    "* **Pair RDD:** Key is the identifier and value is data\n",
    "\n",
    "#### Creating pair RDDs\n",
    "* Two common ways to create pair RDDs:\n",
    "    * From a list of key-value tuples\n",
    "    * From a regular RDD\n",
    "* Get the data into keyvalue form for paired RDD\n",
    "\n",
    "```\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "```\n",
    "\n",
    "#### Transformations on pair RDDs\n",
    "* All regular transformations work on pair RDDs\n",
    "* Because they contain tuples, we need to pass functions that operate on key value pairs rather than on individual elements\n",
    "* Examples of paired RDD Transformations:\n",
    "    * **`reduceByKey()`**: Group values with the same key\n",
    "    * **`groupByKey()`**: Group values with the same key\n",
    "    * **`sortByKey()`**: Return an RDD sorted by the key\n",
    "    * **`join()`**: Join two pair RDDs based on their key\n",
    "    \n",
    "#### reduceByKey() transformation\n",
    "* The most popular pair RDD transformation which combines values with the same key using a function\n",
    "* It runs parallel operations for each key in the dataset\n",
    "* `reduceByKey()` is a transformation and not an action, as datasets can have very large numbers of keys\n",
    "\n",
    "```\n",
    "regularRDD = sc.parallelize([\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
    "pairRDD_reducebykey.collect()\n",
    "```\n",
    "* result: `[('Neymar', 22), ('Ronaldo', 24), ('Messi', 47)]`\n",
    "\n",
    "#### sortByKey() transformation\n",
    "* Sorting of data is necessary for many downstream applications\n",
    "* We can sort pair RDDs as long as there is an ordering defined in the key.\n",
    "* `sortByKey()` operation orders pair RDD by key\n",
    "* It returns an RDD sorted by key in ascending or descending order\n",
    "\n",
    "```\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n",
    "```\n",
    "* result: `[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]`\n",
    "\n",
    "#### groupByKey() transformation\n",
    "* `groupByKey()` groups all the values with the same key in the pair RDD\n",
    "* If the data is already keyed in the way that we want, the `groupByKey` operation groups all the values with the same key in the pair RDD.\n",
    "\n",
    "```\n",
    "airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "```\n",
    "* result:\n",
    "\n",
    "```\n",
    "FR ['CDG']\n",
    "US ['JFK', 'SFO']\n",
    "UK ['LHR']\n",
    "```\n",
    "\n",
    "#### join() transformation\n",
    "* `join()` joins two pair RDDs based on their key \n",
    "\n",
    "```\n",
    "RDD1 = sc.parallelize([(\"Messi\", 34), (\"Ronaldo\", 32), (\"Neymar\", 24)})\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80), (\"Neymar\", 120), (\"Messi\", 100)])\n",
    "RDD1.join(RDD2).collect()\n",
    "```\n",
    "* results: `[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), (\"Messi\", (34,100))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f705d4f",
   "metadata": {},
   "source": [
    "#### reduceByKey\n",
    "\n",
    "```\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bcfa5d",
   "metadata": {},
   "source": [
    "### More actions\n",
    "\n",
    "#### reduct() action\n",
    "* `reduce(func)` action is used for aggregating the elements of a regular RDD\n",
    "* The function should be *commutative* (changing the order of the operands does not change the result) and associative\n",
    "\n",
    "```\n",
    "x = [1, 3, 4, 6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)\n",
    "```\n",
    "\n",
    "#### saveAsTextFile() action\n",
    "* In many cases, it is not advisable to run the `collect()` action on RDDs because of the huge size of the data\n",
    "* In these cases, it's common to write data out to a distributed storage system such as HDFS or Amazzon S3.\n",
    "* `saveAsTextFile()` action saves RDD into a text file inside a directory with each partition as a separate file.\n",
    "* Below is an example of `saveAsTextFile` that saves an RDD with each partition as a separate file inside a directory:\n",
    "\n",
    "```\n",
    "RDD.saveAsTextFile(\"tempFile\")\n",
    "```\n",
    "* However, you can change it to return a new RDD that is reduced into a single partition using the `coalesce()` method\n",
    "* The `coalesce()` method can be used to save RDD as a single text file.\n",
    "\n",
    "```\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c80918",
   "metadata": {},
   "source": [
    "### Action Operations on pair RDDs\n",
    "* RDD actions available for PySpark pair RDDs\n",
    "* Pair RDD actions leverage the key-value data\n",
    "* Few examples of pair RDD actions include:\n",
    "    * `countByKey()`\n",
    "    * `collectAsMap()`\n",
    "    \n",
    "#### countByKey() action\n",
    "* `countByKey()` is only available on RDDs of type (Key, Value)\n",
    "* `countByKey()` operation counts the number of elements for each key.\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "for kee, val in rdd.countByKey().items():\n",
    "    print(kee, val)\n",
    "```\n",
    "* result:\n",
    "\n",
    "```\n",
    "('a', 2)\n",
    "('b', 1)\n",
    "```\n",
    "* One thing to **note** is that `countByKey()` should only be used on a datset whose size is small enough to fit in memory\n",
    "\n",
    "#### collectAsMap() action\n",
    "* `collectAsMap()` return the key-value pairs in the RDD as a dictionary\n",
    "\n",
    "```\n",
    "sc.parallelize([(1, 2), (3,4)]).collectAsMap()\n",
    "```\n",
    "* result: `{1:2, 3:4]`\n",
    "* **Note** this actino should also only be used if the resulting data is expected to be small and all the data is loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de33202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d28cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8aeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
