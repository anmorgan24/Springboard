{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7ab30e",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. Youâ€™ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\n",
    "\n",
    "Instructor: Upendra Devisetty, Science Analyst at CyVerse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc08de",
   "metadata": {},
   "source": [
    "## $\\star$ Introduction to Big Data Analysis with Spark\n",
    "This chapter introduces the exciting world of Big Data, as well as the various concepts and different frameworks for processing Big Data. You will understand why Apache Spark is considered the best framework for BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570980ef",
   "metadata": {},
   "source": [
    "#### The 3 Vs of Big Data\n",
    "* The 3 Vs are used to describe big data's characteristics\n",
    "* **Volume:** Size of the data \n",
    "* **Variety:** Different sources and formats of data\n",
    "* **Velocity:** Speed at which the data is generated and available for processing\n",
    "\n",
    "#### Big Data concepts and Terminology\n",
    "* **Clustered computing:** collection of resources of multiple machines\n",
    "* **Parallel computing:** a type of computation in which many calculations are carried out simultaneously\n",
    "* **Distributed computing:** Collection of nodes (networked computers) that run in parallel\n",
    "* **Batch processing:** Breaking the job into small piece and running them on individual machines\n",
    "* **Real-time processing:** Immediate processing of data\n",
    "\n",
    "#### Big Data processing systems\n",
    "* **Hadoop/MapReduce:** Scalable and fault-tolerant framework; written in Java\n",
    "    * Open source\n",
    "    * Batch processing\n",
    "* **Apache Spark:** General purpose and lightning fast cluster computing system\n",
    "    * Open source\n",
    "    * Suited for both batch and real-tine data processing\n",
    "\n",
    "#### Features of Apache Spark framework\n",
    "* Distributed cluster computing framework\n",
    "* Efficient in-memory computations for large scale data sets\n",
    "* Lightning-fast data processing framework\n",
    "* Provides support for Java, Scala, Python, R, and SQL\n",
    "\n",
    "#### Spark modes of deployment\n",
    "* **Local mode:** Single machine such as your laptop\n",
    "    * Convenient for testing, debugging, and demonstration\n",
    "* **Cluster mode:** Set of pre-defined machines\n",
    "    * Good for production\n",
    "* Typical workflow: Local $\\Rightarrow$ clusters\n",
    "    * During this transition, no code change is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a322f3f",
   "metadata": {},
   "source": [
    "### PySpark: Spark with Python\n",
    "#### What is Spark shell?\n",
    "* Interactive environment for running Spark jobs\n",
    "* Helpful for fast interactive prototyping\n",
    "* Spark's shells allow interacting with data on disk or in memory across many machines or one, and Spark takes care of automatically distributing this processing\n",
    "* Three different Spark shells:\n",
    "    * Spark-shell for Scala\n",
    "    * PySpark-shell for Python\n",
    "    * SparkR for R\n",
    "    \n",
    "#### PySpark shell\n",
    "* PySpark shell is the Python-based command line tool\n",
    "* PySpark shell sllows data scientists to interface with Spark data structures\n",
    "* PySpark shell supports connecting to a cluster\n",
    "\n",
    "#### Understanding SparkContext\n",
    "* SparkContext is an entry point into the world of Spark\n",
    "* An **entry point** is where control is transferred from the Operating system to the provided program.\n",
    "    * An entry point is a way of connecting to Spark cluster\n",
    "    * An entry point is \"like a key to the house.\" \n",
    "* Access the SparkContext in the PySpark shell as a variable named `sc`\n",
    "\n",
    "#### Inspecting SparkContext\n",
    "* **Version:** to retrieve SparkContext version that you are currently running:\n",
    "    * `sc.version`\n",
    "* **Python Version:** to retrieve Python version *that SparkContext is currently using*\n",
    "    * `sc.pythonVer`\n",
    "* **Master:** URL of the cluster of \"local\" string to run in local mode of SparkContext\n",
    "    * `sc.master`\n",
    "    * If returns: `local[*]`, means SparkContext acts as a master on a local node using all available threads on the computer where it is running.\n",
    "    \n",
    "#### Loading data in PySpark\n",
    "* SparkContext's **`parallelize()`** method (used on a list)\n",
    "    * For example, to create parallelized collections holding the numbers 1 to 5:\n",
    "    * `rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "* SparkContext's **`textFile()`** method (used on a file)\n",
    "    * For example, to load a text file named `test.txt` using this method:\n",
    "    * `rdd2 = sc.textFile(\"test.txt\")`\n",
    "    \n",
    "```\n",
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create a Python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd644a",
   "metadata": {},
   "source": [
    "### Use of lambda function in python- filter()\n",
    "* Understanding PySpark becomes a lot easier if we understand functional programming principles in Python:\n",
    "    * `lambda`\n",
    "    * `map`\n",
    "    * `filter`\n",
    "* Python supports the creation of anonymous functions.\n",
    "    * **Anonymous functions** are functions that are not bound to a name at runtime, using a construct called `lambda`\n",
    "    * Lambda functions are very powerful and well-integrated into Python\n",
    "    * Lambda is especially efficient with `map()` and `filter()`\n",
    "    * Like `def`, Python creates a function to be called later in the program. However, it returns the function instead of assigning it to a name (ie **anonymous**).\n",
    "    * In practice, they are used as a way to inline a function definition, or to defer execution of a code. \n",
    "    \n",
    "#### Lambda function syntax\n",
    "* Lambda function can be used whenever function objects are required. \n",
    "* They can have any number of arguments, but only one expression, and the expression is evaluated and returned.\n",
    "* **The general syntax of the lambda function is:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b4f0c",
   "metadata": {},
   "source": [
    "**`lambda arguments: expression`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5565591",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "```\n",
    "double = lambda x: x * 2\n",
    "print(double(3))\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "g = lambda x: X**3\n",
    "print(g(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24488bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "double = lambda x: x * 2\n",
    "print(double(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c7351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "g = lambda x: x**3\n",
    "print(g(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65960802",
   "metadata": {},
   "source": [
    "* No return statement for lambda\n",
    "* Can put lambda function anywhere, without ever assigning it to a variable\n",
    "* We use lambda functions when we require a nameless function for a short period of time\n",
    "\n",
    "#### Use of Lambda function in Python - map()\n",
    "* `map()` function takes a function and a list and returns a new list which contains items returned by that function for each item\n",
    "* General syntax of `map()`: \n",
    "    * **`map(function, list)`**\n",
    "* Example of `map` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))\n",
    "```\n",
    "\n",
    "result:\n",
    "\n",
    "**`[3, 4, 5, 6]`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcff947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5451c",
   "metadata": {},
   "source": [
    "#### Use of lambda function in python- filter()\n",
    "* `filter()` function takes a function and a list and returns a new list for which the function evaluates as true\n",
    "* General syntax of filter():\n",
    "    * **`filter(function, list`**\n",
    "* Example of `filter()` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b58a3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09bba35",
   "metadata": {},
   "source": [
    "```\n",
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a859a",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter  2: Programming in PySpark RDDs\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This chapter introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d5d19",
   "metadata": {},
   "source": [
    "#### Introduction to PySpark RDD\n",
    "In this chapter, we will start working with RDDs which are Spark's core abstraction for working with data. \n",
    "\n",
    "* **RDD** = **Resilient Distributed Datasets**\n",
    "* RDDs are a collection of data distributed across the cluster\n",
    "* RDD is the fundamental and backbone data type in PySpark\n",
    "\n",
    "#### Decomposing RDDs\n",
    "* Resilient Distributed Datasets\n",
    "    * **Reslient:** ability to withstand failures\n",
    "    * **Distributed:** spanning across multiple machines\n",
    "    * **Datasets:** collection of partitioned data e.g., arrays, tables, tuples, etc. ...\n",
    "    \n",
    "#### Creating RDDs\n",
    "* Three methods for creating RDDs\n",
    "* **Parallelize:**\n",
    "    * The **simplest method** to create RDDs is to take an existing collection of objects (for example a list, array, or set) and pass it to SparkContext's parallelize method.\n",
    "* **External datasets:**\n",
    "    * A **more common** way to create RDDs is to load data from external datasets such as:\n",
    "        * files stored in HDFS\n",
    "        * Objects in Amazon S3 bucket\n",
    "        * lines in a text file\n",
    "* **From existing RDDs**\n",
    "\n",
    "#### Parallelized collection (parallelizing)\n",
    "* RDDs are created from a list or a set using the SparkContext's `parallelize` method.\n",
    "\n",
    "```\n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "type(helloRDD)\n",
    "```\n",
    "\n",
    "#### From external datasets\n",
    "* Creating RDDs from external datasets is by far the most common method in PySpark\n",
    "* `textFile()` for creating RDDs from external datasets\n",
    "* For file README stored locally:\n",
    "* `fileRDD = sc.textFile(\"README.md\")`\n",
    "* `type(fileRDD)`\n",
    "\n",
    "#### Understanding Partitioning in PySpark\n",
    "* Data partitioning is an important concept in Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfde39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335537cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f08005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcf22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
