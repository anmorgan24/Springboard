{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf4777e",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. Youâ€™ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\n",
    "\n",
    "Instructor: Upendra Devisetty, Science Analyst at CyVerse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3147d",
   "metadata": {},
   "source": [
    "## $\\star$ Introduction to Big Data Analysis with Spark\n",
    "This chapter introduces the exciting world of Big Data, as well as the various concepts and different frameworks for processing Big Data. You will understand why Apache Spark is considered the best framework for BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07e0b4",
   "metadata": {},
   "source": [
    "#### The 3 Vs of Big Data\n",
    "* The 3 Vs are used to describe big data's characteristics\n",
    "* **Volume:** Size of the data \n",
    "* **Variety:** Different sources and formats of data\n",
    "* **Velocity:** Speed at which the data is generated and available for processing\n",
    "\n",
    "#### Big Data concepts and Terminology\n",
    "* **Clustered computing:** collection of resources of multiple machines\n",
    "* **Parallel computing:** a type of computation in which many calculations are carried out simultaneously\n",
    "* **Distributed computing:** Collection of nodes (networked computers) that run in parallel\n",
    "* **Batch processing:** Breaking the job into small piece and running them on individual machines\n",
    "* **Real-time processing:** Immediate processing of data\n",
    "\n",
    "#### Big Data processing systems\n",
    "* **Hadoop/MapReduce:** Scalable and fault-tolerant framework; written in Java\n",
    "    * Open source\n",
    "    * Batch processing\n",
    "* **Apache Spark:** General purpose and lightning fast cluster computing system\n",
    "    * Open source\n",
    "    * Suited for both batch and real-tine data processing\n",
    "\n",
    "#### Features of Apache Spark framework\n",
    "* Distributed cluster computing framework\n",
    "* Efficient in-memory computations for large scale data sets\n",
    "* Lightning-fast data processing framework\n",
    "* Provides support for Java, Scala, Python, R, and SQL\n",
    "\n",
    "#### Spark modes of deployment\n",
    "* **Local mode:** Single machine such as your laptop\n",
    "    * Convenient for testing, debugging, and demonstration\n",
    "* **Cluster mode:** Set of pre-defined machines\n",
    "    * Good for production\n",
    "* Typical workflow: Local $\\Rightarrow$ clusters\n",
    "    * During this transition, no code change is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ee70c",
   "metadata": {},
   "source": [
    "### PySpark: Spark with Python\n",
    "#### What is Spark shell?\n",
    "* Interactive environment for running Spark jobs\n",
    "* Helpful for fast interactive prototyping\n",
    "* Spark's shells allow interacting with data on disk or in memory across many machines or one, and Spark takes care of automatically distributing this processing\n",
    "* Three different Spark shells:\n",
    "    * Spark-shell for Scala\n",
    "    * PySpark-shell for Python\n",
    "    * SparkR for R\n",
    "    \n",
    "#### PySpark shell\n",
    "* PySpark shell is the Python-based command line tool\n",
    "* PySpark shell sllows data scientists to interface with Spark data structures\n",
    "* PySpark shell supports connecting to a cluster\n",
    "\n",
    "#### Understanding SparkContext\n",
    "* SparkContext is an entry point into the world of Spark\n",
    "* An **entry point** is where control is transferred from the Operating system to the provided program.\n",
    "    * An entry point is a way of connecting to Spark cluster\n",
    "    * An entry point is \"like a key to the house.\" \n",
    "* Access the SparkContext in the PySpark shell as a variable named `sc`\n",
    "\n",
    "#### Inspecting SparkContext\n",
    "* **Version:** to retrieve SparkContext version that you are currently running:\n",
    "    * `sc.version`\n",
    "* **Python Version:** to retrieve Python version *that SparkContext is currently using*\n",
    "    * `sc.pythonVer`\n",
    "* **Master:** URL of the cluster of \"local\" string to run in local mode of SparkContext\n",
    "    * `sc.master`\n",
    "    * If returns: `local[*]`, means SparkContext acts as a master on a local node using all available threads on the computer where it is running.\n",
    "    \n",
    "#### Loading data in PySpark\n",
    "* SparkContext's **`parallelize()`** method (used on a list)\n",
    "    * For example, to create parallelized collections holding the numbers 1 to 5:\n",
    "    * `rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "* SparkContext's **`textFile()`** method (used on a file)\n",
    "    * For example, to load a text file named `test.txt` using this method:\n",
    "    * `rdd2 = sc.textFile(\"test.txt\")`\n",
    "    \n",
    "```\n",
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create a Python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60450b09",
   "metadata": {},
   "source": [
    "### Use of lambda function in python- filter()\n",
    "* Understanding PySpark becomes a lot easier if we understand functional programming principles in Python:\n",
    "    * `lambda`\n",
    "    * `map`\n",
    "    * `filter`\n",
    "* Python supports the creation of anonymous functions.\n",
    "    * **Anonymous functions** are functions that are not bound to a name at runtime, using a construct called `lambda`\n",
    "    * Lambda functions are very powerful and well-integrated into Python\n",
    "    * Lambda is especially efficient with `map()` and `filter()`\n",
    "    * Like `def`, Python creates a function to be called later in the program. However, it returns the function instead of assigning it to a name (ie **anonymous**).\n",
    "    * In practice, they are used as a way to inline a function definition, or to defer execution of a code. \n",
    "    \n",
    "#### Lambda function syntax\n",
    "* Lambda function can be used whenever function objects are required. \n",
    "* They can have any number of arguments, but only one expression, and the expression is evaluated and returned.\n",
    "* **The general syntax of the lambda function is:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ebd0f",
   "metadata": {},
   "source": [
    "**`lambda arguments: expression`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5b9ab",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "```\n",
    "double = lambda x: x * 2\n",
    "print(double(3))\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "g = lambda x: X**3\n",
    "print(g(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6ddd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "double = lambda x: x * 2\n",
    "print(double(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e0e156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "g = lambda x: x**3\n",
    "print(g(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f254d1",
   "metadata": {},
   "source": [
    "* No return statement for lambda\n",
    "* Can put lambda function anywhere, without ever assigning it to a variable\n",
    "* We use lambda functions when we require a nameless function for a short period of time\n",
    "\n",
    "#### Use of Lambda function in Python - map()\n",
    "* `map()` function takes a function and a list and returns a new list which contains items returned by that function for each item\n",
    "* General syntax of `map()`: \n",
    "    * **`map(function, list)`**\n",
    "* Example of `map` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))\n",
    "```\n",
    "\n",
    "result:\n",
    "\n",
    "**`[3, 4, 5, 6]`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad305fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854fa9f",
   "metadata": {},
   "source": [
    "#### Use of lambda function in python- filter()\n",
    "* `filter()` function takes a function and a list and returns a new list for which the function evaluates as true\n",
    "* General syntax of filter():\n",
    "    * **`filter(function, list`**\n",
    "* Example of `filter()` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a532beb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a873b",
   "metadata": {},
   "source": [
    "```\n",
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbfa8a",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter  2: Programming in PySpark RDDs\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This chapter introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43640fb",
   "metadata": {},
   "source": [
    "#### Introduction to PySpark RDD\n",
    "In this chapter, we will start working with RDDs which are Spark's core abstraction for working with data. \n",
    "\n",
    "* **RDD** = **Resilient Distributed Datasets**\n",
    "* RDDs are a collection of data distributed across the cluster\n",
    "* RDD is the fundamental and backbone data type in PySpark\n",
    "\n",
    "#### Decomposing RDDs\n",
    "* Resilient Distributed Datasets\n",
    "    * **Reslient:** ability to withstand failures\n",
    "    * **Distributed:** spanning across multiple machines\n",
    "    * **Datasets:** collection of partitioned data e.g., arrays, tables, tuples, etc. ...\n",
    "    \n",
    "#### Creating RDDs\n",
    "* Three methods for creating RDDs\n",
    "* **Parallelize:**\n",
    "    * The **simplest method** to create RDDs is to take an existing collection of objects (for example a list, array, or set) and pass it to SparkContext's parallelize method.\n",
    "* **External datasets:**\n",
    "    * A **more common** way to create RDDs is to load data from external datasets such as:\n",
    "        * files stored in HDFS\n",
    "        * Objects in Amazon S3 bucket\n",
    "        * lines in a text file\n",
    "* **From existing RDDs**\n",
    "\n",
    "#### Parallelized collection (parallelizing)\n",
    "* RDDs are created from a list or a set using the SparkContext's `parallelize` method.\n",
    "\n",
    "```\n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "type(helloRDD)\n",
    "```\n",
    "\n",
    "#### From external datasets\n",
    "* Creating RDDs from external datasets is by far the most common method in PySpark\n",
    "* `textFile()` for creating RDDs from external datasets\n",
    "* For file README stored locally:\n",
    "* `fileRDD = sc.textFile(\"README.md\")`\n",
    "* `type(fileRDD)`\n",
    "\n",
    "#### Understanding Partitioning in PySpark\n",
    "* Data partitioning is an important concept in Spark and understanding how Spark deals with partitions allows one to control parallelism.\n",
    "* A **partition** is a logical division of a large distributed data set. \n",
    "* By default, Spark partitions the data at the time of creating RDD based on several factors such as available resources, external datasets, etc. but this behavior can also be controlled by passing a second argument called `minPartitions`, which defines the minimum number of partitions to be created for an RDD\n",
    "* `parallelize()` method:\n",
    "    * `numRDD = sc.parallelize(range(10), minPartitions = 6)`\n",
    "* `textFile()` method:\n",
    "    * `fileRDD = sc.textFile(\"README.md\", minPartitions = 6)`\n",
    "* The number of partitions in an RDD can always be found by using the `getNumPartitions()` method\n",
    "\n",
    "```\n",
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf152efa",
   "metadata": {},
   "source": [
    "#### RDD Operations in PySpark\n",
    "* RDDs in PySpark supports two different types of operations:\n",
    "    * Transformations\n",
    "    * Actions\n",
    "* **Transformations** are operations on RDDs that return a new RDD\n",
    "    * follow lazy evaluation\n",
    "    * basic RDD transformations:\n",
    "        * `map()`\n",
    "        * `filter()`\n",
    "        * `flatMap()`\n",
    "        * `union()`\n",
    "* **Actions** are operations that perfomr some computation on the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab236edc",
   "metadata": {},
   "source": [
    "#### map() Transformation \n",
    "* The `map()` transformation applies a function to all elements in the RDD\n",
    "* Example:\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "```\n",
    "\n",
    "#### filter() Transformation\n",
    "* The `filter()` transformation takes in a function and returns an RDD that only has elements that pass the condition.\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "```\n",
    "\n",
    "#### flatmap() Transformation \n",
    "* The `flatMap()` transformation is similar to `map()` transformation, except that it returns multiple values for each element in the source RDD.\n",
    "* A simple usage of `flatMap()` is splitting up an input string into words\n",
    "* Even thought input RDD has 2 elements, for example, the output RDD now contains 5 elements:\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "```\n",
    "\n",
    "#### union() Transformation\n",
    "* The `union()` transformation returns the union of one RDD with another RDD\n",
    "* Similar to pandas' `concat()`\n",
    "\n",
    "```\n",
    "inputRDD = sc.textFile(\"logs.txt\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda c: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)\n",
    "```\n",
    "### RDD Actions\n",
    "* Operation return a value after running a computation on the RDD\n",
    "* Basic RDD Actions\n",
    "    * **`collect()`:** returns all the elements of the dataset as an array\n",
    "        * `RDD_map.collect()`\n",
    "    * **`take(n)`:** returns an array with the first N elements of the dataset\n",
    "        * `RDD_map.take(2)`\n",
    "    * **`first()`:** prints the first element of the RDD\n",
    "        * `RDD_map.first()`\n",
    "    * **`count()`:** return the number of elements in the RDD\n",
    "        * `RDD_flatmap.count()`\n",
    "        \n",
    "```\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "\tprint(numb)\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "  print(line)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe472d8c",
   "metadata": {},
   "source": [
    "#### Working with Pair RDDs in PySpark\n",
    "* Real life datasets are usually keyvaue pairs \n",
    "* Eah row is a key and maps to one or more values\n",
    "* **Pair RDD** is a special data structure to work with this kind of dataset\n",
    "* **Pair RDD:** Key is the identifier and value is data\n",
    "\n",
    "#### Creating pair RDDs\n",
    "* Two common ways to create pair RDDs:\n",
    "    * From a list of key-value tuples\n",
    "    * From a regular RDD\n",
    "* Get the data into keyvalue form for paired RDD\n",
    "\n",
    "```\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "```\n",
    "\n",
    "#### Transformations on pair RDDs\n",
    "* All regular transformations work on pair RDDs\n",
    "* Because they contain tuples, we need to pass functions that operate on key value pairs rather than on individual elements\n",
    "* Examples of paired RDD Transformations:\n",
    "    * **`reduceByKey()`**: Group values with the same key\n",
    "    * **`groupByKey()`**: Group values with the same key\n",
    "    * **`sortByKey()`**: Return an RDD sorted by the key\n",
    "    * **`join()`**: Join two pair RDDs based on their key\n",
    "    \n",
    "#### reduceByKey() transformation\n",
    "* The most popular pair RDD transformation which combines values with the same key using a function\n",
    "* It runs parallel operations for each key in the dataset\n",
    "* `reduceByKey()` is a transformation and not an action, as datasets can have very large numbers of keys\n",
    "\n",
    "```\n",
    "regularRDD = sc.parallelize([\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
    "pairRDD_reducebykey.collect()\n",
    "```\n",
    "* result: `[('Neymar', 22), ('Ronaldo', 24), ('Messi', 47)]`\n",
    "\n",
    "#### sortByKey() transformation\n",
    "* Sorting of data is necessary for many downstream applications\n",
    "* We can sort pair RDDs as long as there is an ordering defined in the key.\n",
    "* `sortByKey()` operation orders pair RDD by key\n",
    "* It returns an RDD sorted by key in ascending or descending order\n",
    "\n",
    "```\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n",
    "```\n",
    "* result: `[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]`\n",
    "\n",
    "#### groupByKey() transformation\n",
    "* `groupByKey()` groups all the values with the same key in the pair RDD\n",
    "* If the data is already keyed in the way that we want, the `groupByKey` operation groups all the values with the same key in the pair RDD.\n",
    "\n",
    "```\n",
    "airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "```\n",
    "* result:\n",
    "\n",
    "```\n",
    "FR ['CDG']\n",
    "US ['JFK', 'SFO']\n",
    "UK ['LHR']\n",
    "```\n",
    "\n",
    "#### join() transformation\n",
    "* `join()` joins two pair RDDs based on their key \n",
    "\n",
    "```\n",
    "RDD1 = sc.parallelize([(\"Messi\", 34), (\"Ronaldo\", 32), (\"Neymar\", 24)})\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80), (\"Neymar\", 120), (\"Messi\", 100)])\n",
    "RDD1.join(RDD2).collect()\n",
    "```\n",
    "* results: `[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), (\"Messi\", (34,100))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045ebe4",
   "metadata": {},
   "source": [
    "#### reduceByKey\n",
    "\n",
    "```\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dba298",
   "metadata": {},
   "source": [
    "### More actions\n",
    "\n",
    "#### reduce() action\n",
    "* `reduce(func)` action is used for aggregating the elements of a regular RDD\n",
    "* The function should be *commutative* (changing the order of the operands does not change the result) and associative\n",
    "\n",
    "```\n",
    "x = [1, 3, 4, 6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)\n",
    "```\n",
    "\n",
    "#### saveAsTextFile() action\n",
    "* In many cases, it is not advisable to run the `collect()` action on RDDs because of the huge size of the data\n",
    "* In these cases, it's common to write data out to a distributed storage system such as HDFS or Amazzon S3.\n",
    "* `saveAsTextFile()` action saves RDD into a text file inside a directory with each partition as a separate file.\n",
    "* Below is an example of `saveAsTextFile` that saves an RDD with each partition as a separate file inside a directory:\n",
    "\n",
    "```\n",
    "RDD.saveAsTextFile(\"tempFile\")\n",
    "```\n",
    "* However, you can change it to return a new RDD that is reduced into a single partition using the `coalesce()` method\n",
    "* The `coalesce()` method can be used to save RDD as a single text file.\n",
    "\n",
    "```\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3befe13",
   "metadata": {},
   "source": [
    "### Action Operations on pair RDDs\n",
    "* RDD actions available for PySpark pair RDDs\n",
    "* Pair RDD actions leverage the key-value data\n",
    "* Few examples of pair RDD actions include:\n",
    "    * `countByKey()`\n",
    "    * `collectAsMap()`\n",
    "    \n",
    "#### countByKey() action\n",
    "* `countByKey()` is only available on RDDs of type (Key, Value)\n",
    "* `countByKey()` operation counts the number of elements for each key.\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "for kee, val in rdd.countByKey().items():\n",
    "    print(kee, val)\n",
    "```\n",
    "* result:\n",
    "\n",
    "```\n",
    "('a', 2)\n",
    "('b', 1)\n",
    "```\n",
    "* One thing to **note** is that `countByKey()` should only be used on a datset whose size is small enough to fit in memory\n",
    "\n",
    "#### collectAsMap() action\n",
    "* `collectAsMap()` return the key-value pairs in the RDD as a dictionary\n",
    "\n",
    "```\n",
    "sc.parallelize([(1, 2), (3,4)]).collectAsMap()\n",
    "```\n",
    "* result: `{1:2, 3:4]`\n",
    "* **Note** this actino should also only be used if the resulting data is expected to be small and all the data is loaded into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae0245",
   "metadata": {},
   "source": [
    "#### Exercises: CountingBykeys\n",
    "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the `Rdd` that you created earlier and count the number of unique keys in that pair RDD.\n",
    "\n",
    "Remember, you already have a SparkContext `sc` and `Rdd` available in your workspace.\n",
    "\n",
    "```\n",
    "# Count the unique keys\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "```\n",
    "\n",
    "#### Exercises: Create a base RDD and transform it\n",
    "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from [Complete Works of William Shakespeare](https://www.gutenberg.org/ebooks/100).\n",
    "\n",
    "Here are the brief steps for writing the word counting program:\n",
    "* Create a base RDD from `Complete_Shakespeare.txt` file.\n",
    "* Use RDD transformation to create a long list of words from each element of the base RDD.\n",
    "* Remove stop words from your data.\n",
    "* Create pair RDD where each element is a pair tuple of `('w', 1)`\n",
    "* Group the elements of the pair RDD by key (word) and add up their values.\n",
    "* Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
    "* Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
    "\n",
    "In this first exercise, you'll create a base RDD from `Complete_Shakespeare.txt` file and transform it to create a long list of words.\n",
    "\n",
    "Remember, you already have a SparkContext `sc` already available in your workspace. A `file_path` variable (which is the path to the `Complete_Shakespeare.txt` file) is also loaded for you.\n",
    "\n",
    "```\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "```\n",
    "\n",
    "#### Exercises: Remove stop words and reduce the dataset\n",
    "After splitting the lines in the file into a long list of words in the previous exercise, in the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words. You can remove many obvious stop words with a list of your own. But for this exercise, you will just remove the stop words from a curated list `stop_words` provided to you in your environment.\n",
    "\n",
    "After removing stop words, you'll next create a pair RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, pair RDD is composed of `(w, 1)` where `w` is for each word in the RDD and 1 is a number. Finally, you'll combine the values with the same key from the pair RDD.\n",
    "\n",
    "Remember you already have a SparkContext `sc` and `splitRDD` available in your workspace.\n",
    "\n",
    "```\n",
    "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "\n",
    "#### Exercises: Print word frequencies\n",
    "After combining the values (counts) with the same key (word), in this exercise, you'll return the first 10 word frequencies. You could have retrieved all the elements at once using collect() but it is bad practice and not recommended. RDDs can be huge: you may run out of memory and crash your computer...\n",
    "\n",
    "What if we want to return the top 10 words? For this, first you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count). This way it is easy to sort the RDD based on the key rather than the key using `sortByKey `operation in PySpark. Finally, you'll return the top 10 words from the sorted RDD.\n",
    "\n",
    "You already have a SparkContext `sc` and `resultRDD` available in your workspace.\n",
    "\n",
    "```\n",
    "# Display the first 10 words and their frequencies from the input RDD\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values from the input RDD\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f810ec4",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: PySpark SQL & DataFrames\n",
    "In this chapter, you'll learn about Spark SQL which is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. This chapter shows how Spark SQL allows you to use DataFrames in Python.\n",
    "\n",
    "### Abstracting Data with DataFrames\n",
    "* **PySpark SQL** is Spark's high-level API for working with structured data.\n",
    "* **PySpark SQL** is a Spark library for structured data. It provides more information about the structure of the data and computation.\n",
    "* A **PySpark DataFrame** is an immutable distributed collection of data with named columns\n",
    "    * Designed to process a large collection of both structured (e.g. relational database) and semi-structured data (e.g. JSON: JavaScript Object Notation)\n",
    "    * Support both SQL queries (`SELECT * from table`) or expression methods (`df.select()`)\n",
    "    \n",
    "#### SparkSession- Entry point for DataFrame API\n",
    "* SparkContext is the main entry point for creating RDDs\n",
    "* SparkSession provides a single point of entry to interact with Spark DataFrames\n",
    "* The SparkSession does for DataFrames with the SparkContext does for RDDs\n",
    "* SparkSession can be used to create DataFrames, register DataFrames, execute SQL queries\n",
    "* SparkSession is available in PySpark shell as `spark`\n",
    "\n",
    "#### Creating DataFrames in PySpark\n",
    "* Two different methods of creating DataFrames in PySpark\n",
    "    * From existing RDDs using SparkSession's `createDataFrame()` method\n",
    "    * From various data sources (CSV, JSON, TXT) using SparkSession's read method\n",
    "* **Schema** controls the data and helps DataFrames to optimize queries\n",
    "* **Schema** provides informational detail such as the column name, the type of data in the column, empty values, etc. ...\n",
    "\n",
    "#### Create a DataFrame from RDD\n",
    "\n",
    "```\n",
    "iphones_RDD = sc.parallelize([\n",
    "    (\"XS\", 2018, 5.65, 2.79, 6.24),\n",
    "    (\"XR\", 2018, 5.94, 2.98, 6.84),\n",
    "    (\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "    (\"8Plus\", 2017, 6.23, 3.07, 7.12)\n",
    "])\n",
    "\n",
    "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\n",
    "\n",
    "iphones_df = spark.createDataFrame(iphones_RDD, schema = names)\n",
    "type(iphones_df)\n",
    "```\n",
    "* **NOTE:** When the schema is a list of column names, the type of each column will be inferred from data.\n",
    "    * However, when the schema is `None`, it will try to infer the schema from data.\n",
    "    \n",
    "#### Create a DataFrame from reading a CSV/JSON/TXT\n",
    "* `df_csv = spark.read.csv('people.csv', header=True, inferSchema=True)`\n",
    "* `df_json = spark.read.json(\"people.json\", header=True, inferSchema=True)`\n",
    "* `df_txt = spark.read.txt(\"people.txt\", header=True, inferSchema=True)`\n",
    "\n",
    "* Requires the path to the file and two optional parameters:\n",
    "    * `header`\n",
    "    * `inferSchema` (default is False)\n",
    "\n",
    "#### Exercises:\n",
    "```\n",
    "# Create an RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create an DataFrame from file_path\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1da4d6",
   "metadata": {},
   "source": [
    "### Interacting with PySpark DataFrames\n",
    "* Just like RDDs, DataFrames also support both transformations and actions\n",
    "\n",
    "#### DataFrame operators in PySpark\n",
    "* DataFrame operations: Transformations and Actions\n",
    "\n",
    "#### DataFrame Transformations\n",
    "* **`select('column')`** : transformation subsets one or more columns in the DataFrame\n",
    "* **`filter(df.column <= condition)`** : transformation filters out the rows based on a condition\n",
    "* **`groupby('column')`** : operation can by used to group a variable *so that we can perform aggregations on them*\n",
    "* **`orderby('column)`** : operation sorts the DataFrame based on one or more columns\n",
    "* **`dropDuplicates()`** : removes the duplicate rows of a DataFrame\n",
    "* **`withColumnRenamed('former_name', 'new_name')`** : renames a column in the DataFrame\n",
    "\n",
    "#### DataFrame actions\n",
    "* **`printSchema()`** : operation prints the types of the columns in DataFrame; see note below\n",
    "* **`head(n)`** : shows first n rows\n",
    "* **`show(n)`** : action prints first n rows in the DataFrame; default n is 20\n",
    "* **`count()`** : counts number of occurences of __\n",
    "* **`columns`** : prints the columns of a DataFrame\n",
    "* **`describe()`** : operation computes summary statistics of numerical columns in the DataFrame\n",
    "* **Correction: `printSchema()` is a method for any Spark dataset/dataframe and not an action**\n",
    "\n",
    "#### Exercises: Inspecting data in PySpark DataFrame\n",
    "\n",
    "```\n",
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e24e2c",
   "metadata": {},
   "source": [
    "#### Interacting with DataFrames using PySpark SQL\n",
    "* DataFrame API vs SQL queries\n",
    "* The DataFrames API provides a programmatic domain-specific language (DSL) for data\n",
    "* DataFrame transformations and actions (queries) are much easier to construct programmatically\n",
    "* SQL queries can be much more concise and easier to understand and portable\n",
    "\n",
    "#### Executing SQL Queries\n",
    "* The SparkSession `sql()` method executes SQL queries\n",
    "* `sql()` method takes a SQL statement as an argument and returns the result as a DataFrame representing the result of the given query.\n",
    "* To issue SQL queries against an existing DataFarme we can leverage the `createOrReplaceTempView` function to build a temporary table as shown in this example:\n",
    "* `df.createOrReplaceTempView(\"table1\")`\n",
    "* After creating the temporary table, we can simply use the `sql` method\n",
    "\n",
    "```\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT field1, field2 FROM table1\")\n",
    "df2.collect()\n",
    "```\n",
    "\n",
    "#### SQL query to extract data\n",
    "\n",
    "```\n",
    "test_df.createOrReplaceTempView(\"test_table\")\n",
    "query = '''SELECT Product_ID FROM test_table'''\n",
    "test_product_df = spark.sql(query)\n",
    "test_product_df.show(5)\n",
    "```\n",
    "* Because the result of SQL query returns a DataFrame, all the usual DataFrame operations are available. \n",
    "\n",
    "#### Exercises: Running SQL Queries Programmatically\n",
    "\n",
    "```\n",
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)\n",
    "```\n",
    "\n",
    "#### Exercises: SQL queries for filtering Table\n",
    "\n",
    "```\n",
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe1ac6",
   "metadata": {},
   "source": [
    "### Data Visualization in PySpark using DataFrames\n",
    "* Currently there are **three** different methods available for plotting graphs using PySpark DataFrames. \n",
    "    * **`Pyspark_dist_explore`** library\n",
    "    * **`toPandas()`**\n",
    "    * **`HandySpark`** library\n",
    "    \n",
    "#### Pyspark_dist_explore\n",
    "* `Pyspark_dist_explore` library provides quick insights into DataFrames\n",
    "* Currently three functions available: \n",
    "    * `hist()`\n",
    "    * `distplot()`\n",
    "    * `pandas_histogram()`\n",
    "    \n",
    "```\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "test_df_age = test_df.select('Age')\n",
    "hist(test_df_age, bins = 20, color='red')\n",
    "```\n",
    "\n",
    "#### Using toPandas() for plotting DataFrames\n",
    "* Converts the PySpark DataFame into a Pandas DataFrame\n",
    "* After conversion, it's easy to create charts from pandas DataFrames using marplotlib or seaborn plotting tools.\n",
    "\n",
    "```\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema= True)\n",
    "test_df_sample_pandas = test_df.toPandas()\n",
    "test_df_sample_pandas.hist('Age')\n",
    "```\n",
    "\n",
    "#### Pandas DataFrame vs PySpark DataFrame\n",
    "* Pandas DataFrames are in-memory, single-server based structions and operation on PySpark run in parallel\n",
    "* Pandas DataFrames are limited by your server memory, and you will process them with the power of a single server.\n",
    "* In contrast, operations on PySpark DataFrames run parallel on different nodes in the cluster\n",
    "* The result is generated as we apply any operation in Pandas whereas operations in PySpark DataFrame are lazy evaluation\n",
    "* Pandas DataFrame is mutable and PySpark DataFrames are immutable\n",
    "* Pandas API supports more operations than PySpark DataFrame API\n",
    "\n",
    "#### HandySpark method of visualization\n",
    "* Relatively new package\n",
    "* Designed to improve PySpark user experience, especially when with EDA\n",
    "* It makes fetching data or computing statistics for columns really easy, returning pandas objects straight away\n",
    "* It brings the long-missing capability of plotting data while retaining the advantage of performing the distributed computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4dbd2b",
   "metadata": {},
   "source": [
    "#### Exercises: PySpark DataFrame visualization \n",
    "\n",
    "```\n",
    "\n",
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Exercises: Part 1: Create a DataFrame from CSV file\n",
    "\n",
    "```\n",
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Show the first 10 observations\n",
    "fifa_df.show(10)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))\n",
    "```\n",
    "\n",
    "#### Exercises: Part 2: SQL Queries on DataFrame\n",
    "\n",
    "```\n",
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "fifa_df_germany_age.describe().show()\n",
    "```\n",
    "\n",
    "#### Exercises: Part 3: Data visualization\n",
    "\n",
    "```\n",
    "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\n",
    "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
    "\n",
    "# Plot the 'Age' density of Germany Players\n",
    "fifa_df_germany_age_pandas.plot(kind='density')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "# $\\star$ Machine Learning with PySpark MLlib\n",
    "PySpark MLlib is the Apache Spark scalable machine learning library in Python consisting of common learning algorithms and utilities. Throughout this last chapter, you'll learn important Machine Learning algorithms. You will build a movie recommendation engine and a spam filter, and use k-means clustering.\n",
    "\n",
    "### Overview of PySpark MLlib\n",
    "* PySpark MLlib is a component of Apache Spark for machine learning with the goal of making practical ML scalable and easy.\n",
    "* Provides various tools including:\n",
    "    * **ML Algorithms:** collaborative filtering, classification, and clustering\n",
    "    * **Featurization:** feature extraction, transformation, dimensionality reduction, and selection\n",
    "    * **Pipelines:** tools for constructing, evaluating, and tuning ML Pipelines\n",
    "    \n",
    "#### Why PySpark MLlib?\n",
    "* sklearn algorithms only work for a small datasets on a single machine\n",
    "* PySpark MLlib algorithms are designed for parallel processing on a cluster\n",
    "* MLlib supports languages such as Scala, Java, and R\n",
    "* Provides a high-level API to build ML pipelines\n",
    "\n",
    "#### PySpark MLlib Algorithms\n",
    "* **Classification (Binary and Multiclass) and Regression:** Linear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes, linear least squares, Lasso, ridge regression, isotonic regression\n",
    "* **Collaborative filtering:** Alternating least squares (ALS)\n",
    "* **Clustering:** K-means, Gaussian mixture, Biseting K-means and Streaming K-Means\n",
    "\n",
    "#### The three C's of ML in PySpark MLlib\n",
    "* **Collaborative filtering (recommender engines):** Produce recommendations\n",
    "* **Classification:** Identifying to which a set of categories a new observation\n",
    "* **Clustering:** Groups data based on similar characteristics\n",
    "\n",
    "#### PySpark MLlib imports\n",
    "* `pyspark.mllib.recommendation`\n",
    "* `from pyspark.mllib.recommendation import ALS`\n",
    "***\n",
    "* `pyspark.mllib.classification`\n",
    "* `from pyspark.mllib.classification import LogisticRegressionWithLBRGS`\n",
    "***\n",
    "* `pyspark.mllib.clustering`\n",
    "* `from pyspark.mllib.clustering import KMeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be8e20",
   "metadata": {},
   "source": [
    "#### Exercises: PySpark MLlib algorithms\n",
    "```\n",
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9869282",
   "metadata": {},
   "source": [
    "### Collaborative filtering\n",
    "* **Collaborative filtering** is a method of making automatic predictions about the interests of a user by collecting preferences or taste information from many users\n",
    "* **Collaborative filtering** is finding users that share common interests\n",
    "* Collaborative filtering is one of the most commonly used algorithms for recommender systems\n",
    "* Two approaches:\n",
    "    * **The User-User Approach**: Finds users that are similar to the target user\n",
    "    * **The Item-Item Approach**: Finds and recommends items that are similar to items with the target user\n",
    "    \n",
    "#### Rating class in pyspark.mllib.recommendation submodule\n",
    "* The Rating class is a wrapper around a tuple (user, product, and rating)\n",
    "* Useful for parsing the RDD and creating a tuple of user, product, and rating\n",
    "\n",
    "```\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "r = Rating(user = 1, product = 2, rating = 5.0)\n",
    "(r[0], r[1], r[2])\n",
    "```\n",
    "* result: `(1, 2, 5.0)`\n",
    "\n",
    "#### Splitting the data using randomSplit()\n",
    "* Splitting data into training and testing sets is important for evaluating predictive modeling\n",
    "* Typically a large portion of data is assigned to training compared to testing data (70-30 or 80-20 split)\n",
    "* PySpark's `randomSplit()` method randomly splits with the provided weights and returns multiple RDDs\n",
    "\n",
    "```\n",
    "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "training, test = data.randomSplit([0.6, 0.4])\n",
    "training.collect()\n",
    "test.collect()\n",
    "```\n",
    "result:\n",
    "\n",
    "```\n",
    "[1, 2, 5, 6, 9, 10]\n",
    "[3, 4, 7, 8]\n",
    "```\n",
    "\n",
    "#### Alternating Least Squares (ALS)\n",
    "* Alternating Least Squares (ALS) algorithm in `spark.mllib` provides collaborative filtering \n",
    "* `ALS.train(ratings, rank, iterations)`\n",
    "\n",
    "```\n",
    "r1 = Rating(1, 1, 1.0)\n",
    "r2 = Rating(1, 2, 2.0)\n",
    "r3 = Rating(2, 1, 2.0)\n",
    "ratings = sc.parallelize([r1, r2, r3])\n",
    "ratings.collect()\n",
    "```\n",
    "returns:\n",
    "\n",
    "```\n",
    "[Rating(user=1, product=1, rating=1.0),\n",
    " Rating(user=1, product=2, rating=2.0),\n",
    " Rating(user=2, product=1, rating=2.0)]\n",
    "```\n",
    "* `model = ALS.train(ratings, rank=10, iterations=10)`\n",
    "\n",
    "#### predictAll() - Returns RDD of Rating Objects\n",
    "* **After training the model, the next step is predicting the ratings for the user and product pairs.**\n",
    "* The method takes in an RDD without ratings to generate the ratings\n",
    "* Below we create an RDD from a list of tuples containing `userId` and `productId` using Spark Context's parallelize method. \n",
    "* `unrated_RDD = sc.parallelize([(1,2), (1,1)])`\n",
    "\n",
    "```\n",
    "predictions = model.predictAll(unrated_RDD)\n",
    "predictions.collect()\n",
    "```\n",
    "returns:\n",
    "\n",
    "```\n",
    "[Rating(user=1, product=1, rating=1.0000278574351853),\n",
    " Rating(user=1, product=2, rating=1.9890355703778122)]\n",
    "```\n",
    "\n",
    "#### Model evaluation using MSE\n",
    "* The MSE measures the average of the squares of the errors between what is estimated and the existing data.\n",
    "* The MSE is the average value of the square of `(actual rating - predicted rating)`\n",
    "* We'll first organize our ratings and prediction data to make (user, product), the rating.\n",
    "\n",
    "```\n",
    "rates = ratings.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "rates.collect()\n",
    "```\n",
    "returns: `[((1, 1), 1.0), ((1, 2), 2.0), ((2,1), 2.0)]`\n",
    "\n",
    "* Next we will join the ratings RDD with the prediction RDD and the results looks as follows:\n",
    "\n",
    "```\n",
    "preds = predictions.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "preds.collect()\n",
    "```\n",
    "returns: `[((1,1), 1.0000278574351853), ((1,2), 1.9890355703778122)]`\n",
    "\n",
    "* Next, we will join the ratings RDD with the prediction RD and the results will look as follows:\n",
    "\n",
    "```\n",
    "rates_preds = rates.join(preds)\n",
    "rates_preds.collect()\n",
    "```\n",
    "returns: `[((1,2),(2.0, 1.9890355703778122)), ((1,1), (1.0, 1.0000278574351853))]`\n",
    "* Finally, we apply a squared difference function to the map\n",
    "* `MSE = rates_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()`\n",
    "\n",
    "#### Exercises: Loading Movie Lens dataset into RDDs\n",
    "* Dataset: [MovieLens 100k dataset](https://grouplens.org/datasets/movielens/100k/)\n",
    "\n",
    "```\n",
    "# Load the data into RDD\n",
    "data = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD\n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])\n",
    "```\n",
    "\n",
    "#### Model training and predictions\n",
    "\n",
    "```\n",
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Return the first 2 rows of the RDD\n",
    "predictions.take(2)\n",
    "```\n",
    "\n",
    "#### Exercises: Model evaluation using MSE\n",
    "\n",
    "```\n",
    "# Prepare ratings data\n",
    "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "\n",
    "# Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f44d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
