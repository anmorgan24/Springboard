{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e99bb1a",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. Youâ€™ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\n",
    "\n",
    "Instructor: Upendra Devisetty, Science Analyst at CyVerse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fab818",
   "metadata": {},
   "source": [
    "## $\\star$ Introduction to Big Data Analysis with Spark\n",
    "This chapter introduces the exciting world of Big Data, as well as the various concepts and different frameworks for processing Big Data. You will understand why Apache Spark is considered the best framework for BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b68fb9",
   "metadata": {},
   "source": [
    "#### The 3 Vs of Big Data\n",
    "* The 3 Vs are used to describe big data's characteristics\n",
    "* **Volume:** Size of the data \n",
    "* **Variety:** Different sources and formats of data\n",
    "* **Velocity:** Speed at which the data is generated and available for processing\n",
    "\n",
    "#### Big Data concepts and Terminology\n",
    "* **Clustered computing:** collection of resources of multiple machines\n",
    "* **Parallel computing:** a type of computation in which many calculations are carried out simultaneously\n",
    "* **Distributed computing:** Collection of nodes (networked computers) that run in parallel\n",
    "* **Batch processing:** Breaking the job into small piece and running them on individual machines\n",
    "* **Real-time processing:** Immediate processing of data\n",
    "\n",
    "#### Big Data processing systems\n",
    "* **Hadoop/MapReduce:** Scalable and fault-tolerant framework; written in Java\n",
    "    * Open source\n",
    "    * Batch processing\n",
    "* **Apache Spark:** General purpose and lightning fast cluster computing system\n",
    "    * Open source\n",
    "    * Suited for both batch and real-tine data processing\n",
    "\n",
    "#### Features of Apache Spark framework\n",
    "* Distributed cluster computing framework\n",
    "* Efficient in-memory computations for large scale data sets\n",
    "* Lightning-fast data processing framework\n",
    "* Provides support for Java, Scala, Python, R, and SQL\n",
    "\n",
    "#### Spark modes of deployment\n",
    "* **Local mode:** Single machine such as your laptop\n",
    "    * Convenient for testing, debugging, and demonstration\n",
    "* **Cluster mode:** Set of pre-defined machines\n",
    "    * Good for production\n",
    "* Typical workflow: Local $\\Rightarrow$ clusters\n",
    "    * During this transition, no code change is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835aea25",
   "metadata": {},
   "source": [
    "### PySpark: Spark with Python\n",
    "#### What is Spark shell?\n",
    "* Interactive environment for running Spark jobs\n",
    "* Helpful for fast interactive prototyping\n",
    "* Spark's shells allow interacting with data on disk or in memory across many machines or one, and Spark takes care of automatically distributing this processing\n",
    "* Three different Spark shells:\n",
    "    * Spark-shell for Scala\n",
    "    * PySpark-shell for Python\n",
    "    * SparkR for R\n",
    "    \n",
    "#### PySpark shell\n",
    "* PySpark shell is the Python-based command line tool\n",
    "* PySpark shell sllows data scientists to interface with Spark data structures\n",
    "* PySpark shell supports connecting to a cluster\n",
    "\n",
    "#### Understanding SparkContext\n",
    "* SparkContext is an entry point into the world of Spark\n",
    "* An **entry point** is where control is transferred from the Operating system to the provided program.\n",
    "    * An entry point is a way of connecting to Spark cluster\n",
    "    * An entry point is \"like a key to the house.\" \n",
    "* Access the SparkContext in the PySpark shell as a variable named `sc`\n",
    "\n",
    "#### Inspecting SparkContext\n",
    "* **Version:** to retrieve SparkContext version that you are currently running:\n",
    "    * `sc.version`\n",
    "* **Python Version:** to retrieve Python version *that SparkContext is currently using*\n",
    "    * `sc.pythonVer`\n",
    "* **Master:** URL of the cluster of \"local\" string to run in local mode of SparkContext\n",
    "    * `sc.master`\n",
    "    * If returns: `local[*]`, means SparkContext acts as a master on a local node using all available threads on the computer where it is running.\n",
    "    \n",
    "#### Loading data in PySpark\n",
    "* SparkContext's **`parallelize()`** method (used on a list)\n",
    "    * For example, to create parallelized collections holding the numbers 1 to 5:\n",
    "    * `rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "* SparkContext's **`textFile()`** method (used on a file)\n",
    "    * For example, to load a text file named `test.txt` using this method:\n",
    "    * `rdd2 = sc.textFile(\"test.txt\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba72751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b74ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da100caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd381b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d864f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
