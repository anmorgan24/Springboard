{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e62311f",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. Youâ€™ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\n",
    "\n",
    "Instructor: Upendra Devisetty, Science Analyst at CyVerse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca23e7",
   "metadata": {},
   "source": [
    "## $\\star$ Introduction to Big Data Analysis with Spark\n",
    "This chapter introduces the exciting world of Big Data, as well as the various concepts and different frameworks for processing Big Data. You will understand why Apache Spark is considered the best framework for BigData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c0295",
   "metadata": {},
   "source": [
    "#### The 3 Vs of Big Data\n",
    "* The 3 Vs are used to describe big data's characteristics\n",
    "* **Volume:** Size of the data \n",
    "* **Variety:** Different sources and formats of data\n",
    "* **Velocity:** Speed at which the data is generated and available for processing\n",
    "\n",
    "#### Big Data concepts and Terminology\n",
    "* **Clustered computing:** collection of resources of multiple machines\n",
    "* **Parallel computing:** a type of computation in which many calculations are carried out simultaneously\n",
    "* **Distributed computing:** Collection of nodes (networked computers) that run in parallel\n",
    "* **Batch processing:** Breaking the job into small piece and running them on individual machines\n",
    "* **Real-time processing:** Immediate processing of data\n",
    "\n",
    "#### Big Data processing systems\n",
    "* **Hadoop/MapReduce:** Scalable and fault-tolerant framework; written in Java\n",
    "    * Open source\n",
    "    * Batch processing\n",
    "* **Apache Spark:** General purpose and lightning fast cluster computing system\n",
    "    * Open source\n",
    "    * Suited for both batch and real-tine data processing\n",
    "\n",
    "#### Features of Apache Spark framework\n",
    "* Distributed cluster computing framework\n",
    "* Efficient in-memory computations for large scale data sets\n",
    "* Lightning-fast data processing framework\n",
    "* Provides support for Java, Scala, Python, R, and SQL\n",
    "\n",
    "#### Spark modes of deployment\n",
    "* **Local mode:** Single machine such as your laptop\n",
    "    * Convenient for testing, debugging, and demonstration\n",
    "* **Cluster mode:** Set of pre-defined machines\n",
    "    * Good for production\n",
    "* Typical workflow: Local $\\Rightarrow$ clusters\n",
    "    * During this transition, no code change is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14235081",
   "metadata": {},
   "source": [
    "### PySpark: Spark with Python\n",
    "#### What is Spark shell?\n",
    "* Interactive environment for running Spark jobs\n",
    "* Helpful for fast interactive prototyping\n",
    "* Spark's shells allow interacting with data on disk or in memory across many machines or one, and Spark takes care of automatically distributing this processing\n",
    "* Three different Spark shells:\n",
    "    * Spark-shell for Scala\n",
    "    * PySpark-shell for Python\n",
    "    * SparkR for R\n",
    "    \n",
    "#### PySpark shell\n",
    "* PySpark shell is the Python-based command line tool\n",
    "* PySpark shell sllows data scientists to interface with Spark data structures\n",
    "* PySpark shell supports connecting to a cluster\n",
    "\n",
    "#### Understanding SparkContext\n",
    "* SparkContext is an entry point into the world of Spark\n",
    "* An **entry point** is where control is transferred from the Operating system to the provided program.\n",
    "    * An entry point is a way of connecting to Spark cluster\n",
    "    * An entry point is \"like a key to the house.\" \n",
    "* Access the SparkContext in the PySpark shell as a variable named `sc`\n",
    "\n",
    "#### Inspecting SparkContext\n",
    "* **Version:** to retrieve SparkContext version that you are currently running:\n",
    "    * `sc.version`\n",
    "* **Python Version:** to retrieve Python version *that SparkContext is currently using*\n",
    "    * `sc.pythonVer`\n",
    "* **Master:** URL of the cluster of \"local\" string to run in local mode of SparkContext\n",
    "    * `sc.master`\n",
    "    * If returns: `local[*]`, means SparkContext acts as a master on a local node using all available threads on the computer where it is running.\n",
    "    \n",
    "#### Loading data in PySpark\n",
    "* SparkContext's **`parallelize()`** method (used on a list)\n",
    "    * For example, to create parallelized collections holding the numbers 1 to 5:\n",
    "    * `rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "* SparkContext's **`textFile()`** method (used on a file)\n",
    "    * For example, to load a text file named `test.txt` using this method:\n",
    "    * `rdd2 = sc.textFile(\"test.txt\")`\n",
    "    \n",
    "```\n",
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create a Python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076719f7",
   "metadata": {},
   "source": [
    "### Use of lambda function in python- filter()\n",
    "* Understanding PySpark becomes a lot easier if we understand functional programming principles in Python:\n",
    "    * `lambda`\n",
    "    * `map`\n",
    "    * `filter`\n",
    "* Python supports the creation of anonymous functions.\n",
    "    * **Anonymous functions** are functions that are not bound to a name at runtime, using a construct called `lambda`\n",
    "    * Lambda functions are very powerful and well-integrated into Python\n",
    "    * Lambda is especially efficient with `map()` and `filter()`\n",
    "    * Like `def`, Python creates a function to be called later in the program. However, it returns the function instead of assigning it to a name (ie **anonymous**).\n",
    "    * In practice, they are used as a way to inline a function definition, or to defer execution of a code. \n",
    "    \n",
    "#### Lambda function syntax\n",
    "* Lambda function can be used whenever function objects are required. \n",
    "* They can have any number of arguments, but only one expression, and the expression is evaluated and returned.\n",
    "* **The general syntax of the lambda function is:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa12e76",
   "metadata": {},
   "source": [
    "**`lambda arguments: expression`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12dcde0",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "```\n",
    "double = lambda x: x * 2\n",
    "print(double(3))\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "g = lambda x: X**3\n",
    "print(g(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e35c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "double = lambda x: x * 2\n",
    "print(double(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ff6fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "g = lambda x: x**3\n",
    "print(g(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1a246",
   "metadata": {},
   "source": [
    "* No return statement for lambda\n",
    "* Can put lambda function anywhere, without ever assigning it to a variable\n",
    "* We use lambda functions when we require a nameless function for a short period of time\n",
    "\n",
    "#### Use of Lambda function in Python - map()\n",
    "* `map()` function takes a function and a list and returns a new list which contains items returned by that function for each item\n",
    "* General syntax of `map()`: \n",
    "    * **`map(function, list)`**\n",
    "* Example of `map` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))\n",
    "```\n",
    "\n",
    "result:\n",
    "\n",
    "**`[3, 4, 5, 6]`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbd4908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x + 2, items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bffe5c",
   "metadata": {},
   "source": [
    "#### Use of lambda function in python- filter()\n",
    "* `filter()` function takes a function and a list and returns a new list for which the function evaluates as true\n",
    "* General syntax of filter():\n",
    "    * **`filter(function, list`**\n",
    "* Example of `filter()` with `lambda`:\n",
    "\n",
    "```\n",
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14d9e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: (x%2 != 0), items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c8260",
   "metadata": {},
   "source": [
    "```\n",
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24593bc4",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter  2: Programming in PySpark RDDs\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This chapter introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e0792",
   "metadata": {},
   "source": [
    "#### Introduction to PySpark RDD\n",
    "In this chapter, we will start working with RDDs which are Spark's core abstraction for working with data. \n",
    "\n",
    "* **RDD** = **Resilient Distributed Datasets**\n",
    "* RDDs are a collection of data distributed across the cluster\n",
    "* RDD is the fundamental and backbone data type in PySpark\n",
    "\n",
    "#### Decomposing RDDs\n",
    "* Resilient Distributed Datasets\n",
    "    * **Reslient:** ability to withstand failures\n",
    "    * **Distributed:** spanning across multiple machines\n",
    "    * **Datasets:** collection of partitioned data e.g., arrays, tables, tuples, etc. ...\n",
    "    \n",
    "#### Creating RDDs\n",
    "* Three methods for creating RDDs\n",
    "* **Parallelize:**\n",
    "    * The **simplest method** to create RDDs is to take an existing collection of objects (for example a list, array, or set) and pass it to SparkContext's parallelize method.\n",
    "* **External datasets:**\n",
    "    * A **more common** way to create RDDs is to load data from external datasets such as:\n",
    "        * files stored in HDFS\n",
    "        * Objects in Amazon S3 bucket\n",
    "        * lines in a text file\n",
    "* **From existing RDDs**\n",
    "\n",
    "#### Parallelized collection (parallelizing)\n",
    "* RDDs are created from a list or a set using the SparkContext's `parallelize` method.\n",
    "\n",
    "```\n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "type(helloRDD)\n",
    "```\n",
    "\n",
    "#### From external datasets\n",
    "* Creating RDDs from external datasets is by far the most common method in PySpark\n",
    "* `textFile()` for creating RDDs from external datasets\n",
    "* For file README stored locally:\n",
    "* `fileRDD = sc.textFile(\"README.md\")`\n",
    "* `type(fileRDD)`\n",
    "\n",
    "#### Understanding Partitioning in PySpark\n",
    "* Data partitioning is an important concept in Spark and understanding how Spark deals with partitions allows one to control parallelism.\n",
    "* A **partition** is a logical division of a large distributed data set. \n",
    "* By default, Spark partitions the data at the time of creating RDD based on several factors such as available resources, external datasets, etc. but this behavior can also be controlled by passing a second argument called `minPartitions`, which defines the minimum number of partitions to be created for an RDD\n",
    "* `parallelize()` method:\n",
    "    * `numRDD = sc.parallelize(range(10), minPartitions = 6)`\n",
    "* `textFile()` method:\n",
    "    * `fileRDD = sc.textFile(\"README.md\", minPartitions = 6)`\n",
    "* The number of partitions in an RDD can always be found by using the `getNumPartitions()` method\n",
    "\n",
    "```\n",
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143159e",
   "metadata": {},
   "source": [
    "#### RDD Operations in PySpark\n",
    "* RDDs in PySpark supports two different types of operations:\n",
    "    * Transformations\n",
    "    * Actions\n",
    "* **Transformations** are operations on RDDs that return a new RDD\n",
    "    * follow lazy evaluation\n",
    "    * basic RDD transformations:\n",
    "        * `map()`\n",
    "        * `filter()`\n",
    "        * `flatMap()`\n",
    "        * `union()`\n",
    "* **Actions** are operations that perfomr some computation on the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17709db7",
   "metadata": {},
   "source": [
    "#### map() Transformation \n",
    "* The `map()` transformation applies a function to all elements in the RDD\n",
    "* Example:\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "```\n",
    "\n",
    "#### filter() Transformation\n",
    "* The `filter()` transformation takes in a function and returns an RDD that only has elements that pass the condition.\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "```\n",
    "\n",
    "#### flatmap() Transformation \n",
    "* The `flatMap()` transformation is similar to `map()` transformation, except that it returns multiple values for each element in the source RDD.\n",
    "* A simple usage of `flatMap()` is splitting up an input string into words\n",
    "* Even thought input RDD has 2 elements, for example, the output RDD now contains 5 elements:\n",
    "\n",
    "```\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "```\n",
    "\n",
    "#### union() Transformation\n",
    "* The `union()` transformation returns the union of one RDD with another RDD\n",
    "* Similar to pandas' `concat()`\n",
    "\n",
    "```\n",
    "inputRDD = sc.textFile(\"logs.txt\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda c: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)\n",
    "```\n",
    "### RDD Actions\n",
    "* Operation return a value after running a computation on the RDD\n",
    "* Basic RDD Actions\n",
    "    * **`collect()`:** returns all the elements of the dataset as an array\n",
    "        * `RDD_map.collect()`\n",
    "    * **`take(n)`:** returns an array with the first N elements of the dataset\n",
    "        * `RDD_map.take(2)`\n",
    "    * **`first()`:** prints the first element of the RDD\n",
    "        * `RDD_map.first()`\n",
    "    * **`count()`:** return the number of elements in the RDD\n",
    "        * `RDD_flatmap.count()`\n",
    "        \n",
    "```\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "\tprint(numb)\n",
    "```\n",
    "*** \n",
    "\n",
    "```\n",
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "  print(line)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c8a20",
   "metadata": {},
   "source": [
    "#### Working with Pair RDDs in PySpark\n",
    "* Real life datasets are usually keyvaue pairs \n",
    "* Eah row is a key and maps to one or more values\n",
    "* **Pair RDD** is a special data structure to work with this kind of dataset\n",
    "* **Pair RDD:** Key is the identifier and value is data\n",
    "\n",
    "#### Creating pair RDDs\n",
    "* Two common ways to create pair RDDs:\n",
    "    * From a list of key-value tuples\n",
    "    * From a regular RDD\n",
    "* Get the data into keyvalue form for paired RDD\n",
    "\n",
    "```\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "```\n",
    "\n",
    "#### Transformations on pair RDDs\n",
    "* All regular transformations work on pair RDDs\n",
    "* Because they contain tuples, we need to pass functions that operate on key value pairs rather than on individual elements\n",
    "* Examples of paired RDD Transformations:\n",
    "    * **`reduceByKey()`**: Group values with the same key\n",
    "    * **`groupByKey()`**: Group values with the same key\n",
    "    * **`sortByKey()`**: Return an RDD sorted by the key\n",
    "    * **`join()`**: Join two pair RDDs based on their key\n",
    "    \n",
    "#### reduceByKey() transformation\n",
    "* The most popular pair RDD transformation which combines values with the same key using a function\n",
    "* It runs parallel operations for each key in the dataset\n",
    "* `reduceByKey()` is a transformation and not an action, as datasets can have very large numbers of keys\n",
    "\n",
    "```\n",
    "regularRDD = sc.parallelize([\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)])\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
    "pairRDD_reducebykey.collect()\n",
    "```\n",
    "* result: `[('Neymar', 22), ('Ronaldo', 24), ('Messi', 47)]`\n",
    "\n",
    "#### sortByKey() transformation\n",
    "* Sorting of data is necessary for many downstream applications\n",
    "* We can sort pair RDDs as long as there is an ordering defined in the key.\n",
    "* `sortByKey()` operation orders pair RDD by key\n",
    "* It returns an RDD sorted by key in ascending or descending order\n",
    "\n",
    "```\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n",
    "```\n",
    "* result: `[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]`\n",
    "\n",
    "#### groupByKey() transformation\n",
    "* `groupByKey()` groups all the values with the same key in the pair RDD\n",
    "* If the data is already keyed in the way that we want, the `groupByKey` operation groups all the values with the same key in the pair RDD.\n",
    "\n",
    "```\n",
    "airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "```\n",
    "* result:\n",
    "\n",
    "```\n",
    "FR ['CDG']\n",
    "US ['JFK', 'SFO']\n",
    "UK ['LHR']\n",
    "```\n",
    "\n",
    "#### join() transformation\n",
    "* `join()` joins two pair RDDs based on their key \n",
    "\n",
    "```\n",
    "RDD1 = sc.parallelize([(\"Messi\", 34), (\"Ronaldo\", 32), (\"Neymar\", 24)})\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80), (\"Neymar\", 120), (\"Messi\", 100)])\n",
    "RDD1.join(RDD2).collect()\n",
    "```\n",
    "* results: `[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), (\"Messi\", (34,100))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25fd4b",
   "metadata": {},
   "source": [
    "#### reduceByKey\n",
    "\n",
    "```\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd82ede",
   "metadata": {},
   "source": [
    "### More actions\n",
    "\n",
    "#### reduce() action\n",
    "* `reduce(func)` action is used for aggregating the elements of a regular RDD\n",
    "* The function should be *commutative* (changing the order of the operands does not change the result) and associative\n",
    "\n",
    "```\n",
    "x = [1, 3, 4, 6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)\n",
    "```\n",
    "\n",
    "#### saveAsTextFile() action\n",
    "* In many cases, it is not advisable to run the `collect()` action on RDDs because of the huge size of the data\n",
    "* In these cases, it's common to write data out to a distributed storage system such as HDFS or Amazzon S3.\n",
    "* `saveAsTextFile()` action saves RDD into a text file inside a directory with each partition as a separate file.\n",
    "* Below is an example of `saveAsTextFile` that saves an RDD with each partition as a separate file inside a directory:\n",
    "\n",
    "```\n",
    "RDD.saveAsTextFile(\"tempFile\")\n",
    "```\n",
    "* However, you can change it to return a new RDD that is reduced into a single partition using the `coalesce()` method\n",
    "* The `coalesce()` method can be used to save RDD as a single text file.\n",
    "\n",
    "```\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e9c8a",
   "metadata": {},
   "source": [
    "### Action Operations on pair RDDs\n",
    "* RDD actions available for PySpark pair RDDs\n",
    "* Pair RDD actions leverage the key-value data\n",
    "* Few examples of pair RDD actions include:\n",
    "    * `countByKey()`\n",
    "    * `collectAsMap()`\n",
    "    \n",
    "#### countByKey() action\n",
    "* `countByKey()` is only available on RDDs of type (Key, Value)\n",
    "* `countByKey()` operation counts the number of elements for each key.\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "for kee, val in rdd.countByKey().items():\n",
    "    print(kee, val)\n",
    "```\n",
    "* result:\n",
    "\n",
    "```\n",
    "('a', 2)\n",
    "('b', 1)\n",
    "```\n",
    "* One thing to **note** is that `countByKey()` should only be used on a datset whose size is small enough to fit in memory\n",
    "\n",
    "#### collectAsMap() action\n",
    "* `collectAsMap()` return the key-value pairs in the RDD as a dictionary\n",
    "\n",
    "```\n",
    "sc.parallelize([(1, 2), (3,4)]).collectAsMap()\n",
    "```\n",
    "* result: `{1:2, 3:4]`\n",
    "* **Note** this actino should also only be used if the resulting data is expected to be small and all the data is loaded into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f0467",
   "metadata": {},
   "source": [
    "#### Exercises: CountingBykeys\n",
    "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the `Rdd` that you created earlier and count the number of unique keys in that pair RDD.\n",
    "\n",
    "Remember, you already have a SparkContext `sc` and `Rdd` available in your workspace.\n",
    "\n",
    "```\n",
    "# Count the unique keys\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "```\n",
    "\n",
    "#### Exercises: Create a base RDD and transform it\n",
    "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from [Complete Works of William Shakespeare](https://www.gutenberg.org/ebooks/100).\n",
    "\n",
    "Here are the brief steps for writing the word counting program:\n",
    "* Create a base RDD from `Complete_Shakespeare.txt` file.\n",
    "* Use RDD transformation to create a long list of words from each element of the base RDD.\n",
    "* Remove stop words from your data.\n",
    "* Create pair RDD where each element is a pair tuple of `('w', 1)`\n",
    "* Group the elements of the pair RDD by key (word) and add up their values.\n",
    "* Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
    "* Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
    "\n",
    "In this first exercise, you'll create a base RDD from `Complete_Shakespeare.txt` file and transform it to create a long list of words.\n",
    "\n",
    "Remember, you already have a SparkContext `sc` already available in your workspace. A `file_path` variable (which is the path to the `Complete_Shakespeare.txt` file) is also loaded for you.\n",
    "\n",
    "```\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "```\n",
    "\n",
    "#### Exercises: Remove stop words and reduce the dataset\n",
    "After splitting the lines in the file into a long list of words in the previous exercise, in the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words. You can remove many obvious stop words with a list of your own. But for this exercise, you will just remove the stop words from a curated list `stop_words` provided to you in your environment.\n",
    "\n",
    "After removing stop words, you'll next create a pair RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, pair RDD is composed of `(w, 1)` where `w` is for each word in the RDD and 1 is a number. Finally, you'll combine the values with the same key from the pair RDD.\n",
    "\n",
    "Remember you already have a SparkContext `sc` and `splitRDD` available in your workspace.\n",
    "\n",
    "```\n",
    "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "\n",
    "#### Exercises: Print word frequencies\n",
    "After combining the values (counts) with the same key (word), in this exercise, you'll return the first 10 word frequencies. You could have retrieved all the elements at once using collect() but it is bad practice and not recommended. RDDs can be huge: you may run out of memory and crash your computer...\n",
    "\n",
    "What if we want to return the top 10 words? For this, first you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count). This way it is easy to sort the RDD based on the key rather than the key using `sortByKey `operation in PySpark. Finally, you'll return the top 10 words from the sorted RDD.\n",
    "\n",
    "You already have a SparkContext `sc` and `resultRDD` available in your workspace.\n",
    "\n",
    "```\n",
    "# Display the first 10 words and their frequencies from the input RDD\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values from the input RDD\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4162b",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: PySpark SQL & DataFrames\n",
    "In this chapter, you'll learn about Spark SQL which is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. This chapter shows how Spark SQL allows you to use DataFrames in Python.\n",
    "\n",
    "### Abstracting Data with DataFrames\n",
    "* **PySpark SQL** is Spark's high-level API for working with structured data.\n",
    "* **PySpark SQL** is a Spark library for structured data. It provides more information about the structure of the data and computation.\n",
    "* A **PySpark DataFrame** is an immutable distributed collection of data with named columns\n",
    "    * Designed to process a large collection of both structured (e.g. relational database) and semi-structured data (e.g. JSON: JavaScript Object Notation)\n",
    "    * Support both SQL queries (`SELECT * from table`) or expression methods (`df.select()`)\n",
    "    \n",
    "#### SparkSession- Entry point for DataFrame API\n",
    "* SparkContext is the main entry point for creating RDDs\n",
    "* SparkSession provides a single point of entry to interact with Spark DataFrames\n",
    "* The SparkSession does for DataFrames with the SparkContext does for RDDs\n",
    "* SparkSession can be used to create DataFrames, register DataFrames, execute SQL queries\n",
    "* SparkSession is available in PySpark shell as `spark`\n",
    "\n",
    "#### Creating DataFrames in PySpark\n",
    "* Two different methods of creating DataFrames in PySpark\n",
    "    * From existing RDDs using SparkSession's `createDataFrame()` method\n",
    "    * From various data sources (CSV, JSON, TXT) using SparkSession's read method\n",
    "* **Schema** controls the data and helps DataFrames to optimize queries\n",
    "* **Schema** provides informational detail such as the column name, the type of data in the column, empty values, etc. ...\n",
    "\n",
    "#### Create a DataFrame from RDD\n",
    "\n",
    "```\n",
    "iphones_RDD = sc.parallelize([\n",
    "    (\"XS\", 2018, 5.65, 2.79, 6.24),\n",
    "    (\"XR\", 2018, 5.94, 2.98, 6.84),\n",
    "    (\"X10\", 2017, 5.65, 2.79, 6.13),\n",
    "    (\"8Plus\", 2017, 6.23, 3.07, 7.12)\n",
    "])\n",
    "\n",
    "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\n",
    "\n",
    "iphones_df = spark.createDataFrame(iphones_RDD, schema = names)\n",
    "type(iphones_df)\n",
    "```\n",
    "* **NOTE:** When the schema is a list of column names, the type of each column will be inferred from data.\n",
    "    * However, when the schema is `None`, it will try to infer the schema from data.\n",
    "    \n",
    "#### Create a DataFrame from reading a CSV/JSON/TXT\n",
    "* `df_csv = spark.read.csv('people.csv', header=True, inferSchema=True)`\n",
    "* `df_json = spark.read.json(\"people.json\", header=True, inferSchema=True)`\n",
    "* `df_txt = spark.read.txt(\"people.txt\", header=True, inferSchema=True)`\n",
    "\n",
    "* Requires the path to the file and two optional parameters:\n",
    "    * `header`\n",
    "    * `inferSchema` (default is False)\n",
    "\n",
    "#### Exercises:\n",
    "```\n",
    "# Create an RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Create an DataFrame from file_path\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc3f0e",
   "metadata": {},
   "source": [
    "### Interacting with PySpark DataFrames\n",
    "* Just like RDDs, DataFrames also support both transformations and actions\n",
    "\n",
    "#### DataFrame operators in PySpark\n",
    "* DataFrame operations: Transformations and Actions\n",
    "\n",
    "#### DataFrame Transformations\n",
    "* **`select('column')`** : transformation subsets one or more columns in the DataFrame\n",
    "* **`filter(df.column <= condition)`** : transformation filters out the rows based on a condition\n",
    "* **`groupby('column')`** : operation can by used to group a variable *so that we can perform aggregations on them*\n",
    "* **`orderby('column)`** : operation sorts the DataFrame based on one or more columns\n",
    "* **`dropDuplicates()`** : removes the duplicate rows of a DataFrame\n",
    "* **`withColumnRenamed('former_name', 'new_name')`** : renames a column in the DataFrame\n",
    "\n",
    "#### DataFrame actions\n",
    "* **`printSchema()`** : operation prints the types of the columns in DataFrame; see note below\n",
    "* **`head(n)`** : shows first n rows\n",
    "* **`show(n)`** : action prints first n rows in the DataFrame; default n is 20\n",
    "* **`count()`** : counts number of occurences of __\n",
    "* **`columns`** : prints the columns of a DataFrame\n",
    "* **`describe()`** : operation computes summary statistics of numerical columns in the DataFrame\n",
    "* **Correction: `printSchema()` is a method for any Spark dataset/dataframe and not an action**\n",
    "\n",
    "#### Exercises: Inspecting data in PySpark DataFrame\n",
    "\n",
    "```\n",
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14d38f",
   "metadata": {},
   "source": [
    "#### Interacting with DataFrames using PySpark SQL\n",
    "* DataFrame API vs SQL queries\n",
    "* The DataFrames API provides a programmatic domain-specific language (DSL) for data\n",
    "* DataFrame transformations and actions (queries) are much easier to construct programmatically\n",
    "* SQL queries can be much more concise and easier to understand and portable\n",
    "\n",
    "#### Executing SQL Queries\n",
    "* The SparkSession `sql()` method executes SQL queries\n",
    "* `sql()` method takes a SQL statement as an argument and returns the result as a DataFrame representing the result of the given query.\n",
    "* To issue SQL queries against an existing DataFarme we can leverage the `createOrReplaceTempView` function to build a temporary table as shown in this example:\n",
    "* `df.createOrReplaceTempView(\"table1\")`\n",
    "* After creating the temporary table, we can simply use the `sql` method\n",
    "\n",
    "```\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT field1, field2 FROM table1\")\n",
    "df2.collect()\n",
    "```\n",
    "\n",
    "#### SQL query to extract data\n",
    "\n",
    "```\n",
    "test_df.createOrReplaceTempView(\"test_table\")\n",
    "query = '''SELECT Product_ID FROM test_table'''\n",
    "test_product_df = spark.sql(query)\n",
    "test_product_df.show(5)\n",
    "```\n",
    "* Because the result of SQL query returns a DataFrame, all the usual DataFrame operations are available. \n",
    "\n",
    "#### Exercises: Running SQL Queries Programmatically\n",
    "\n",
    "```\n",
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)\n",
    "```\n",
    "\n",
    "#### Exercises: SQL queries for filtering Table\n",
    "\n",
    "```\n",
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5d12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7606050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
