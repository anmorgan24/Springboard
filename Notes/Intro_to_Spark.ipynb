{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5109e865",
   "metadata": {},
   "source": [
    "# Intro to Spark with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33d43a",
   "metadata": {},
   "source": [
    "### What is Spark\n",
    "* Distributed data processing framework\n",
    "* \"Yet another Hadoop\"\n",
    "* Based on Resilient Distributed Datasets\n",
    "* Used for 'big' data processing\n",
    "\n",
    "\n",
    "\n",
    "* Relational databases are great, but they don't scale above one box.\n",
    "* Relational databases: query optimization\n",
    "\n",
    "### Resilient Distributed Datasets\n",
    "* How to distribute/parallelize a big set of objects\n",
    "* We can divide in slices and keep each slice in a different nodes\n",
    "    * **Values are computed only when needed**: speed\n",
    "    * To guarantee **fault tolerance** we also keep info about how we calculated each slice, so we can re-generate it if a node fails\n",
    "    * We can hint to keep in cache, or even save on disk\n",
    "* Immutable ! not designed for read/write\n",
    "    * Instead, transform an existing one into a new one\n",
    "* It is basically a huge list\n",
    "    * But distributed over many computers\n",
    "    \n",
    "### Shared Spark Variables\n",
    "* **Broadcase variables**\n",
    "    * copy is kept at each node\n",
    "* **Accumulators**\n",
    "    * you can only add; main node can read\n",
    "\n",
    "### Functional programming in Python\n",
    "* A lot of these concepts are already in python, which is an OOP\n",
    "    * But Python community tends to promote loops\n",
    "    * **List comprehensions are more similar to functional programming**\n",
    "* Functional tools in python\n",
    "    * `map`: applies a function to each element in a list; returns another list of results; the SELECT of Python\n",
    "    * `filter`: will only select the elements in a list that satisfy a given function; the WHERE of Python\n",
    "    * `reduce`: the AGG of Python; aggregates; reduces the elements in a list into a single value or values by applying a function repeatedly to pairs of elements until you get only one value\n",
    "    * `lambda`: writing functions, simplified\n",
    "    * itertools\n",
    "        * `chain`\n",
    "        * `flatmap`: specifically used in Spark\n",
    "        \n",
    "### Map in Python\n",
    "* Python supports the map operation, over any list\n",
    "* We apply an operation to each element of a list, return a new list with the results\n",
    "\n",
    "```\n",
    "a = [1, 2, 3]\n",
    "def add1(x):\n",
    "    return x + 1\n",
    "```\n",
    "   * `map(add1, a)` $\\Rightarrow$ `[2, 3, 4]`\n",
    "   * `map(add1, [1, 2, 3])` $\\Rightarrow$ `[2, 3, 4]`\n",
    "* We usually do this with a for loop\n",
    "* This (`map`) is a slightly different way of thinking\n",
    "* **Important to note:** the original list here is never changed, rather a new list is created.\n",
    "\n",
    "### Filter\n",
    "* Select only certain elements from a list\n",
    "* Example:\n",
    "\n",
    "```\n",
    "a = [1, 2, 3, 4]\n",
    "def isOdd(x):\n",
    "    return x%2==1\n",
    "```\n",
    "* `filter(isOdd, a)` $\\Rightarrow$ `[1, 3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a1de5",
   "metadata": {},
   "source": [
    "### Reduce in Python\n",
    "* Applies a function to all pairs of elements of a list; returns ONE value, not a list\n",
    "* Example:\n",
    "\n",
    "```\n",
    "a = [1, 2, 3, 4]\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "```\n",
    "* `reduce(add, a)` $\\Rightarrow$ `10`\n",
    "    * `add(1, add(2, add(3, 4)))`\n",
    "* **Better for functions that are commutative and association doesn't matter**\n",
    "    * Jobs in Spark work in parallel\n",
    "    \n",
    "### Lambda\n",
    "* When doing map/reduce/filter, we end up with many tiny functions\n",
    "* Lambda allows us to define a function as a value, without giving it a name\n",
    "* example: `lambda x: x + 1`\n",
    "    * Can only have one expression\n",
    "    * Do not write return\n",
    "    * Option to put parenthesis around it, but usually not needed by syntax\n",
    "* `(lambda x: x + 1)(3)` $\\Rightarrow$ `4`\n",
    "* `map(lambda X: x + 1, [1, 2, 3])` $\\Rightarrow$ `[2, 3, 4]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d196bbe",
   "metadata": {},
   "source": [
    "#### Exercises (1)\n",
    "* `(lambda x: 2*x)(3)` $\\Rightarrow$ **`6`**\n",
    "* `map(lambda x: 2*x, [1, 2, 3])` $\\Rightarrow$ **`[2, 4, 6]`**\n",
    "* `map(lambda t: t[0], [(1,2), (3,4), (5,6)])` $\\Rightarrow$ **`[1, 3, 5]`**\n",
    "* `reduce(lambda x,y: x+y, [1,2,3])` $\\Rightarrow$ **`6`**\n",
    "* `reduce(lambda x,y: x+y, map(lambda t: t[0], [(1,2),(3,4),(5,6)]))` $\\Rightarrow$ **`9`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5f6ed",
   "metadata": {},
   "source": [
    "#### Exercises (2)\n",
    "* Given: `a = [(1,2), (3,4), (5,6)]`\n",
    "\n",
    "    * **(a)** Write an expression to get only the second elements of each tuple\n",
    "    * **(b)** Write an expression to get the sum of the second elements\n",
    "    * **(c)** Write an expression to get the sum of the odd first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41966fbb",
   "metadata": {},
   "source": [
    "* `map(lambda t: t[1], a)`\n",
    "* `reduce(lambda x,y: x+y, map(lambda t: t[1], a))`\n",
    "* `reduce(lambda x, y: x+y, filter(isOdd, map(lambda t: t[0], a)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457fd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d1f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1,2), (3,4), (5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd74b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOdd(x):\n",
    "    return x%2==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b47f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, filter(isOdd, map(lambda t: t[0], a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181f92b",
   "metadata": {},
   "source": [
    "### Flatmap\n",
    "* Sometimes we end up with a list of lists, and we want a \"flat\" list\n",
    "* Python doesn't actually have a flatmap function, but provides something similar with `itertools.chain`\n",
    "* Many functional programming languages (and Spark) provide a function called flatMap, which flattens such a list\n",
    "* For example:\n",
    "    * `map(lambda t: range(t[0], t[1], [(1,5),(7,10)])` # Returns a list of lists\n",
    "* `itertools.chain` maps a list of iterables into a flat list\n",
    "    * And so enables us to define our own flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750829f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dd575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840857c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a376c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
