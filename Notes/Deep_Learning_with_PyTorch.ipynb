{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e962141d",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "Neural networks have been at the forefront of Artificial Intelligence research during the last few years, and have provided solutions to many difficult problems like image classification, language translation or Alpha Go. PyTorch is one of the leading deep learning frameworks, being at the same time both powerful and easy to use. In this course you will use PyTorch to first learn about the basic concepts of neural networks, before building your first neural network to predict digits from MNIST dataset. You will then learn about convolutional neural networks, and use them to build much more powerful models which give more accurate results. You will evaluate the results and use different techniques to improve them. Following the course, you will be able to delve deeper into neural networks and start your career in this fascinating field.\n",
    "\n",
    "**Instructor:** Ismail Elezi, PhD researched at Ca' Foscari University of Venice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6fe32",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter 1: Introduction to PyTorch\n",
    "In this first chapter, we introduce basic concepts of neural networks and deep learning using PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad00bf8",
   "metadata": {},
   "source": [
    "* **Why PyTorch?**\n",
    "    * Simplicity\n",
    "    * \"PyThonic\"- easy to use\n",
    "    * Strong GPU support - models run fast\n",
    "    * Many algorithms are already implemented\n",
    "    * Automatic differentiation\n",
    "    * Strong OOP\n",
    "    * Natural choice for many companies like Facebook and SalesForce\n",
    "    * One of the most used deep learning libraries in academical research\n",
    "    * Similar to NumPy, making the switch pretty painless\n",
    "    \n",
    "* Calculating derivatives and gradients is a very important aspect of deep learning algorithms \n",
    "* Luckily, PyTorch is very good at doing it for us\n",
    "\n",
    "#### PyTorch compared to NumPy\n",
    "* PyTorch's equivalent to ndarrays is a `torch.tensor`\n",
    "* Image a tensor as an array with an arbitrary number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ef467fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c0f7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 5],\n",
       "        [1, 2, 9]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 3, 5], [1, 2, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd64ffea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8223, 0.1763],\n",
       "        [0.8878, 0.3068]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b09b8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d980011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8539, 0.2192, 0.4255, 0.3891, 0.9884],\n",
      "        [0.7929, 0.6227, 0.6128, 0.2260, 0.3861],\n",
      "        [0.2820, 0.8878, 0.0554, 0.6852, 0.8450]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a31d154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbeb34e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71bf8930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8819, 1.2884, 1.5931],\n",
       "        [1.6487, 1.3359, 1.2814],\n",
       "        [1.5481, 1.1524, 1.1297]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ca3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97849c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2440, 0.0965, 0.2734, 0.2897, 0.5755],\n",
       "        [0.0186, 0.6216, 0.6122, 0.2168, 0.2859],\n",
       "        [0.0304, 0.1187, 0.0416, 0.4657, 0.7632]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f1974a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b396c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e153ed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3dd7ee",
   "metadata": {},
   "source": [
    "#### from NumPy to PyTorch\n",
    "* `d_torch = torch.from_numpy(c_numpy)`\n",
    "\n",
    "#### from PyTorch to NumPy\n",
    "* `c_torch.numpy()`\n",
    "\n",
    "<img src='data/basic_torch_functions.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f127c",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "* Also known as \"foward pass\"\n",
    "* Intuitively, a **computational graph** is a network of nodes that represent numbers, scalars, or tensors and are connected via edges that represent functions or operations\n",
    "\n",
    "#### PyTorch Implementation \n",
    "\n",
    "<img src='data/graph1.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42cdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First initialize tensors a, b, c, and d\n",
    "a = torch.Tensor([2])\n",
    "b = torch.Tensor([-4])\n",
    "c = torch.Tensor([-2])\n",
    "d = torch.Tensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b9639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = a + b\n",
    "f = c * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fe21984",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = e * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b31327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.]) tensor([-4.]) tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "print(e, f, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805f81",
   "metadata": {},
   "source": [
    "* Neural networks (and most other classifiers) can be understood as **computational graphs**\n",
    "    * In fact, your code gets converted to a computational graph\n",
    "    * An additional benefit of computational graphs, is that they make the automatic computation of derivatives (or gradients) much easier.\n",
    "    \n",
    "#### Exercises: Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c85f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(124.9853)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tensors x, y and z\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.rand(1000, 1000)\n",
    "z = torch.rand(1000, 1000)\n",
    "\n",
    "# Multiply x with y\n",
    "q = torch.matmul(x, y)\n",
    "\n",
    "# Multiply elementwise z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "print(mean_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894923be",
   "metadata": {},
   "source": [
    "### Backpropagation by auto-differentiation\n",
    "* The main algorithm in neural networks: the **backpropagation algorithm**\n",
    "\n",
    "#### Derivatives\n",
    "* Derivatives are one of the central concepts in calculus\n",
    "* In layman's terms, the derivatives represent the rate of change in a function\n",
    "    * Where the function is **rapidly changing**, the absolute value of the **derviatives is high**.\n",
    "    * When the function **is not changing** the derivtives are **close to 0**.\n",
    "    * They could also be interpreted as describing the steepness of a function\n",
    "    \n",
    "<img src='data/derivatives.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da016e",
   "metadata": {},
   "source": [
    "* In this graph, points `A` and `C` have large derivatives, while point `B` has a very small derivative\n",
    "* Khan Academy Derivatives course comes highly recommended\n",
    "\n",
    "<img src='data/derivative_rules.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae252f",
   "metadata": {},
   "source": [
    "* The **Addition** or **Sum** rule says that for two functions, $f$ and $g$, the derivative of their sum is the sum of their individual derivatives\n",
    "* The **Multiplication** rule says that the derivative of their product is $f$ times derivative of $g$ times derivative of $f$\n",
    "* The derivative of a number times a function is the number\n",
    "    * For example, the derivative of $3x$ is $3$\n",
    "* The derivative of a number itself is always zero\n",
    "* The derivative of something with respect to itself is always 1 \n",
    "* **Chain rule** deals with the composition of functions\n",
    "* A closely related term with derivatives is the gradient\n",
    "* The **gradient** is a multi-variable generalization of the derivative\n",
    "    * Considering that neural networks have many variables, we will typically use the term gradient instead of derivative when working with NNs\n",
    "    \n",
    "#### Backpropagation in PyTorch\n",
    "* The derivatives are calculated in PyTorch using the reverse mode of auto-differentiation, so you will rarely need to write code to calculate derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76b9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-3., requires_grad=True)\n",
    "y = torch.tensor(5., requires_grad=True)\n",
    "z = torch.tensor(-2., requires_grad=True)\n",
    "\n",
    "q = x + y\n",
    "f = q * z\n",
    "\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93a55b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z is : tensor(2.)\n",
      "Gradient of y is : tensor(-2.)\n",
      "Gradient of x is : tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient of z is : \" + str(z.grad))\n",
    "print(\"Gradient of y is : \" + str(y.grad))\n",
    "print(\"Gradient of x is : \" + str(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a8e0a",
   "metadata": {},
   "source": [
    "* **Note** that we need to set the `requires_grad` parameter to `True` in order to tell PyTorch that we need their derivatives\n",
    "* `f.backward()` tells PyTorch to compute the derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd8a24",
   "metadata": {},
   "source": [
    "### Introduction to Neural Networks\n",
    "* The simplest form of modern neural networks is: fully-connected neural networks (Dense)\n",
    "\n",
    "#### Fully connected neural networks with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01fedeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38504ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.rand(10, 20)\n",
    "w2 = torch.rand(20, 20)\n",
    "w3 = torch.rand(20, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a7ddf",
   "metadata": {},
   "source": [
    "* In order to get the values of the first hidden layer `h1`, we multiply the vector of features with the first matrix of weights `w1`\n",
    "* Look at the matrix of weights. The first dimension should always correspond to the preceding layer, and the second dimension to the following layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53c4c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = torch.matmul(input_layer, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8820a",
   "metadata": {},
   "source": [
    "* Similarly, we continue for the second hidden layer, `h2`, which is the product of the first hidden layer `h1` and the second matrix of weights `w2`.\n",
    "* Finally, we get the results of the `output_layer`, which has 4 classes, by multiplying the second hidden layer `h2` with the third matrix of weights `w3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea5afbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = torch.matmul(h1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e1fd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = torch.matmul(h2, w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7933d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([199.4510, 194.9810, 236.2898, 185.9965])\n"
     ]
    }
   ],
   "source": [
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7ec82",
   "metadata": {},
   "source": [
    "### Building a neural network- PyTorch style\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.output = nn.Linear(20, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "input_layer = torch.rand(10)\n",
    "net = Net()\n",
    "result = net(input_layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1f5c4",
   "metadata": {},
   "source": [
    "* In the `__init__` method, we define our parameters, the tensors of weights.\n",
    "* For fully connected layers, they are called `nn.Linear`\n",
    "    * The first parameter is the number of units of the current layer\n",
    "    * The second parameter is the number of units in the next layer\n",
    "    * In the forward method, we apply all those weights to our input\n",
    " \n",
    "#### Exercises: Your first neural network\n",
    "\n",
    "```\n",
    "# Initialize the weights of the neural network\n",
    "weight_1 = torch.rand(1, 1)\n",
    "weight_2 = torch.rand(1, 1)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Multiply hidden_1 with weight_2\n",
    "output_layer = torch.matmul(hidden_1, weight_2)\n",
    "print(output_layer)\n",
    "```\n",
    "***\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate all 2 linear layers  \n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Use the instantiated layers and return x\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23122c02",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee9b08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9696, 0.7527])\n"
     ]
    }
   ],
   "source": [
    "input_layer = torch.tensor([2., 1.])\n",
    "weight_1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "hidden_layer = torch.matmul(input_layer, weight_1)\n",
    "weight_2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])\n",
    "output_layer = torch.matmul(hidden_layer, weight_2)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc9b77",
   "metadata": {},
   "source": [
    "#### Matrix multiplication is a linear transformation\n",
    "* Now, let's try to do something different\n",
    "* Let's first multiply the matrices with `torch.matmul` and then we'll multiply the input with the product of these matrices\n",
    "* When we print the results, we see something interesting: **the result of the output layer is exactly the same as before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "663419c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9696, 0.7527])\n",
      "tensor([[0.4208, 0.2372],\n",
      "        [0.1280, 0.2783]])\n"
     ]
    }
   ],
   "source": [
    "input_layer = torch.tensor([2., 1.])\n",
    "weight_1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "weight_2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])\n",
    "weight = torch.matmul(weight_1, weight_2)\n",
    "output_layer = torch.matmul(input_layer, weight)\n",
    "print(output_layer)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fa1af",
   "metadata": {},
   "source": [
    "* This means that we can achieve the exact result by using a single layer neural network, with this particular set of weights. \n",
    "* Linear algebra demonstrates that matrix multiplication is actually a linear transformation, meaning that we can simplify any neural network in a single layer neural network\n",
    "* But, this comes with an irritating consequence: our neural nets are not that powerful; using them *alone* only allows us to separate linearly separable datasets (for which there are a host of more intuitive ML algorithms).\n",
    "* To separate non-linearly-separable functions, we use **activation functions.**\n",
    "\n",
    "<img src='data/activation_functions.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d2ecd",
   "metadata": {},
   "source": [
    "* **Activation functions** are non-linear functions which are inserted in each layer of the neural network, making neural networks nonlinear and allowing them to deal with highly non-linear datasets, thus making them much more powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16432894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0.])\n",
      "tensor([[2.0000, 0.0000],\n",
      "        [1.2000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "tensor_1 = torch.tensor([2., -4.])\n",
    "print(relu(tensor_1))\n",
    "\n",
    "tensor_2 = torch.tensor([[2., -4.], [1.2, 0.]])\n",
    "print(relu(tensor_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401c18d",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "* So far, all neural networks in this course have had random weights (and so they weren't particularly useful)\n",
    "* The recipe for training neural networks is the following:\n",
    "    * Initialize neural networks with random weights\n",
    "    * Do a forward pass\n",
    "    * Calculate loss function (1 number)\n",
    "    * Calculate the gradients using backpropagation\n",
    "    * Change the weights based on gradients\n",
    "* Loss (cost) function for **regression: least squared loss**\n",
    "* Loss (cost) function for **classification: softmax or (categorical) cross-entropy loss**\n",
    "* For more complicated problems (like object detection), more complicated losses\n",
    "* Loss functions should be **differentiable**; otherwise we won't be able to compute gradients\n",
    "* For this reason, instead of using accuracy (which is not differentiable), we need to use some proxy loss functions (in neural nets, a softmax function followed by a cross-entropy function performs really well).\n",
    "* **Softmax** is a function that turns numbers into probabilities\n",
    "\n",
    "<img src='data/softmax_cross_entropy2.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "### CE loss in PyTorch\n",
    "* `logits` = scores for each class\n",
    "* `ground_truth` = cat\n",
    "* `criterion` = loss function\n",
    "* Below we choose `nn.CrossEntropyLoss()` which combines **softmax** with **cross-entropy**\n",
    "* Note that we get the same result from the code below as we do in the illustration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "600c51d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0404)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[3.2, 5.1, -1.7]])\n",
    "ground_truth = torch.tensor([0])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e77b3d",
   "metadata": {},
   "source": [
    "What is the cat class prediction had been much higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5780612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0061)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[10.2, 5.1, -1.7]])\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c5681",
   "metadata": {},
   "source": [
    "What is the cat class prediction had been much lower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb286540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.1011)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[-10, 5.1, -1.7]])\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a2bae",
   "metadata": {},
   "source": [
    "The rule of thumb is that **the more accurate the network is, the smaller the loss (and vice versa).**\n",
    "\n",
    "#### Exercises: Calculating loss function in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8375146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the scores and ground truth\n",
    "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
    "ground_truth = torch.tensor([2])\n",
    "\n",
    "# Instantiate cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459269f",
   "metadata": {},
   "source": [
    "#### Exercises: Loss function of random scores\n",
    "If the neural network predicts random scores, what would be its loss function? Let's find it out in PyTorch. The neural network is going to have 1000 classes, each having a random score. For ground truth, it will have class 111. Calculate the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc6f18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.3071)\n"
     ]
    }
   ],
   "source": [
    "# Import torch and torch.nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize logits and ground truth\n",
    "logits = torch.rand(1,1000)\n",
    "ground_truth = torch.tensor([111])\n",
    "\n",
    "# Instantiate cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f620eb",
   "metadata": {},
   "source": [
    "### Preparing a dataset in PyTorch\n",
    "* In order to be able to use datasets in PyTorch, they need to be in some PyTorch friendly format that the framework will be able to understand\n",
    "* **`torchvision`:** a package which deals with datasets and pretrained neural nets\n",
    "* Below we define a transformation of images to torch tensors, usings `transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82fa0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.4914, 0.48216, 0.44653),\n",
    "                                  (0.24703, 0.24349, 0.26159))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19957a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aab057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e375735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85fc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d31fc4",
   "metadata": {},
   "source": [
    "<img src='data/visualize_parameters.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "<img src='data/whats_for_dinner.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
