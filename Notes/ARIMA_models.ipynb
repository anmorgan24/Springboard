{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba825af9",
   "metadata": {},
   "source": [
    "# ARIMA models in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce422f",
   "metadata": {},
   "source": [
    "Course Description: Have you ever tried to predict the future? What lies ahead is a mystery which is usually only solved by waiting. In this course, you will stop waiting and learn to use the powerful ARIMA class models to forecast the future. You will learn how to use the statsmodels package to analyze time series, to build tailored models, and to forecast under uncertainty. How will the stock market move in the next 24 hours? How will the levels of CO2 change in the next decade? How many earthquakes will there be next year? You will learn to solve all these problems and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7da85f",
   "metadata": {},
   "source": [
    "## ARMA models\n",
    "* Time series are everywhere:\n",
    "    * Science\n",
    "    * Technology\n",
    "    * Business\n",
    "    * Finance\n",
    "    * Policy\n",
    "* ARIMA models are one of the go-to time-series tools\n",
    "* **Trend:** a positive trend is a line that generally slopes up; a negative trend is a line that generally slopes down\n",
    "* **Seasonality:** has patterns that repeat at regular intervals, for example high sales every weekend\n",
    "* **Cyclicality:** in contrast to seasonality, has a repeating pattern but no fixed periods/time intervals.\n",
    "* **White noise:** has uncorrelated values\n",
    "\n",
    "#### Stationarity\n",
    "* To model a time series, it must be stationary\n",
    "* **Stationary:** Means that the distribution of the data doesn't change with time. For time series to be stationary, it must fulfill three criteria:\n",
    "    * **Trend stationary:** series has zero trend\n",
    "    * **Variance is constant:** the avererage distance of the data points from the zero line isn't changing\n",
    "    * **Autocorrelation is constant:** how each value in the time series is related to its neighbors stays the same.\n",
    "* For train-test split, the data must be split in time (not shuffled or reordered)\n",
    "* We train on the data earlier in the time series and test on the data that comes later\n",
    "\n",
    "\n",
    "#### Making time series stationary:\n",
    "#### Augmented Dickey-Fuller Test:\n",
    "* tests for trend non-stationarity\n",
    "* Null hypothesis is time series is non-stationary due to trend\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "results = adfuller(df['close'])\n",
    "```\n",
    "* 0th element: test statistic\n",
    "    * More negative means more likely to be stationary\n",
    "* 1st element: p-value\n",
    "    * If p-value is small (smaller than 0.05) $\\Rightarrow$ reject null hypothesis (reject non-stationarity)\n",
    "* 4th element: dictionary of critical values of the test statistic\n",
    "    \n",
    "* **Plotting time series can stop you from making incorrect assumptions and ends up saving you time!\n",
    "\n",
    "* Remember: the Dickey Fuller test only tests for stationarity.\n",
    "\n",
    "* Making a time series stationary $\\Rightarrow$ A bit like feature engineering in classic ML.\n",
    "\n",
    "* One very common way to make a time series stationary is to **take first differences.**\n",
    "* For some time series, **we may need to take the difference more than once.**\n",
    "* **Sometimes, we will need to perform other transformations to make the time series stationary.**\n",
    "\n",
    "#### Other transformations\n",
    "* **Take the log:** `np.log(df)`\n",
    "* **Take the square root:** `np.sqrt(df)`\n",
    "* **Take the proportional change:** `df.shift(1)/df`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8e4e7",
   "metadata": {},
   "source": [
    "### Autoregressive (AR) Models\n",
    "* In an AR(1) model:\n",
    "    * Today's value = a mean + a fraction ($\\phi$) of yesterday's value + noise\n",
    "    * $R_t$ = $\\mu$ + $\\phi$$R_{t-1}$ + $\\epsilon_t$\n",
    "* Since there is only 1 lagged value on the right hand side, this is called an AR model of order 1, or simply an AR(1) model.\n",
    "* If the AR parameter **$\\phi$ is 1**, then the process is a **random walk**.\n",
    "* If **$\\phi$ is 0**, then the process is **white noise**.\n",
    "* In order for the process to be **stable** and **stationary**, $\\phi$ has to be between -1 and 1\n",
    "    * -1 < $\\phi$ < 1\n",
    "* **Negative $\\phi$:** Mean reversion\n",
    "* **Positive $\\phi$:** Momentum\n",
    "* The autocorrelation **decays exponentially at a rate of $\\phi$.**\n",
    "    * This means that if $\\phi$ is 0.9:\n",
    "        * the lag-1 autocorrelation is 0.9 \n",
    "        * the lag-2 autocorrelation is $0.9^2$\n",
    "        * the lag-3 autocorrelation is $0.9^3$\n",
    "        * ... etc. ...\n",
    "    * When $\\phi$ is negative, the autocorrelation function still decays exponentially, but the signs of the autocorrelation function reverse at each lag.\n",
    "    \n",
    "* Higher Order AR Models:\n",
    "    * AR(1) \n",
    "        * $R_t$ = $\\mu$ + $\\phi_1$$R_{t-1}$ + $\\epsilon_t$\n",
    "    * AR(2)\n",
    "        * $R_t$ = $\\mu$ + $\\phi_1$$R_{t-1}$ + $\\phi_2$$R_{t-2}$ + $\\epsilon_t$\n",
    "    * AR(3)\n",
    "        * $R_t$ = $\\mu$ + $\\phi_1$$R_{t-1}$ + $\\phi_2$$R_{t-2}$ + $\\phi_3$$R_{t-3}$ + $\\epsilon_t$\n",
    "    * etc. ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642abcb",
   "metadata": {},
   "source": [
    "#### Simulating an AR Process\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.array([1, -0.9])\n",
    "ma = np.array([1])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)\n",
    "```\n",
    "* The convention for defining the order and parameters of the AR process is a little counterintuitive:\n",
    "    * You must include the zero-lag coefficient of 1, and the sign of the other coefficient is the opposite of what we have been using. \n",
    "    * For example, for an AR(1) process with $\\phi$ = **+0.9**, the second element of the ar array should be the opposite sign, **-0.9**\n",
    "    \n",
    "#### Estimating and Forecasting as AR Model\n",
    "* Statsmodels has another model for estimating the parameters of a given AR model\n",
    "\n",
    "#### Estimating an AR Model\n",
    "* To estimate parameters from data (simulated):\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order=(1,0))\n",
    "result = mod.fit()\n",
    "```\n",
    "* The arguments of `mod` are: 1) the data you are trying to fit and 2) the order of the model\n",
    "    * An order (1,0) would mean you're fitting the data to an AR(2) model.\n",
    "    * An order (2,0) would mean you're fitting the data to an AR(2) model.\n",
    "    * The second part of the order is the MA part (discusssed in next chapter).\n",
    "    * To see the full output, use the summary method on result:\n",
    "        * `print(result.summary())\n",
    "            * `const` = $\\mu$\n",
    "            * `ar.L1.y` = $\\phi$\n",
    "    * If you just want to see the coefficients rather than the entire regression output, you can use:\n",
    "        * `print(result.params)`\n",
    "        * returns array of the fitted coefficients $\\mu$ and $\\phi$\n",
    "        \n",
    "#### Forecasting an AR Model \n",
    "* To do forecasting, both in sample and out of sample, you still create an instance of the class using ARMA, and use `.plot_predict` to do forecasting\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order=(1,0))\n",
    "res = mod.fit()\n",
    "res.plot_predict(start='2016-07-01', end='2017-06-01')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Choosing the Right Model\n",
    "* In practice, you will ordinarily not be told the order of the model that you're trying to estimate\n",
    "* Two techniques to determine order: \n",
    "    * The **Partial Autocorrelation Function (PACF):** measures the incremental benefit of adding another lag\n",
    "        * **`.plot_pacf`:** same usage as `plt.acf`; is the statsmodels function for plotting the partial autocorrelation function\n",
    "    * The **Information criteria:** adjusts the goodness-of-fit of a model by imposing a penalty based on the number of parameters used.\n",
    "    * Two popular adjusted goodness-of-fit measures:\n",
    "        * **AIC (Akaike Information Criterion)**\n",
    "        * **BIC (Bayesian Information Criterion):** In practice, the best way to use the information criteria is to fit several models, each with a different number of parameters, and choose the one with the lowest information criterion\n",
    "        * Both AIC and BIC are included in the full estimation output of an ARMA model (`result.summary()`)\n",
    "        * To get solely the AIC or BIC statistics:\n",
    "            * `result.aic`\n",
    "            * `result.bic`\n",
    "    \n",
    "#### PACF\n",
    "```\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(x, lags=20, alpha=0.05)\n",
    "```\n",
    "* The input `x` is a series or array \n",
    "* The argument `lags` indicates how many lags of the partial autocorrelation function will be plotted \n",
    "* The `alpha` argument sets the width of the confidence interval\n",
    "\n",
    "\n",
    "\n",
    "### Moving Average (MA) and ARMA Models\n",
    "#### Describe Model\n",
    "* In a moving average, or MA model \n",
    "* Mathematical Description of a MA(1) Model:\n",
    "    * Today's value equals a mean plus noise, plus a fraction of theta of yesterday's noise\n",
    "    * $R_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$\n",
    "    * Since there is only one lagged error on the right hand side, this is called an MA model of order 1, or simply an MA(1) model. \n",
    "    * If $\\theta$ is 0, then the process is white noise\n",
    "    * MA models are stationary for all models of $\\theta$\n",
    "    * **Negative $\\theta$: One-Period Mean Reversion**; a shock two periods ago would have **no** effect on today's return- only the stock now and last period\n",
    "    * **Positive $\\theta$: One-Period Momentum**\n",
    "    * **Note:** One-period autocorrelation is $\\theta / (1 + \\theta^2)$, not $\\theta$\n",
    "    \n",
    "#### Simulating an MA Process\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.array([1])\n",
    "ma = np.array([1, 0.5])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)\n",
    "```\n",
    "* For an MA(1), the AR order is just an array containing 1 \n",
    "* The MA order is an array containing 1 and the MA(1) parameter $\\theta$\n",
    "* Unlike with the AR simulation, no need to reverse the sign of $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1d030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa62794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b26b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049c1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60da07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef1b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7bd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40131e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff15a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe37701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059df40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebee1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
