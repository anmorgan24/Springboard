{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba825af9",
   "metadata": {},
   "source": [
    "# ARIMA models in Python\n",
    "\n",
    "* Modeling wait times in hospitals? Emergency rooms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce422f",
   "metadata": {},
   "source": [
    "Course Description: Have you ever tried to predict the future? What lies ahead is a mystery which is usually only solved by waiting. In this course, you will stop waiting and learn to use the powerful ARIMA class models to forecast the future. You will learn how to use the statsmodels package to analyze time series, to build tailored models, and to forecast under uncertainty. How will the stock market move in the next 24 hours? How will the levels of CO2 change in the next decade? How many earthquakes will there be next year? You will learn to solve all these problems and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7da85f",
   "metadata": {},
   "source": [
    "## ARMA models\n",
    "* Time series are everywhere:\n",
    "    * Science\n",
    "    * Technology\n",
    "    * Business\n",
    "    * Finance\n",
    "    * Policy\n",
    "* ARIMA models are one of the go-to time-series tools\n",
    "* **Trend:** a positive trend is a line that generally slopes up; a negative trend is a line that generally slopes down\n",
    "* **Seasonality:** has patterns that repeat at regular intervals, for example high sales every weekend\n",
    "* **Cyclicality:** in contrast to seasonality, has a repeating pattern but no fixed periods/time intervals.\n",
    "* **White noise:** has uncorrelated values\n",
    "\n",
    "#### Stationarity\n",
    "* To model a time series, it must be stationary\n",
    "* **Stationary:** Means that the distribution of the data doesn't change with time. For time series to be stationary, it must fulfill three criteria:\n",
    "    * **Trend stationary:** series has zero trend\n",
    "    * **Variance is constant:** the avererage distance of the data points from the zero line isn't changing\n",
    "    * **Autocorrelation is constant:** how each value in the time series is related to its neighbors stays the same.\n",
    "* For train-test split, the data must be split in time (not shuffled or reordered)\n",
    "* We train on the data earlier in the time series and test on the data that comes later\n",
    "\n",
    "\n",
    "#### Making time series stationary:\n",
    "#### Augmented Dickey-Fuller Test:\n",
    "* tests for trend non-stationarity\n",
    "* Null hypothesis is time series is non-stationary due to trend\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "results = adfuller(df['close'])\n",
    "```\n",
    "* 0th element: test statistic\n",
    "    * More negative means more likely to be stationary\n",
    "* 1st element: p-value\n",
    "    * If p-value is small (smaller than 0.05) $\\Rightarrow$ reject null hypothesis (reject non-stationarity)\n",
    "* 4th element: dictionary of critical values of the test statistic\n",
    "    \n",
    "* **Plotting time series can stop you from making incorrect assumptions and ends up saving you time!\n",
    "\n",
    "* Remember: the Dickey Fuller test only tests for stationarity.\n",
    "\n",
    "* Making a time series stationary $\\Rightarrow$ A bit like feature engineering in classic ML.\n",
    "\n",
    "* One very common way to make a time series stationary is to **take first differences.**\n",
    "* For some time series, **we may need to take the difference more than once.**\n",
    "* **Sometimes, we will need to perform other transformations to make the time series stationary.**\n",
    "\n",
    "#### Other transformations\n",
    "* **Take the log:** `np.log(df)`\n",
    "* **Take the square root:** `np.sqrt(df)`\n",
    "* **Take the proportional change:** `df.shift(1)/df`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe226e",
   "metadata": {},
   "source": [
    "### Autoregressive (AR) Models\n",
    "* In an AR(1) model:\n",
    "    * Today's value = a mean + a fraction ($\\phi$) of yesterday's value + noise\n",
    "    * $R_t$ = $\\mu$ + $\\phi$$R_{t-1}$ + $\\epsilon_t$\n",
    "* Since there is only 1 lagged value on the right hand side, this is called an AR model of order 1, or simply an AR(1) model.\n",
    "* If the AR parameter **$\\phi$ is 1**, then the process is a **random walk**.\n",
    "* If **$\\phi$ is 0**, then the process is **white noise**.\n",
    "* In order for the process to be **stable** and **stationary**, $\\phi$ has to be between -1 and 1\n",
    "    * -1 < $\\phi$ < 1\n",
    "* **Negative $\\phi$:** Mean reversion\n",
    "* **Positive $\\phi$:** Momentum\n",
    "* The autocorrelation **decays exponentially at a rate of $\\phi$.**\n",
    "    * This means that if $\\phi$ is 0.9:\n",
    "        * the lag-1 autocorrelation is 0.9 \n",
    "        * the lag-2 autocorrelation is $0.9^2$\n",
    "        * the lag-3 autocorrelation is $0.9^3$\n",
    "        * ... etc. ...\n",
    "    * When $\\phi$ is negative, the autocorrelation function still decays exponentially, but the signs of the autocorrelation function reverse at each lag.\n",
    "    \n",
    "* Higher Order AR Models:\n",
    "    * AR(1) \n",
    "        * $R_t$ = $\\mu$ + $\\phi_1$$R_{t-1}$ + $\\epsilon_t$\n",
    "    * AR(2)\n",
    "        * $R_t$ = $\\mu$ + $\\phi_1$$R_{t-1}$ + $\\phi_2$$R_{t-2}$ + $\\epsilon_t$\n",
    "    * AR(3)\n",
    "        * $R_t$ = $\\mu$ + $\\phi_1$$R_{t-1}$ + $\\phi_2$$R_{t-2}$ + $\\phi_3$$R_{t-3}$ + $\\epsilon_t$\n",
    "    * etc. ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861d260",
   "metadata": {},
   "source": [
    "#### Simulating an AR Process\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.array([1, -0.9])\n",
    "ma = np.array([1])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)\n",
    "```\n",
    "* The convention for defining the order and parameters of the AR process is a little counterintuitive:\n",
    "    * You must include the zero-lag coefficient of 1, and the sign of the other coefficient is the opposite of what we have been using. \n",
    "    * For example, for an AR(1) process with $\\phi$ = **+0.9**, the second element of the ar array should be the opposite sign, **-0.9**\n",
    "    \n",
    "#### Estimating and Forecasting as AR Model\n",
    "* Statsmodels has another model for estimating the parameters of a given AR model\n",
    "\n",
    "#### Estimating an AR Model\n",
    "* To estimate parameters from data (simulated):\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order=(1,0))\n",
    "result = mod.fit()\n",
    "```\n",
    "* The arguments of `mod` are: 1) the data you are trying to fit and 2) the order of the model\n",
    "    * An order (1,0) would mean you're fitting the data to an AR(2) model.\n",
    "    * An order (2,0) would mean you're fitting the data to an AR(2) model.\n",
    "    * The second part of the order is the MA part (discusssed in next chapter).\n",
    "    * To see the full output, use the summary method on result:\n",
    "        * `print(result.summary())\n",
    "            * `const` = $\\mu$\n",
    "            * `ar.L1.y` = $\\phi$\n",
    "    * If you just want to see the coefficients rather than the entire regression output, you can use:\n",
    "        * `print(result.params)`\n",
    "        * returns array of the fitted coefficients $\\mu$ and $\\phi$\n",
    "        \n",
    "#### Forecasting an AR Model \n",
    "* To do forecasting, both in sample and out of sample, you still create an instance of the class using ARMA, and use `.plot_predict` to do forecasting\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order=(1,0))\n",
    "res = mod.fit()\n",
    "res.plot_predict(start='2016-07-01', end='2017-06-01')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Choosing the Right Model\n",
    "* In practice, you will ordinarily not be told the order of the model that you're trying to estimate\n",
    "* Two techniques to determine order: \n",
    "    * The **Partial Autocorrelation Function (PACF):** measures the incremental benefit of adding another lag\n",
    "        * **`.plot_pacf`:** same usage as `plt.acf`; is the statsmodels function for plotting the partial autocorrelation function\n",
    "    * The **Information criteria:** adjusts the goodness-of-fit of a model by imposing a penalty based on the number of parameters used.\n",
    "    * Two popular adjusted goodness-of-fit measures:\n",
    "        * **AIC (Akaike Information Criterion)**\n",
    "        * **BIC (Bayesian Information Criterion):** In practice, the best way to use the information criteria is to fit several models, each with a different number of parameters, and choose the one with the lowest information criterion\n",
    "        * Both AIC and BIC are included in the full estimation output of an ARMA model (`result.summary()`)\n",
    "        * To get solely the AIC or BIC statistics:\n",
    "            * `result.aic`\n",
    "            * `result.bic`\n",
    "    \n",
    "#### PACF\n",
    "```\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(x, lags=20, alpha=0.05)\n",
    "```\n",
    "* The input `x` is a series or array \n",
    "* The argument `lags` indicates how many lags of the partial autocorrelation function will be plotted \n",
    "* The `alpha` argument sets the width of the confidence interval\n",
    "\n",
    "\n",
    "\n",
    "### Moving Average (MA) and ARMA Models\n",
    "#### Describe Model\n",
    "* In a moving average, or MA model \n",
    "* Mathematical Description of a MA(1) Model:\n",
    "    * Today's value equals a mean plus noise, plus a fraction of theta of yesterday's noise\n",
    "    * $R_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$\n",
    "    * Since there is only one lagged error on the right hand side, this is called an MA model of order 1, or simply an MA(1) model. \n",
    "    * If $\\theta$ is 0, then the process is white noise\n",
    "    * MA models are stationary for all models of $\\theta$\n",
    "    * **Negative $\\theta$: One-Period Mean Reversion**; a shock two periods ago would have **no** effect on today's return- only the stock now and last period\n",
    "    * **Positive $\\theta$: One-Period Momentum**\n",
    "    * **Note:** One-period autocorrelation is $\\theta / (1 + \\theta^2)$, not $\\theta$\n",
    "    \n",
    "#### Simulating an MA Process\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.array([1])\n",
    "ma = np.array([1, 0.5])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)\n",
    "```\n",
    "* For an MA(1), the AR order is just an array containing 1 \n",
    "* The MA order is an array containing 1 and the MA(1) parameter $\\theta$\n",
    "* Unlike with the AR simulation, no need to reverse the sign of $\\theta$\n",
    "\n",
    "```\n",
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "# Plot 1: MA parameter = -0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1])\n",
    "ma1 = np.array([1, -0.9])\n",
    "MA_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = MA_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: MA parameter = +0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1])\n",
    "ma2 = np.array([1, 0.9])\n",
    "MA_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = MA_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Estimation and Forecasting an MA Model\n",
    "* Same as estimating an AR model (except for `order=(0,1)`)\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order=(0,1))\n",
    "result = mod.fit()\n",
    "```\n",
    "**Note** that now the order is (0,1) for an MA(1) model, and not (1,0) as for an AR(1) model\n",
    "\n",
    "#### Forecasting an MA Model\n",
    "* The procedure for forecasting an MA model is the same as that for an AR model, but set the order to (0,1):\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(simulated_data, order=(0,1))\n",
    "res = mod.fit()\n",
    "res.plot_predict(start='2016-07-01', end='2017-06-01')\n",
    "plt.show()\n",
    "```\n",
    "* **One thing to note** is that with an MA(1) model, unlike an AR model, all forecasts beyond the one-step ahead forecast will be the same\n",
    "\n",
    "### ARMA models\n",
    "* An **ARMA model** is a combination of an AR and MA model.\n",
    "* Mathematcial equation for an ARMA(1,1) model:\n",
    "    * $R_t = \\mu + \\phi R_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}$\n",
    "* an ARMA(1,1) model contains AR(1) and MA(1) model components\n",
    "* ARMA models can be converted to pure AR or pure MA models\n",
    "\n",
    "### Cointegration Models\n",
    "* What is **Cointegration?**\n",
    "    * Two series, $P_t$ and $Q_t$ can be random walks\n",
    "    * But the linear combination $P_t - cQ_t$ may not be a random walk!\n",
    "    * If that's true:\n",
    "        * $P_t - cQ_t$ is forecastable\n",
    "        * $P_t$ and $Q_t$ are said to be **cointegrated**\n",
    "        \n",
    "* Analogy: **Dog on a (retractable) Leash**\n",
    "    * $P_t$ = Owner\n",
    "    * $Q_t$ = Dog\n",
    "    * Both series (individually) look like a random walk\n",
    "    * However the difference or distance between them, will be mean reverting.\n",
    "    * The dog and its owner are linked together and their distance is a mean reverting process\n",
    "* What types of series are cointegrated?\n",
    "    * Commodities\n",
    "    * Competitors are not necessarily economic substitutes/ cointegrated\n",
    "    \n",
    "#### Two Steps to Test for Cointegration\n",
    "* First: Regress the level of one series on the level of the other series, to the get the slope coefficient c\n",
    "    * Regress $P_t$ on $Q_t$ and get slope $c$\n",
    "* Secong: Run Augmented Dickey-Fuller test on $P_t - cQ_t$ to test for random walk\n",
    "\n",
    "* Alternatively:\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.stattools import coint\n",
    "coint(P,Q)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ace64",
   "metadata": {},
   "source": [
    "## ARIMA vs SARIMAX models in Python\n",
    "[Medium article](https://medium.com/swlh/a-brief-introduction-to-arima-and-sarima-modeling-in-python-87a58d375def)\n",
    "* Time series analysis is a great tool for predicting future events such as market values changing. \n",
    "* ARIMA and SARIMA require data in a ‘long’ format.\n",
    "* **ARIMA** stands for **A**uto **R**egressive **I**ntegrated **M**oving **A**verage\n",
    "* **SARIMAX** is similar and stands for **S**easonal **A**uto **R**egressive **I**ntegrated **M**oving **A**verage with e**X**ogenous factors\n",
    "\n",
    "* In an auto regressive (AR) model the model predicts the next data point by looking at previous data points and using a mathematical formula similar to linear regression.\n",
    "* There is something called an **order** (represented by **“p”**) that determines how many previous data points will be used. \n",
    "    * The best way to get a good p value is to just try a few different values and see which model came out with the minimum (best) AIC.\n",
    "    * Next up is to explain the **integrated** part of ARIMA and SARIMAX. For the auto regressive and moving average models to work the data must be stationary. This means the data needs to not have trends or seasonality.\n",
    "    * **Integration** is taking a difference of the time-series, subtracting the previous value from each value, which tends to make the data more stationary.\n",
    "    * There is a value called **“d”** which represents **how many times the data is to be differenced.**\n",
    "        * Like the p value in the auto regressive model, it is best to simply try out a few different values for d and see which model has the minimal AIC.\n",
    "    * A moving average (MA) model performs calculations based on noise in the data along with the data’s slope. \n",
    "    * **Combining AR and MA along with differencing (I) creates the ARIMA model.** \n",
    "        * There is a value called **“q”** in the moving average model that is the **order**. The order is similar to the auto regressive order where it is the amount of past data points to put into the equation.\n",
    "        \n",
    "        \n",
    "#### SARIMAX\n",
    "* SARIMAX is used on data sets that have seasonal cycles. The difference between ARIMA and SARIMAX is the seasonality and exogenous factors (seasonality and regular ARIMA don’t mix well)\n",
    "    * the key take away is that SARIMAX requires not only the p, d, and q arguments that ARIMA requires, but it also requires another set of p, d, and q arguments for the seasonality aspect as well as an argument called **“s”** which is **the periodicity of the data’s seasonal cycle.**\n",
    "        * When choosing an s value try to get an idea of when the seasonal data cycles. If your data points are separated by a monthly basis and the seasonal cycle is a year, then set s to 12. Or if the data points are separated by a daily basis and the seasonal cycle is a week then make s equal to 7.\n",
    "* For all the values other than SARIMAX’s “s” value I recommend using a grid search this can be used with the itertools library in python\n",
    "    * find the minimum value in the list of AICs and then find that key in the dictionary. The resulting key will be the optimal order to create a model with.\n",
    "    \n",
    "* To run an ARIMA model there are two arguments: the actual data and the order. The order is a tuple containing p, d, and q.\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import itertools\n",
    "# Grid Search\n",
    "p = d = q = range(0,3) # p, d, and q can be either 0, 1, or 2\n",
    "pdq = list(itertools.product(p,d,q)) # gets all possible combinations of p, d, and q\n",
    "combs = {} # stores aic and order pairs\n",
    "aics = [] # stores aics\n",
    "# Grid Search continued\n",
    "for combination in pdq:\n",
    "    try:\n",
    "        model = ARIMA(data, order=combination) # create all possible models\n",
    "        model = model.fit()\n",
    "        combs.update({model.aic : combination}) # store combinations\n",
    "        aics.append(model.aic)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "best_aic = min(aics)\n",
    "# Model Creation and Forecasting\n",
    "model = ARIMA(data, order=combs[best_aic])\n",
    "model = model.fit()\n",
    "model.forecast(7)[0]\n",
    "```\n",
    "\n",
    "* SARIMAX is much like ARIMA, but a little more complicated. \n",
    "* Not only do you have to use a loop and grid search for the optimal values of p, d, and q, but you have to also use a nested loop and grid search for the seasonal values for p, d, and q. \n",
    "* There are also many more parameters in the SARIMAX function.\n",
    "\n",
    "```\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "# Grid Search\n",
    "p = d = q = range(0,3) # p, d, and q can be either 0, 1, or 2\n",
    "pdq = list(itertools.product(p,d,q)) # gets all possible combinations of p, d, and q \n",
    "p2 = d2 = q2 = range(0, 2) # second set of p's, d's, and q's\n",
    "pdq2 = list(itertools.product(p2,d2,q2)) # simular too code above but for seasonal parameters\n",
    "s = 12 # here I use twelve but the number here is representative of the periodicty of the seasonal cycle\n",
    "pdqs2 = [(c[0], c[1], c[2], s) for c in pdq2]\n",
    "combs = {}\n",
    "aics = []\n",
    "# Grid Search Continued\n",
    "for combination in pdq:\n",
    "    for seasonal_combination in pdqs2:\n",
    "        try:\n",
    "            model = sm.tsa.statespace.SARIMAX(data, order=combination, seasonal_order=seasonal_combination,\n",
    "                                             enforce_stationarity=False,\n",
    "                                             enforce_invertibility=False)\n",
    "            model = model.fit()\n",
    "            combs.update({model.aic : [combination, seasonal_combination]})\n",
    "            aics.append(model.aic)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "best_aic = min(aics)\n",
    "# Modeling and forcasting\n",
    "model = sm.tsa.statespace.SARIMAX(data, order=combs[best_aic][0], seasonal_order=combs[best_aic][1],\n",
    "                                             enforce_stationarity=False,\n",
    "                                             enforce_invertibility=False)\n",
    "model = model.fit()\n",
    "model.forecast(7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91d82a",
   "metadata": {},
   "source": [
    "# DataCamp ARIMA Models with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818e954",
   "metadata": {},
   "source": [
    "* Time series data:\n",
    "    * Science\n",
    "    * Technology\n",
    "    * Business\n",
    "    * Finance\n",
    "    * Policy\n",
    "    * Medicine/ health/ disease/ public health\n",
    "    * Energy sector\n",
    "    * Population growth\n",
    "    * etc. ...\n",
    "* Forecast all of these datasets using time series models, and ARIMA models are one of the go-to time series tools. \n",
    "\n",
    "#### Trend:\n",
    "* positive trend slopes up\n",
    "* negative trend slopes down\n",
    "\n",
    "```\n",
    "fig, ax = plt.subplots()\n",
    "df.plot(ax=ax)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Seasonality \n",
    "* A seasonal time series has patterns that repeat at regular intervals, for example high sales every weekend, or lowest temperatures in the winter. \n",
    "\n",
    "#### Cyclicality\n",
    "* Cyclicality is where there is a repeating pattern but no fixed period\n",
    "\n",
    "#### White noise\n",
    "* White noise series has uncorrelated values\n",
    "* A series of measurement, where each value is uncorrelated with previous values\n",
    "* Just like flipping a coin, with white noise, the series value doesn't depend on the values that came before\n",
    "\n",
    "* **To model a time series, it must be stationary** (meaning that the distribution of the data doesn't change with time). \n",
    "* **For a time series to be stationary, it must fulfill three criteria:**\n",
    "    * 1) **The series has zero trend:** it is neither growing nor shrinking\n",
    "    * 2) **The variance is constant:** The average distance of the data points from the zero line isn't changing\\\n",
    "    * 3) **The autocorrelation is constant:** How each value in the time series is related to its neighbors stays the same.\n",
    "    \n",
    "* Generally, in machine learning, you have a training set, which you fit your model on, and a test set, which you will test your predictions against. Time series forecasting is just the same, but the train-test split will be different. \n",
    "    * We use the past values to make future predictions, and so we will need to split the data in time. \n",
    "    * We train on the data earlier in the time series and test on the data that comes later. \n",
    "    * Split time series at a given date as shown below:\n",
    "        * `df_train = df.loc[:'2018']`\n",
    "        * `df_test = df.loc['2019':]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7c3e7",
   "metadata": {},
   "source": [
    "### Making time series stationary\n",
    "* The most common test for identifying whether a time series is non-stationary or not is the **Augmented Dickey-Fuller Test** aka the **Adfuller Test**\n",
    "\n",
    "#### The Augmented Dickey-Fuller Test\n",
    "* Tests for trend non-stationarity\n",
    "* Null hypothesis is time series is non-stationary\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "results = adfuller(df['close'])\n",
    "```\n",
    "* The results object is a tuple containing:\n",
    "    * **Oth element:** the test statistic\n",
    "        * The more negative this number is, the more likely that the data is stationary\n",
    "    * **1st element:** test p-value\n",
    "        * If p-value is small (for example <0.05) $\\Rightarrow$ reject null hypothesis. Reject non-stationary.\n",
    "    * **2nd element:** a dictionary storing the critical values of the test statistic, which equate to different p-values\n",
    "    \n",
    "* **It is always worth plotting your time series as well as doing the statistical tests.**\n",
    "* Plotting time series can stop you making wrong assumptions.\n",
    "* Remember that the Dickey-Fuller test **only tests for trend stationarity.**\n",
    "\n",
    "* 1) **Taking the difference:**\n",
    "    * `df_stationary = df.diff()`\n",
    "    * By default, will lead to `NaN`s:\n",
    "    * `df_stationary = df.diff().dropna()`\n",
    "    * For some time series, you may need to take the difference more than once.\n",
    "* 2) **Other transforms:**\n",
    "    * Sometimes, we will need to perform other transformations to make the time series stationary\n",
    "    * Examples of other transforms/ transformations:\n",
    "        * Take the **log**:\n",
    "        * `np.log(df)`\n",
    "        * Take the **square root**:\n",
    "        * `np.sqrt(df)`\n",
    "        * Take the **proportional change**:\n",
    "        * `df.shift(1)/df`\n",
    "    * Often, the simplest solution is the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a8e2f",
   "metadata": {},
   "source": [
    "## Intro to AR, MA, and ARMA models\n",
    "* AR and MA models and how these are combined into ARMA models.\n",
    "\n",
    "#### AR models\n",
    "* **Autoregressive** (AR) model\n",
    "* In an autoregressive model, we regress the values of the time series against previous values of this same time series:\n",
    "    * First order AR model, or AR(1) model:\n",
    "    * $y_t = a_1y_{t-1}+\\epsilon_t$\n",
    "* The \"shock term\", $\\epsilon_t$, is white noise.\n",
    "* $a_1$ is the autoregressive coefficient at lag one\n",
    "* Compare this to a simple linear regression where the dependent variable is $y_t$ and the independent variable is $y_{t-1}$\n",
    "* The coefficient $a_1$ is just the slope of the line and the shocks are the residuals to the line\n",
    "* The **order** of the model is the **number of time lags** used.\n",
    "* An order two AR model, AR(2) has two autoregressive coefficients, and has two independent variables, the series at lag one, and the series at lag two:\n",
    "    * $y_t = a_1y_{t-1} + a_2y_{t-2}+\\epsilon_t$\n",
    "* More generally, we use **p** to mean the order of the AR model: **AR(p)**\n",
    "    * This means we have **p** autoregressive coefficients and us p lags\n",
    "    \n",
    "#### MA models\n",
    "* Moving average (MA) models\n",
    "* In a **moving average model**, we regress the values of the time series against the **previous shock values** of this same time series\n",
    "* First order MA model: \n",
    "    * $y_t = m_1\\epsilon_{t-1}+\\epsilon_t$\n",
    "* The value of the time series is $m_1$ times the value of the shock at the previous step; plus the shock term for the current time step\n",
    "* The order of the model means how many time lags we use\n",
    "* An MA(2) model would include shocks from one and two steps ago:\n",
    "    * $y_t = m_1\\epsilon_{t-1}+m_2\\epsilon_{t-2}+\\epsilon_t$\n",
    "* More generally, we use **q** to mean the **order of the MA model**.\n",
    "\n",
    "#### ARMA models\n",
    "* Autoregressive moving-average (ARMA) model\n",
    "* ARMA = AR + MA\n",
    "* An ARMA model is a combination of the AR and MA models\n",
    "* The time series is regressed on the previous values and the precious shock terms\n",
    "* An ARMA(1,1) model:\n",
    "    * $y_t=a_1y_{t-1}+m_1\\epsilon_{t-1}+\\epsilon_t$\n",
    "* More generally, we use ARMA-p-q to define an ARMA model\n",
    "\n",
    "#### ARMA(p, q)\n",
    "* p is order of AR (autoregressive) part\n",
    "* q is order of MA (moving-average) part\n",
    "\n",
    "* Using the statsmodels package, we can both fit ARMA models and create ARMA data.\n",
    "\n",
    "* Let's take this ARMA-one-one model:\n",
    "    * $y_t = a_1y_{t-1} + m_1\\epsilon_{t-1}+\\epsilon_t$\n",
    "* Say we want to simulate the data with these coefficients:\n",
    "    * $y_t = 0.5y_{t-1}+0.2\\epsilon_{t-1}+\\epsilon_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ce10a",
   "metadata": {},
   "source": [
    "* First, we import the arma_generate_sample function\n",
    "* Then we make lists for the AR and MA coefficients\n",
    "    * Note that both coefficient lists start with 1.\n",
    "    * This is for the zero lag term and we will always set this to one (unless we are simulating an ARMA sample of an order other than (1,1)\n",
    "    * **Note that the AR coefficient sign is reversed** (as always)\n",
    "* Generate the data, passing in the coefficients, the **number of data points to create**, and the **standard deviation of the shocks**.\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "ar_coefs = [1, -0.5]\n",
    "ma_coefs = [1, 0.2]\n",
    "y = arma_generate_sample(ar_coefs, ma_coefs, nsample=100, scale=0.5)\n",
    "```\n",
    "\n",
    "#### Fitting and ARMA model\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "# Instantiate model object\n",
    "model = ARMA(y, order=(1,1))\n",
    "# Fit model\n",
    "results = model.fit()\n",
    "```\n",
    "\n",
    "* Remember for any model ARMA(p,q):\n",
    "    * The list ar_coefs has the form [1, -a_1, -a_2, ..., -a_p].\n",
    "    * The list ma_coefs has the form [1, m_1, m_2, ..., m_q],\n",
    "* where a_i are the lag-i AR coefficients and m_j are the lag-j MA coefficients.\n",
    "\n",
    "#### For MA(1) model with MA lag-1 coefficient of -0.7:\n",
    "```\n",
    "# Import data generation function and set random seed\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "np.random.seed(1)\n",
    "\n",
    "# Set coefficients\n",
    "ar_coefs = [1]\n",
    "ma_coefs = [1, -0.7]\n",
    "\n",
    "# Generate data\n",
    "y = arma_generate_sample(ar_coefs, ma_coefs, nsample=100, scale=0.5)\n",
    "\n",
    "plt.plot(y)\n",
    "plt.ylabel(r'$y_t$')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### For AR(2) model with AR lag-1 and lag-2 coefficients of 0.3 and 0.2 respectively:\n",
    "\n",
    "```\n",
    "# Import data generation function and set random seed\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "np.random.seed(2)\n",
    "\n",
    "# Set coefficients\n",
    "ar_coefs = [1, -0.3, -0.2]\n",
    "ma_coefs = [1]\n",
    "\n",
    "# Generate data\n",
    "y = arma_generate_sample(ar_coefs, ma_coefs, nsample=100, scale=0.5)\n",
    "\n",
    "plt.plot(y)\n",
    "plt.ylabel(r'$y_t$')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.show()\n",
    "```\n",
    "**For model with form: $y_t = -0.2y_{t-1}+0.3\\epsilon_{t-1}+0.4\\epsilon_{t-2}+\\epsilon_t$\n",
    "```\n",
    "# Import data generation function and set random seed\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "np.random.seed(3)\n",
    "\n",
    "# Set coefficients\n",
    "ar_coefs = [1, 0.2]\n",
    "ma_coefs = [1, 0.3, 0.4]\n",
    "\n",
    "# Generate data\n",
    "y = arma_generate_sample(ar_coefs, ma_coefs, nsample=100, scale=0.5)\n",
    "\n",
    "plt.plot(y)\n",
    "plt.ylabel(r'$y_t$')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb41f6",
   "metadata": {},
   "source": [
    "#### Fitting prelude\n",
    "\n",
    "```\n",
    "# Import the ARMA model\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Instantiate the model\n",
    "model = ARMA(y, order=(1,1))\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "```\n",
    "\n",
    "# CHAPTER 2- Fitting the Future\n",
    "What lies ahead in this chapter is you predicting what lies ahead in your data. You'll learn how to use the elegant statsmodels package to fit ARMA, ARIMA and ARMAX models. Then you'll use your models to predict the uncertain future of stock prices!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec264d",
   "metadata": {},
   "source": [
    "### Fitting time series models\n",
    "* **Creating a model:**\n",
    "* The training data can be a pandas dataframe, a pandas series or a numpy array.\n",
    "* Remember that the order for an ARMA model is **p**, the number of autoregressive lags, and **q**, the number of moving average lags. \n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "model = ARMA(timeseries, order=(p,q))\n",
    "```\n",
    "\n",
    "* **To fit an AR model, we can simply use the ARMA class with q=0.**\n",
    "* `ar_model = ARMA(timeseries, order=(p,0))`\n",
    "* **To fit an MA model, we set p equal to zero.**\n",
    "* `ma_model = ARMA(timeseries, order=(0,q))`\n",
    "* To fit the model, we use the model's `.fit()` method:\n",
    "\n",
    "```\n",
    "model = ARMA(timeseries, order=(2,1))\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "```\n",
    "\n",
    "#### Fit summary\n",
    "* The top section includes useful information such as the order of the model that we fit, the number of observations or data points and the name of the time series.\n",
    "* **S.D. of innovations:** is the standard deviation of the shock terms\n",
    "* The next section of the summary shows the fitted model parameters\n",
    "* first column: model coefficients\n",
    "* second column: standard error in these coefficients (this is the uncertainty on the fitted coefficient values)\n",
    "\n",
    "#### Introduction to the ARMAX models\n",
    "* **Exogenous ARMA**\n",
    "* One possible extension to the ARMA model is to use exogenous inputs to create the ARMAX model. \n",
    "* This means that we model the time series using other independent variables as well as the time series itself. \n",
    "* Use external variables as well as time series\n",
    "* ARMAX = ARMA + linear regression\n",
    "* ARMAX is like a combination between an ARMA model and a normal linear regression model \n",
    "\n",
    "* **Equation for simple ARMA(1,1) model:**\n",
    "* $y_t = a_1y_{t-1}+m_1\\epsilon_{t-1}+\\epsilon_t$\n",
    "\n",
    "* **Equation for simple ARMAX(1,1) model:**\n",
    "* $y_t = x_1z_t +a_1y_{t-1}+m_1\\epsilon_{t-1}+\\epsilon_t$\n",
    "\n",
    "* The only difference is one extra term: $x_1z_t$\n",
    "* New independent variable: $z_t$\n",
    "* Its coefficient: $x_1$\n",
    "\n",
    "\n",
    "* When might ARMAX be useful?\n",
    "* Imagine you wanted to model your own daily personal productivity\n",
    "* This could reasonably be an ARMA model as your productivity on previous days may have an effect on your productivity today (you could be overworked or just on a roll).\n",
    "* A useful exogenous variable could be the amount of sleep you got the night before, since this might affect your productivity\n",
    "* Here, $z_1$ would be hours slept, and if more sleep made you more productive, then the coefficient $x_1$ would be positive. \n",
    "* We can fit an ARMAX model using the same ARMA model class we used before. The only difference is that we will now feed in our exogenous variabe using the `exog` keyword.\n",
    "* The model order and the fitting procedure are just the same. \n",
    "\n",
    "#### Fitting ARMAX\n",
    "\n",
    "```\n",
    "# Instantiate the model\n",
    "model = ARMA(df['productivity'], order=(2,1), exog=df['hours_sleep'])\n",
    "```\n",
    "\n",
    "* The ARMAX summary now includes information in the $x_1$ row for the exogenous variable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4b70d",
   "metadata": {},
   "source": [
    "#### Forecasting\n",
    "* Now that we know how to fit models, let's use them to predict the future. \n",
    "* $y_t = a_1y_{t-1} + \\epsilon_t$\n",
    "* $y_t = 0.6 * 10 + \\epsilon_t$\n",
    "* $y_t = 6.0 + \\epsilon_t$\n",
    "\n",
    "* If the shock term had a standard deviation of 1, we would predict our lower and upper uncertainty levels to be 5 and\n",
    "7\n",
    "\n",
    "* **$5.0<y_t<7.0$**\n",
    "* In the time period we have data for, we can make lots of these predictions in-sample; using the previous series value to extimate the next ones. This is called a **one-step_ahead prediction.** This allows us to evaluate how good out model is at predicting just one value ahead. \n",
    "* The uncertainty is due to the random shock terms that we can't predict\n",
    "\n",
    "* Previously we had been using the ARMA model class from statsmodels.\n",
    "\n",
    "## SARIMAX\n",
    "* The SARIMAX model class can do everything that the ARMA model class can do and can be extended even further\n",
    "* This class has an additional model order for **Seasonality**\n",
    "* For now, a SARIMAX model with order **p, 0, q** is exactly the same as an **ARMA(p, q)** model. \n",
    "* We can also add a constant to the model using trend equals c.\n",
    "* If the time series isn't centered around zero, the constant c is a must. \n",
    "\n",
    "```\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Just an ARMA(p,q) model\n",
    "model = SARIMAX(df, order=(p, 0, q))\n",
    "```\n",
    "\n",
    "#### Making one-step-ahead predictions\n",
    "* Starting from a SARIMAX fitted results object, we can use its `get_prediction` method to generate in-sample predictions. \n",
    "\n",
    "```\n",
    "# Make predictions for last 25 values\n",
    "results = model.fit()\n",
    "# Make in-sample prediction\n",
    "forecast = results.get_prediction(start=-25)\n",
    "# Get confidence intervals of forecasts\n",
    "confidence_intervals = forecast.conf_int()\n",
    "```\n",
    "* We set the start parameter as a negatice integer stating how many steps back to begin the forecast.\n",
    "* Setting start to -25 means we make predictions for the last 25 entries of the training data. \n",
    "* This returns a forecast object\n",
    "* The central value of the forecast is stored in the `predicted_mean` attribute of the forecast object\n",
    "* This predicted mean is a pandas series\n",
    "* To get the lower and upper limits on the values of our predictions, we use the `conf_int` method of the forecast object\n",
    "* This generates a pandas DataFrame of the lower and upper certainty range of our prediction\n",
    "* We can use pyplot's `.plot` method to plot the mean value:\n",
    "\n",
    "\n",
    "```\n",
    "plt.figure()\n",
    "\n",
    "# Plot prediction\n",
    "plt.plot(dates,\n",
    "         mean_forecast.values,\n",
    "         color='red',\n",
    "         label='forecast')\n",
    "# Shade uncertainty area\n",
    "plt.fill_between(dates, lower_limits, upper_limits, color='pink')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "* We use `pyplot.fill_between` to shade the area between our lower and upper limits\n",
    "* We can also make predictions further than just one step ahead \n",
    "\n",
    "#### Dynamic predictions\n",
    "* To make these dynamic predictions we predict one step ahead and use this predicted value to forecast the next value after that... and so on. \n",
    "* Since we don't know the shock term at each step, our uncertainty can grow very quickly.\n",
    "* Making dynamic predictions is very similar to making one-step-ahead predictions\n",
    "\n",
    "```\n",
    "results = model.fit()\n",
    "forecast = results.get_prediction(start=-25, dynamic=True)\n",
    "```\n",
    "* The only difference is that in the `get_predictions` method, we set the parameter `dynamic = True`\n",
    "\n",
    "```\n",
    "results = model.fit()\n",
    "forecast = results.get_prediction(start=-25, dynamic=True)\n",
    "#forecast mean\n",
    "mean_forecast = forecast.predicted_mean\n",
    "# Get confidence intervals of forecasts\n",
    "confidence_intervals = forecast.conf_int()\n",
    "```\n",
    "\n",
    "* Finally, after testing our predictions in-sample, we can use our model to predict the future \n",
    "* To make future forecasts, we use the `get_forecast` method of the results object:\n",
    "\n",
    "```\n",
    "forecast = results.get_forecast(steps=20)\n",
    "```\n",
    "\n",
    "* We choose the number of steps after the end of the training data to forecast up to.\n",
    "* Everything else neatly follows as before:\n",
    "\n",
    "```\n",
    "#forecast mean\n",
    "mean_forecast = forecast.predicted_mean\n",
    "\n",
    "# Get confidence intervals of forecasts\n",
    "confidence_intervals = forecast.conf_int()\n",
    "```\n",
    "* This is also a dynamic forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08d35a",
   "metadata": {},
   "source": [
    "```\n",
    "# Generate predictions\n",
    "one_step_forecast = results.get_prediction(start=-30)\n",
    "\n",
    "# Extract prediction mean\n",
    "mean_forecast = one_step_forecast.predicted_mean\n",
    "\n",
    "# Get confidence intervals of predictions\n",
    "confidence_intervals = one_step_forecast.conf_int()\n",
    "\n",
    "# Select lower and upper confidence limits\n",
    "lower_limits = confidence_intervals.loc[:,'lower close']\n",
    "upper_limits = confidence_intervals.loc[:,'upper close']\n",
    "\n",
    "# Print best estimate predictions\n",
    "print(mean_forecast)\n",
    "```\n",
    "\n",
    "```\n",
    "# plot the amazon data\n",
    "plt.plot(amazon.index, amazon, label='observed')\n",
    "\n",
    "# plot your mean predictions\n",
    "plt.plot(mean_forecast.index, mean_forecast, color='r', label='forecast')\n",
    "\n",
    "# shade the area between your confidence limits\n",
    "plt.fill_between(lower_limits.index, lower_limits,\n",
    "\t\t upper_limits, color='pink')\n",
    "\n",
    "# set labels, legends and show plot\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amazon Stock Price - Close USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Generate predictions\n",
    "dynamic_forecast = results.get_prediction(start=-30, dynamic=True)\n",
    "\n",
    "# Extract prediction mean\n",
    "mean_forecast = dynamic_forecast.predicted_mean\n",
    "\n",
    "# Get confidence intervals of predictions\n",
    "confidence_intervals = dynamic_forecast.conf_int()\n",
    "\n",
    "# Select lower and upper confidence limits\n",
    "lower_limits = confidence_intervals.loc[:,'lower close']\n",
    "upper_limits = confidence_intervals.loc[:,'upper close']\n",
    "\n",
    "# Print best estimate predictions\n",
    "print(mean_forecast)\n",
    "```\n",
    "\n",
    "```\n",
    "# plot the amazon data\n",
    "plt.plot(amazon.index, amazon, label='observed')\n",
    "\n",
    "# plot your mean forecast\n",
    "plt.plot(mean_forecast.index, mean_forecast, color='r', label='forecast')\n",
    "\n",
    "# shade the area between your confidence limits\n",
    "plt.fill_between(lower_limits.index, lower_limits, \n",
    "         upper_limits, color='pink')\n",
    "\n",
    "# set labels, legends and show plot\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amazon Stock Price - Close USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7747d14",
   "metadata": {},
   "source": [
    "#### Introduction to ARIMA models\n",
    "* Tying together forecasting and non-stationarity and show how to overcome some problems that arise when these things meet.\n",
    "* Remember: You cannot apply an ARMA model to non-stationary time series.\n",
    "    * Take the difference of the time series\n",
    "    * Once the data is stationary, we can model it with an ARMA model.\n",
    "    * **However, when we do this, we will have a model which is trained to predict *the value of the difference of the time series.***\n",
    "    * Obviously, what we really want to predict is not the difference, but the actual value of the time series\n",
    "    * We can achieve this by careful transforming our prediction of the differences.\n",
    "    \n",
    "    \n",
    "## Reconstructing original time series after differencing\n",
    "\n",
    "```\n",
    "diff_forecast = results.get_forecast(steps=10).predicted_mean\n",
    "from numpy import cumsum\n",
    "mean_forecast = cumsum(diff_forecast) + df.iloc[-1,0]\n",
    "```\n",
    "\n",
    "* We start with predictions of the difference values \n",
    "* **The opposite of taking the difference is taking the cumulative sum or integral.**\n",
    "* We will need to use this transform to go from predictions of the difference values to predictions of the absolute values: `np.cumsum`\n",
    "* If we apply this function, we now have a prediction of how much the time series changed from its initial value over the forecast period. \n",
    "* To get an **absolute value, we need to add the last value of the original time series to this.**\n",
    "* We now have a forecast of the non-stationary time series\n",
    "* If we would like to plot our uncertainties as before, we will need to carefully transform them as well.\n",
    "\n",
    "\n",
    "#### ARIMA model:\n",
    "* Take the difference\n",
    "* Fit the ARMA model\n",
    "* **Integrate the forecast**\n",
    "\n",
    "\n",
    "* These steps are very common in time series modeling, and are referred to as ARIMA\n",
    "* This is a lot of work, but thankfully there is an extension of the ARMA model which does this work for us.\n",
    "* **This is the ARIMA model, or, Autoregressive Integrated Moving Average model.**\n",
    "* We can implement an ARIMA model using the SARIMAX model class from statsmodels. \n",
    "* The ARIMA model has three model orders:\n",
    "    * **P**: number of autoregressive lags (AR order)\n",
    "    * **D**: order of differencing\n",
    "    * **Q**: moving average order\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "model = SARIMAX(df, order = (p,d,q))\n",
    "```\n",
    "\n",
    "* ARIMA(p, 0, q) = ARMA(p, q)\n",
    "\n",
    "#### Using the ARIMA model\n",
    "* Pass it in a non-differenced time series and the model order:\n",
    "* Below, we want to differene the time series data just once and then apply an ARMA(2, 1) model.\n",
    "* This achieved by using an ARIMA(2,1,1) model.\n",
    "* Once we have stated the difference parameter, we don't need to worry about differencing any more.\n",
    "\n",
    "```\n",
    "# Create model\n",
    "model = SARIMAX(df, order=(2,1,1))\n",
    "# Fit model\n",
    "model.fit()\n",
    "# Make forecast\n",
    "mean_forecast = results.get_forecast(steps=10).predicted_mean\n",
    "```\n",
    "* The differencing and integration steps are all taken care of by the model object\n",
    "* We must still be careful about selecting the right amount of differencing\n",
    "* Remember: we difference our data only until it is stationary and no more\n",
    "* We will work this out before we apply our model, using the Augmented Dickey-Fuller test to decide the difference order\n",
    "* By the time we come to apply a model, we already know the degree of differencing we should apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9ded5",
   "metadata": {},
   "source": [
    "```\n",
    "# Take the first difference of the data\n",
    "amazon_diff = amazon.diff().dropna()\n",
    "\n",
    "# Create ARMA(2,2) model\n",
    "arma = SARIMAX(amazon_diff, order = (2, 0, 2))\n",
    "\n",
    "# Fit model\n",
    "arma_results = arma.fit()\n",
    "\n",
    "# Print fit summary\n",
    "print(arma_results.summary())\n",
    "```\n",
    "\n",
    "```\n",
    "# Make arma forecast of next 10 differences\n",
    "arma_diff_forecast = arma_results.get_forecast(steps=10).predicted_mean\n",
    "\n",
    "# Integrate the difference forecast\n",
    "arma_int_forecast = np.cumsum(arma_diff_forecast)\n",
    "\n",
    "# Make absolute value forecast\n",
    "arma_value_forecast = arma_int_forecast + amazon.iloc[-1,0]\n",
    "\n",
    "# Print forecast\n",
    "print(arma_value_forecast)\n",
    "```\n",
    "\n",
    "```\n",
    "# Create ARIMA(2,1,2) model\n",
    "arima = SARIMAX(amazon, order=(2,1,2))\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_results = arima.fit()\n",
    "\n",
    "# Make ARIMA forecast of next 10 values\n",
    "arima_value_forecast = arima_results.get_forecast(steps=10).predicted_mean\n",
    "\n",
    "# Print forecast\n",
    "print(arima_value_forecast)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229841f",
   "metadata": {},
   "source": [
    "## CHAPTER 3- The Best of the Best Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428eb870",
   "metadata": {},
   "source": [
    " You'll learn how to identify promising model orders from the data itself, then, once the most promising models have been trained, you'll learn how to choose the best model from this fitted selection. You'll also learn a great framework for structuring your time series projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09f7c2",
   "metadata": {},
   "source": [
    "#### Intro to ACF and PACF\n",
    "* How do we choose which ARIMA model to fit?\n",
    "* The model order is very important to the quality of the forecasts.\n",
    "* **One of the main ways of identifying the correct model order is by using the autocorrelation function, the ACF, and the partial autocorrelation function, the PACF.**\n",
    "    * **ACF**: Autocorrelation function\n",
    "    * **PACF**: Partial autocorrelation function \n",
    "\n",
    "\n",
    "#### What is the ACF?\n",
    "    * The autocorrelation function at lag-1 is the correlation between a time series and the same time series offset by one step.\n",
    "        * lag-1 autocorrelation $\\Rightarrow$ $corr(y_t,y_{t-1})$\n",
    "    * The autocorrelation at lag-2 is the correlation between a time series and itself offset by two steps\n",
    "        * lag-2 autocorrelation $\\Rightarrow$ $corr(y_t,y_{t-2})$\n",
    "    * ...\n",
    "    * The autocorrelation function at lag-n is the correlation between a time series and the same time series offset by n steps\n",
    "        * lag-n autocorrelation $\\Rightarrow$ $corr(y_t,y_{t-n})$\n",
    "* When we talk about the autocorrelation function, we mean the set of correlation values for different lags. \n",
    "\n",
    "#### What is the PACF?\n",
    "* The partial autocorrelation is the correlation between a time series and the lagged version of itself **after we subtract the effect of correlation at smaller lags.**\n",
    "    * So, **it is the correlation associated with just that particular lag.**\n",
    "    \n",
    "#### Using ACF and PACF to choose model order\n",
    "* **By comparing the ACF and PACF for a time series we can deduce the model order.**\n",
    "    * $\\star$ If the amplitude of the ACF tails off with increasing lag, and the PACF cuts off after some lag p, then we have an AR(p) model.\n",
    "    * $\\star$ If the amplitude of the ACF cuts off after some lag q, and the amplitude of the PACF tails off then we have MA(q) model.\n",
    "    * $\\star$ If both the ACF and PACF tail off then we have an ARMA model.\n",
    "        * In this case, we can't deduce the model orders of p and q from the plot\n",
    "    \n",
    "    \n",
    "    \n",
    "#### AR(p)\n",
    "* ACF: Tails off\n",
    "* PACF: Cuts off after lag p\n",
    "\n",
    "#### MA(q)\n",
    "* ACF: Cuts off after lag q\n",
    "* PACF: Tails off\n",
    "\n",
    "#### ARMA(p, q)\n",
    "* ACF: Tails off\n",
    "* PACF: Tails off\n",
    "* In this case, we can't deduce the model orders of p and q from the plots\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "```\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,8))\n",
    "# Make ACF plot\n",
    "plot_acf(df, lags=10, zero=False, ax=ax1)\n",
    "# Make PACF plot\n",
    "plot_pacf(df, lags=10, zero=False, ax=ax2)\n",
    "\n",
    "plt.show())\n",
    "```\n",
    "\n",
    "* To use them, we start by creating a figure with two sublots.\n",
    "* Into each function we pass the time series DataFrame and the maximum number of lags we would like to see.\n",
    "* We also tell it whether to show the autocorrelation at lag-0 (`zero=False`).\n",
    "    * The ACF and PACF at lag-0 will always have a value of one, so we'll set this arguent to false to simplify the plot\n",
    "* **NOTE!:** The time series MUST be made stationary BEFORE making these plots. \n",
    "    * $\\Rightarrow$ If the ACF values are high and tail off very, very slowly, this is a sign that the data is non-stationary, and so needs to be differenced.\n",
    "    * $\\Rightarrow$ If the autocorrelation at lag-1 is very negative, this is a sign that we have taken the difference too many times.\n",
    "\n",
    "```\n",
    "# Import\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n",
    " \n",
    "# Plot the ACF of df\n",
    "plot_acf(df, lags=10, zero=False, ax=ax1)\n",
    "\n",
    "# Plot the PACF of df\n",
    "plot_pacf(df, lags=10, zero=False, ax=ax2)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n",
    "\n",
    "# Plot ACF and PACF\n",
    "plot_acf(earthquake, lags=10, zero=False, ax=ax1)\n",
    "plot_pacf(earthquake, lags=10, zero=False, ax=ax2)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Instantiate model\n",
    "model = SARIMAX(earthquake, order=(1,0,0))\n",
    "\n",
    "# Train model\n",
    "results = model.fit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbd37d",
   "metadata": {},
   "source": [
    "### Intro to AIC and BIC\n",
    "* In the last chapter, we mentioned how ACF and PACF can't be used to choose the order of a model, when both of the orders p and q are non-zero.\n",
    "* However, there are more tools we can use: the **AIC** and the **BIC**.\n",
    "\n",
    "#### The Akaike Information Criterion (AIC)\n",
    "* The AIC is a metric which tells us how good a model is. \n",
    "* A model which makes better predications is given a lower AIC score\n",
    "* The AIC also penalies models which have lots of parameters\n",
    "    * This means, if we set the order too high compared to the data, we will get a high AIC value\n",
    "    * This stops us overfitting to the training data. \n",
    "    \n",
    "#### The Bayesian Information Criterion (BIC)\n",
    "* Very similar to AIC\n",
    "* Models which fit the data better have lower BIC's \n",
    "* The BIC penalizes overly complex models\n",
    "\n",
    "#### AIC vs BIC\n",
    "* For both of these metrics a lower value suggests a better model. \n",
    "* The difference between these two metrics is how much they penalize model complexity\n",
    "* The BIC penalizes additional model orders more than AIC, and so the BIC will sometimes suggest a simpler model\n",
    "* The AIC and BIC will often choose the same model, but when they don't, we will have to make a choice. \n",
    "    * **AIC is better at choosing predictive models.**\n",
    "    * **BIC is better at choose a good explanatory model.**\n",
    "* After fitting a model in Python, we can find both the AIC and BIC by using the summary of the fitted model results object:\n",
    "\n",
    "```\n",
    "# Create a model\n",
    "model = SARIMAX(df, order =(1,0,1))\n",
    "# Fit model \n",
    "results = model.fit()\n",
    "# Print fit summary \n",
    "print(results.summary())\n",
    "\n",
    "# Print AIC and BIC\n",
    "print('AIC:', results.aic)\n",
    "print('BIC:', results.bic)\n",
    "```\n",
    "* **You can also access the AIC and BIC directly by using the `.aic` and `.bic`attributes of the fitted model results object.**\n",
    "* **Being able to access the AIC and BIC directly means we can write for loops to fit multiple ARIMA models to a dataset, to find the best model order.**\n",
    "\n",
    "#### Searching over AIC and BIC\n",
    "* **You can also access the AIC and BIC directly by using the `.aic` and `.bic`attributes of the fitted model results object.**\n",
    "* **Being able to access the AIC and BIC directly means we can write for loops to fit multiple ARIMA models to a dataset, to find the best model order.**\n",
    "\n",
    "```\n",
    "# Loop over AR order\n",
    "for p in range(3):\n",
    "    # Loop over MA order\n",
    "    for q in range(3):\n",
    "        # Fit model\n",
    "        model = SARIMAX(df, order=(p,0,q))\n",
    "        results = model.fit()\n",
    "        # print the model order and the AIC/BIC values\n",
    "        print(p, q, results.aic, results.bic)\n",
    "```\n",
    "\n",
    "* If we want to test a large number of model orders, we can append the model order and the AIC and BIC to a list, and later convert it to a DataFrame\n",
    "\n",
    "```\n",
    "# Loop over AR order\n",
    "for p in range(3):\n",
    "    # Loop over MA order\n",
    "    for q in range(3):\n",
    "        # Fit model\n",
    "        model = SARIMAX(df, order=(p,0,q))\n",
    "        results = model.fit()\n",
    "        #Add order and scores to list\n",
    "        order_aic_bic.append((p, q, results.aic, results.bic))\n",
    "# Make DataFrame of model order and AIC/BIC scores\n",
    "order_df = pd. DataFrame(order_aic_bic, columns=['p', 'q', 'aic', 'bic'])\n",
    "```\n",
    "* This means we can sort by the AIC score and not have to search through the orders by eye\n",
    "\n",
    "```\n",
    "# Sort by AIC\n",
    "print(order_df.sort_values('aic'))\n",
    "# Sort by BIC\n",
    "print(order_df.sort_values('bic'))\n",
    "```\n",
    "* Reminder: When AIC and BIC favor different models, determine whether you'd rather a good predictive model (AIC) or a good explanatory model (BIC), and choose accordingly.\n",
    "\n",
    "* Sometimes when searching over model orders, you will attempt to fit an order that leads to an error\n",
    "\n",
    "```\n",
    "# Fit model\n",
    "model = SARIMAX(df, order=(2,0,1))\n",
    "results = model.fit()\n",
    "\n",
    "ValueError: Non-stationary starting autoregressice parameters found with 'enforce_stationarity' set to True\n",
    "```\n",
    "* The above ValueError tells us that we have tried to fit a model which would result in a non-stationary set of AR coefficients\n",
    "* This is just a bad model for this data, and when we loop over p and q we would like to skip this one (above example)\n",
    "* **We can skip these orders in our loop by using a try and except block in Python.**\n",
    "\n",
    "#### When certain orders don't work\n",
    "* We can skip certain oders that don't work in our for loop by using a **try and except block** in Python.\n",
    "\n",
    "```\n",
    "# Loop over AR order\n",
    "for p in range(3):\n",
    "    # Loop over MA order\n",
    "    for q in range(3):\n",
    "        try:\n",
    "            # Fit model\n",
    "            model = SARIMAX(df, order=(p, 0, q))\n",
    "            results = model.fit()\n",
    "            \n",
    "            # Print the model order and the AIC/BIC values\n",
    "            print(p, q, results.aic, results.bic)\n",
    "        except:\n",
    "            # Print AIC and BIC as None when fails\n",
    "            print(p, q, None, None)\n",
    "```\n",
    "\n",
    "```\n",
    "# Create empty list to store search results\n",
    "order_aic_bic=[]\n",
    "\n",
    "# Loop over p values from 0-2\n",
    "for p in range(3):\n",
    "  # Loop over q values from 0-2\n",
    "    for q in range(3):\n",
    "      \t# create and fit ARMA(p,q) model\n",
    "        model = SARIMAX(df, order=(p, 0, q))\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Append order and results tuple\n",
    "        order_aic_bic.append((p, q, results.aic, results.bic))\n",
    "\n",
    "# Construct DataFrame from order_aic_bic\n",
    "order_df = pd.DataFrame(order_aic_bic, \n",
    "                        columns=['p', 'q', 'AIC', 'BIC'])\n",
    "\n",
    "# Print order_df in order of increasing AIC\n",
    "print(order_df.sort_values('AIC'))\n",
    "\n",
    "# Print order_df in order of increasing BIC\n",
    "print(order_df.sort_values('BIC'))\n",
    "```\n",
    "***\n",
    "\n",
    "```\n",
    "# Loop over p values from 0-2\n",
    "for p in range(3):\n",
    "    # Loop over q values from 0-2\n",
    "    for q in range(3):\n",
    "      \n",
    "        try:\n",
    "            # create and fit ARMA(p,q) model\n",
    "            model = SARIMAX(earthquake, order = (p, 0, q))\n",
    "            results = model.fit()\n",
    "            \n",
    "            # Print order and results\n",
    "            print(p, q, results.aic, results.bic)\n",
    "            \n",
    "        except:\n",
    "            print(p, q, None, None)     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d1a81",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "* Our work isn't finished oce we've built our model.\n",
    "* The next step is using common model diagnostics to confirm our model is behaving well. \n",
    "* After we have picked the final model, or the final few models, we should ask how good they are.\n",
    "* **To diagnose our model, we focus on the residuals to the training data.**\n",
    "    * The residuals are the difference between our model's one-step ahead predictions and the real values of the time series.\n",
    "    * In statsmodels, the residuals over the training period can be accessed using the `.resid` attribute of the results object.\n",
    "    \n",
    "```\n",
    "# Fit model\n",
    "model = SARIMAX(df, order=(p, d, q))\n",
    "results = model.fit()\n",
    "# Assign residual to variable \n",
    "residuals = results.resid\n",
    "```\n",
    "* We might like to know, on average, how large the residuals are and, so, how far our predictions are from the true values.\n",
    "* To answer this, we can calculate the mean absolute error of the residuals\n",
    "* `mae = np.mean(np.abs(residuals))`\n",
    "* **For an ideal model, the residuals should be uncorrelated Gaussian white noise, centered on zero.**\n",
    "* The rest of our diagnostics will help us to see if this true. \n",
    "\n",
    "#### Create the 4 diagnostics plots\n",
    "* 1) Standardized residual\n",
    "* 2) Histogram plus estimated density\n",
    "* 3) Normal Q-Q\n",
    "* 4) Correlogram\n",
    "\n",
    "```\n",
    "results.plot_diagnostics()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Standardized residual\n",
    "* Shows the one-step-ahead standardized residuals\n",
    "* If our model is working correctly, there should be no obvious structure in the residuals\n",
    "\n",
    "#### Histogram plus estimated density\n",
    "* Shows us the distribution of the residuals \n",
    "* The histogram shows us the measured distribution\n",
    "* The orange line shows us a smoothed version of this histogram (KDE; Kernel Density Estimate)\n",
    "* The green line shows a normal distribution: N(0,1)\n",
    "* If our model is good, the orange and green lines should be almost the same.\n",
    "\n",
    "#### Normal Q-Q\n",
    "* The normal Q-Q plot is another way to show how the distribution of the model residuals compares to a normal distribution.\n",
    "* If our residuals are normally distributed, then all the points should lie along the red line, except perhaps some values at either end. \n",
    "\n",
    "#### Correlogram \n",
    "* The last plot is the correlogram, which is just **an ACF plot of the residuals rather than the data.**\n",
    "* 95% of the correlations for lag greater than zero should not be significant.\n",
    "* If there is significant correlation in the residuals, it means that there is information in the data that our model hasn't captured. \n",
    "\n",
    "#### Summary statistics\n",
    "* Some of these plot also have accompanying test statistics in `results.summary()` tables.\n",
    "* **`Prob(Q)`** is the p-value associated with the null hypothesis that the residuals have no correlation structure. \n",
    "* **`Prob(JB)`** is the p-value associated with the null hypothesis that the residuals are Gaussian normally distributed.\n",
    "* If either p-value above is less than 0.05, we reject that respective null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcdbc8",
   "metadata": {},
   "source": [
    "```\n",
    "# Fit model\n",
    "model = SARIMAX(earthquake, order=(1,0,1))\n",
    "results = model.fit()\n",
    "\n",
    "# Calculate the mean absolute error from residuals\n",
    "mae = np.mean(np.abs(results.resid))\n",
    "\n",
    "# Print mean absolute error\n",
    "print(mae)\n",
    "\n",
    "# Make plot of time series for comparison\n",
    "earthquake.plot()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Create and fit model\n",
    "model1 = SARIMAX(df, order=(3, 0, 1))\n",
    "results1 = model1.fit()\n",
    "\n",
    "# Print summary\n",
    "print(results1.summary())\n",
    "```\n",
    "\n",
    "```\n",
    "# Create and fit model\n",
    "model = SARIMAX(df, order=(1, 1, 1))\n",
    "results=model.fit()\n",
    "\n",
    "# Create the 4 diagostics plots\n",
    "results.plot_diagnostics()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61730a",
   "metadata": {},
   "source": [
    "### Box-Jenkins method\n",
    "* In the previous lessons you've learned lots of tools and methods for working with and modeling time series\n",
    "* In this lesson: learn about the best practices framework for using these tools.\n",
    "\n",
    "#### The Box-Jenkins method\n",
    "* The Box-Jenkins method is a kind of checklist for you to go from raw data to a model ready for production.\n",
    "* The three main steps that stand between you and a production-ready model are:\n",
    "    * identification\n",
    "    * estimation \n",
    "    * model diagnostics\n",
    "\n",
    "**Identification** $\\Rightarrow$ **Estimation** $\\Rightarrow$ **Model diagnostics**\n",
    "\n",
    "#### Identification\n",
    "* In the **identification** step, we explore and characterize the data to find some form of it which is appropriate to ARIMA modeling. \n",
    "* Is the time series stationary?\n",
    "* What differencing will make it stationary?\n",
    "* What transformations will make it stationary? \n",
    "* What values of p and q are most promising?\n",
    "* **Identification tools:**\n",
    "    * Plot the time series:\n",
    "        * `df.plot()`\n",
    "    * Use Augmented Dickey-Fuller test\n",
    "        * `adfuller()`\n",
    "    * Use transforms and/or differencing\n",
    "        * `df.diff()`, `np.log()`, `np.sqrt()`\n",
    "    * Plot ACF/PACF:\n",
    "        * `plot_acf()`, `plot_pacf()`\n",
    "        \n",
    "#### Estimation\n",
    "* Use the data to train the model coefficients\n",
    "    * Done for us using `model.fit()`\n",
    "* Choose between models using AIC and BIC\n",
    "    * `results.aic`, `results.bic`\n",
    "    \n",
    "#### Model diagnostics\n",
    "* Evaluate the quality of the best fitting model\n",
    "* Here is where we use our test statistics and diagnostic plots to make sure the residuals are well behaved.\n",
    "* Are the results uncorrelated?\n",
    "* Are the residuals normally distributed?\n",
    "    * `results.plot_diagnostics()`\n",
    "    * `results.summary()`\n",
    "    \n",
    "#### Decision\n",
    "* Using the information gathered from the statistical tests and plots during the diagnostic step, we need to make a decision:\n",
    "    * Is the model good enough, or do we need to go back and rework it?\n",
    "    \n",
    "* If the residuals aren't as they should be we will go back and rethink our choices in the earlier steps\n",
    "* If the residuals are okay, then we an go ahead and make forecasts\n",
    "\n",
    "* **This should be your general project workflow when developing time series models.**\n",
    "* You may have to repeat the process a few times in order to build a model that fits well.\n",
    "\n",
    "#### Identification I\n",
    "\n",
    "```\n",
    "# Plot time series\n",
    "savings.plot()\n",
    "plt.show()\n",
    "\n",
    "# Run Dicky-Fuller test\n",
    "result = adfuller(savings['savings'])\n",
    "\n",
    "# Print test statistic\n",
    "print(result[0])\n",
    "\n",
    "# Print p-value\n",
    "print(result[1])\n",
    "```\n",
    "\n",
    "#### Identification II\n",
    "\n",
    "```\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n",
    " \n",
    "# Plot the ACF of savings on ax1\n",
    "plot_acf(savings, lags=10, zero=False, ax=ax1)\n",
    "\n",
    "# Plot the PACF of savings on ax2\n",
    "plot_pacf(savings, lags=10, zero=False, ax=ax2)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Estimation\n",
    "\n",
    "```\n",
    "# Loop over p values from 0-3\n",
    "for p in range(4):\n",
    "  \n",
    "  # Loop over q values from 0-3\n",
    "    for q in range(4):\n",
    "      try:\n",
    "        # Create and fit ARMA(p,q) model\n",
    "        model = SARIMAX(savings, order=(p,0,q), trend='c')\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Print p, q, AIC, BIC\n",
    "        print(p, q, results.aic, results.bic)\n",
    "        \n",
    "      except:\n",
    "        print(p, q, None, None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d0137",
   "metadata": {},
   "source": [
    "## Chapter 4: Seasonal ARIMA Models\n",
    "Learn how to use seasonal ARIMA models to fit more complex data. Learn how to decompose this data into seasonal and non-seasonal parts and then get a change to utilize all your ARIMA tools on one last global forecast challenge.\n",
    "\n",
    "#### Seasonal Time Series\n",
    "* ARIMA models can be used to predict a great many types of time series\n",
    "* In this chapter, learn about **seasonal time series.**\n",
    "* Seasonal data has predictable and repeated patterns.\n",
    "* Repeats after any amount of time.\n",
    "    * These seasonal cycles might repeat every year, every week, every hour, etc. ...\n",
    "    \n",
    "* **We can think of any time series as being made of 3 parts:**\n",
    "    * The trend\n",
    "    * The seasonal component\n",
    "    * The residual\n",
    "* **The full time series is these three parts added together.**\n",
    "\n",
    "#### Seasonal decomposition using statsmodels\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Decompose data\n",
    "decomp_results = seasonal_decompose(df['IPG3113N'], period=12)\n",
    "```\n",
    "* We use the function by passing in the series\n",
    "* We also need to set the period parameter, which is the number of data points in each repeated cycle. In the example above, our cycle repeats every 12 steps.\n",
    "* This function retuns a decompose-results object:\n",
    "* `type(decomp_results)`\n",
    "* `statsmodels.tsa.seasonal.DecomposeResult`\n",
    "* We can use the plot method of this object to plot the components\n",
    "\n",
    "```\n",
    "# Plot decomposed data\n",
    "decomp_results.plot()\n",
    "plt.show()\n",
    "```\n",
    "* In order to decompose the data, we need to know how often the cycles repeat.\n",
    "* Often, you'll be able to guess this, but we can also use the ACF to identify the period.\n",
    "* The ACF shows a **periodic correlation pattern.**\n",
    "* To find the **period**, we look for a lag greater than one, which is a peak in the ACF plot.\n",
    "* For example, if the first peak after lag 1 is at 12 lags, the seasonal component repeats every 12 time steps\n",
    "\n",
    "* Sometimes it can be hard to tell by eye whether a time series is seasonal or not\n",
    "    * This is where the ACF is particularly useful. \n",
    "    \n",
    "#### Detrending time series\n",
    "\n",
    "* We have detrended time series before by taking the difference.\n",
    "* However, this time we are only trying to find the period of the time series, and the ACF plot will be clearer if we just subtract the rolling mean. \n",
    "* **Rememver that you can detrend by subtracting the moving average using a window size of any value bigger than the likely period.**\n",
    "\n",
    "```\n",
    "# Subtract long rolling average over N steps\n",
    "df = df - df.rolling(N).mean()\n",
    "# Drop NaN values\n",
    "df = df.dropna()\n",
    "```\n",
    "\n",
    "* The time series is now stationary and it will be easier to interpret the ACF plot \n",
    "* We plot the ACF of the detrended data and we can clearly see that there is a seasonal period of 12 steps\n",
    "\n",
    "```\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(df.dropna(), ax=ax, lags=25, zero=False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "* Since the data is seasonal, we will always have correlated residuals left if we try to fit an ARIMA model to it (meaning we aren't using all of the information in the data, and so we aren't making the best predictions possible). \n",
    "\n",
    "```\n",
    "# Import seasonal decompose\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Perform additive decomposition\n",
    "decomp = seasonal_decompose(milk_production['pounds_per_cow'], \n",
    "                            period=12)\n",
    "\n",
    "# Plot decomposition\n",
    "decomp.plot()\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "# Create figure and subplot\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the ACF on ax1\n",
    "plot_acf(water['water_consumers'], lags=25, zero=False,  ax=ax1)\n",
    "\n",
    "# Show figure\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "# Subtract the rolling mean\n",
    "water_2 = water - water.rolling(15).mean()\n",
    "\n",
    "# Drop the NaN values\n",
    "water_2 = water_2.dropna()\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(water_2['water_consumers'], lags=25, zero=False, ax=ax1)\n",
    "\n",
    "# Show figure\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## SARIMA models\n",
    "* Seasonal ARIMA = SARIMA\n",
    "* The tool of choice for a seasonal time series\n",
    "* Previously we saw that we could split up our time series into a seasonal component and some non-seasonal components.\n",
    "* Fitting a SARIMA model is like fitting two different ARIMA models at once: one to the seasonal part and another to the non-seasonal part\n",
    "* **Since we have these two models, we will have these two models we will have two sets of orders.**\n",
    "    * **`SARIMA(p,d,q)(P,D,Q)`**\n",
    "    * SARIMA$(p, d, q)(P, D, Q)_S$\n",
    "    \n",
    "* We have non-seasonal orders for the autoregressive, difference, and moving average parts: (p, d, q)\n",
    "* **Non-seasonal orders:**\n",
    "    * **p**: autoregressive order\n",
    "    * **d**: differencing order\n",
    "    * **q**: moving average order\n",
    "* **Seasonal Orders:**\n",
    "    * **P**: seasonal autoregressive order\n",
    "    * **D**: seasonal differencing order\n",
    "    * **Q**: seasonal moving average order\n",
    "    * **S**: number of time steps per cycle\n",
    " \n",
    "* **Note that there is also a new order, S, which is the length of the seasonal cycle.**\n",
    "\n",
    "#### ARIMA vs SARIMA\n",
    "* Simple ARIMA(2,0,1) model equation:\n",
    "    * $y_t = a_1y_{t-1}+a_2y_{t-2}+m_1\\epsilon_{t-1}+\\epsilon_t$\n",
    "    \n",
    "    \n",
    "* Simple SARIMA$(0,0,0)(2,0,1)_7$ model equation:\n",
    "    * $y_t = a_7y_{t-7}+a_{14}y_{y-14}+m_7\\epsilon_{t-7}+\\epsilon_t$\n",
    "\n",
    "\n",
    "* Above is the equation for a simple SARIMA model with season length of 7 days\n",
    "* Note: This SARIMA model only has a seasonal part; we have set the non-seasonal order to zero.\n",
    "* This particular SARIMA model will be able to capture seasonal weekly patterns, but won't be able to capture loval, day to day patterns.\n",
    "* **If we construct a SARIMA model and include non-seasonal orders as well, then we can capture both of these patterns.**\n",
    "* Fitting a SARIMA model is almost the same as fitting an ARIMA model. \n",
    "\n",
    "#### Fitting a SARIMA model\n",
    "* Fitting a SARIMA model is almost the same as fitting an ARIMA model.\n",
    "* **The only difference is that we have to specify the seasonal order, as well as the regular order, when instantiating the model.**\n",
    "* This means that **there are a lot of model orders we need to find.**\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# Instantiate model \n",
    "model = SARIMAX(df, order=(p,d,q), seasonal_order=(P,D,Q,S))\n",
    "# Fit model\n",
    "results = model.fit()\n",
    "```\n",
    "* In the last lesson, we learned how to find the seasonal period, S using the ACF\n",
    "* The next task is to find the order of differencing\n",
    "\n",
    "#### Seasonal differencing\n",
    "* To make a time series stationary, we may need to apply seasonal differencing.\n",
    "* Subtract the time series value of one season ago. \n",
    "* In seasonal differencing, instead of subtracting the most recent time series value, we subtract the time series value from one cycle ago. \n",
    "\n",
    "```\n",
    "# Take the seasonal difference\n",
    "df_diff = df.diff(S)\n",
    "```\n",
    "* This time, we pass in an **integer S, the length of the seasonal cycle.**\n",
    "* If the time series shows a trend, then we take the normal difference.\n",
    "* If there is a strong seasonal cycle, then we will also take the seasonal difference.\n",
    "* Once we have found the two orders of differencing, and made the time series stationary, we need to find the other model orders.\n",
    "* To find the non-seasonal orders, we plot the ACF and the PACF of the differenced time series.\n",
    "* To find the seasonal orders, we plot the ACF and PACF of the differenced time series at multiple seasonal steps.\n",
    "* Then, we can use the same table of ACF and PACF rules to work out the seasonal order. \n",
    "\n",
    "#### Plotting seasonal ACF and PACF\n",
    "\n",
    "```\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "# Plot seasonal ACF\n",
    "plot_acf(df_diff, lags=[12,24,36,48,60,72], ax=ax1)\n",
    "# Plot seasonal PACF\n",
    "plot_pacf(df_diff, lags=[12,24,36,48,60,72], ax=ax2)\n",
    "plt.show()\n",
    "```\n",
    "* This time we set the lags parameter to a list of lags instead of a maximum\n",
    "* This plots the ACF and PACF at these specific lags only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd205d87",
   "metadata": {},
   "source": [
    "```\n",
    "# Create a SARIMAX model\n",
    "model = SARIMAX(df1, order=(1,0,0), seasonal_order=(1,1,0,7))\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the results summary\n",
    "print(results.summary())\n",
    "```\n",
    "\n",
    "```\n",
    "# Create a SARIMAX model\n",
    "model = SARIMAX(df2, order=(2,1,1), seasonal_order=(1,0,0,4))\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the results summary\n",
    "print(results.summary())\n",
    "```\n",
    "```\n",
    "# Take the first and seasonal differences and drop NaNs\n",
    "aus_employment_diff = aus_employment.diff().diff(12).dropna()\n",
    "```\n",
    "\n",
    "```\n",
    "# Create the figure \n",
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(8,6))\n",
    "\n",
    "# Plot the ACF on ax1\n",
    "plot_acf(aus_employment_diff, lags=11, zero=False, ax=ax1)\n",
    "\n",
    "# Plot the PACF on ax2\n",
    "plot_pacf(aus_employment_diff, lags=11, zero=False, ax=ax2)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Make list of lags\n",
    "lags = [12, 24, 36, 48, 60]\n",
    "\n",
    "# Create the figure \n",
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(8,6))\n",
    "\n",
    "# Plot the ACF on ax1\n",
    "plot_acf(aus_employment_diff, zero=False, lags=lags, ax=ax1)\n",
    "\n",
    "# Plot the PACF on ax2\n",
    "plot_pacf(aus_employment_diff, zero=False, lags=lags, ax=ax2)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce226b",
   "metadata": {},
   "source": [
    "### Automation and Saving\n",
    "* A powerful tool to search over model orders plus how to save the good models you've created so you can update them later if you want to.\n",
    "\n",
    "#### Searching over model orders\n",
    "* Previously, we searched over ARIMA model orders using for-loops\n",
    "* Now that we have seasonal orders as well, this is very complex\n",
    "* **Fortunately, there is a package that will do most of this work for us: he `pmdarima` package**\n",
    "* The **`auto_arima()`** function from this package loops over model orders to find the best one. \n",
    "* The object returned by the function is the results object of the best model found by the search. \n",
    "    * This object is almost exactly like a statsmodels SARIMAX results object and has the `.summary()` and the `.plot_diagnostics()` method as well.\n",
    "\n",
    "```\n",
    "import pmdarima as pm\n",
    "results = pm.auto_arima(df)\n",
    "```\n",
    "\n",
    "#### Non-seasonal search parameters\n",
    "* The `auto_arima()` function has a lot of parameters that we may want to set.\n",
    "    * Many parameters have default values.\n",
    "    * The only *required* argument to the function is **data**.\n",
    "    * Optionally, we can also set:\n",
    "        * Non-seasonal difference order\n",
    "        * Initial estimates of the non-seasonal orders\n",
    "            * Initial guess for p\n",
    "            * Initial guess for q\n",
    "        * Maximum values of non-seasonal orders to test\n",
    "                \n",
    "```\n",
    "import pmdarima as pm\n",
    "results = pm.auto_arima(df,                                          # data\n",
    "                        d = 0,                                       # non-seasonal difference order\n",
    "                        start_p = 1,                                 # initial guess for p\n",
    "                        start_q = 1,                                 # initial guess for q\n",
    "                        max_p = 3,                                   # max_value of p to test\n",
    "                        max_q = 3,                                   # max value of q to test\n",
    "                        )                        \n",
    "```\n",
    "\n",
    "#### Seasonal search parameters\n",
    "* Must set `seasonal` argument to `True`\n",
    "* Required: data/dataframe\n",
    "* Non-seasonal arguments\n",
    "* Length of the seasonal period \n",
    "* Order of seasonal differencing\n",
    "\n",
    "\n",
    "```\n",
    "results = pm.auto_arima(df,                                          # data\n",
    "                        d = 0,                                       # non-seasonal difference order   \n",
    "                        start_p = 1,                                 # initial guess for p\n",
    "                        start_q = 1,                                 # initial guess for q\n",
    "                        max_p = 3,                                   # max value of p to test\n",
    "                        max_q = 3,                                   # max value of q to test\n",
    "                        seasonal = True,                             # is the time series seasonal\n",
    "                        m = 7,                                       # the seasonal period\n",
    "                        D = 1,                                       # seasonal difference order\n",
    "                        start_P = 1,                                 # initial guess for P\n",
    "                        start_Q = 1,                                 # initial guess for Q\n",
    "                        max_P = 2,                                   # max value of P to test\n",
    "                        max_Q = 2,                                   # max value of Q to test\n",
    "                        )                        \n",
    "```\n",
    "\n",
    "* **Finally, there are a few non-order parameters that we may want to set:**\n",
    "\n",
    "```\n",
    "results = pm.auto_arima(df,                                          # data\n",
    "                        d = 0,                                       # non-seasonal difference order   \n",
    "                        start_p = 1,                                 # initial guess for p\n",
    "                        start_q = 1,                                 # initial guess for q\n",
    "                        max_p = 3,                                   # max value of p to test\n",
    "                        max_q = 3,                                   # max value of q to test\n",
    "                        seasonal = True,                             # is the time series seasonal\n",
    "                        m = 7,                                       # the seasonal period\n",
    "                        D = 1,                                       # seasonal difference order\n",
    "                        start_P = 1,                                 # initial guess for P\n",
    "                        start_Q = 1,                                 # initial guess for Q\n",
    "                        max_P = 2,                                   # max value of P to test\n",
    "                        max_Q = 2,                                   # max value of Q to test\n",
    "                        information_criterion = 'aic',               # used to select best model\n",
    "                        trace = True,                                # print results while training\n",
    "                        error_action = 'ignore',                     # ignore orders that don't work\n",
    "                        stepwise = True,                             # apply intelligent order search\n",
    "                        )                        \n",
    "```\n",
    "\n",
    "#### Saving model objects\n",
    "* Once you have fit a model in this way, you may want to save it and load it later\n",
    "* Do this using the `joblib` package\n",
    "\n",
    "```\n",
    "import joblib\n",
    "# Select a filepath\n",
    "filepath = 'localpath/great_model.pkl'\n",
    "\n",
    "# Save model to filepath\n",
    "joblib.dump(model_results_object, filepath)\n",
    "```\n",
    "* Later on, when we want to make new predictions, we can load this model again, like so:\n",
    "\n",
    "```\n",
    "# Select a filepath\n",
    "filepath = 'localpath/great_model.pkl'\n",
    "\n",
    "# Load model object from filepath\n",
    "model_results_object = joblib.load(filepath)\n",
    "```\n",
    "\n",
    "#### Updating model\n",
    "* Some time may have passed since we trained the save model, and we may want to incorporate data that we have collected since then.\n",
    "* We can do this using the `pmdarima` model's `.update()` method:\n",
    "\n",
    "```\n",
    "# Add new observations and update parameters\n",
    "model_results_object.update(df_new)\n",
    "```\n",
    "* This adds the new observations in `df_new` and updates the model parameters\n",
    "* **This isn't the same as choosing the model order again** and so if you are updating with a large amount of new data, it may be best to back back to the start of the Box-Jenkins method.\n",
    "* **Updated time series models with new data is really important since they use the most recent available data for future predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4e261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22864596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6560530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
