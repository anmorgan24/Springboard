{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e0247e",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682809ec",
   "metadata": {},
   "source": [
    "* Decision trees are supervised learning models used for problems involving classification and regression. \n",
    "* Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. \n",
    "* By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. \n",
    "* Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. \n",
    "\n",
    "\n",
    "* **Classification and Regression Trees (CART)** are a set of supervised learning models used for problems involving classification and regression.\n",
    "* Given a labeled dataset, classification tree learns a sequence of if-else questions about individual features in order to infer the labels \n",
    "    * **Objective: infer class labels**\n",
    "* Able to capture non-linear relationships between features and labels\n",
    "* Don't require feature scaling (ex: Standardization, etc..)\n",
    "* When a classification tree is trained, the tree learns a sequence of if-else questions, with each question involving one feature and one split point.\n",
    "* The maximum number of branches separating the top from an extreme end is known as the `maximum depth`. For example, a max_depth of 2 means 2 levels of branches (and 3 levels of if-else statements).\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(X_test, y_pred)\n",
    "```\n",
    "* `stratify= y` means train and test sets to have the same proportion of class labels as the unsplit dataset.\n",
    "\n",
    "\n",
    "* **Decision Regions:** A classification model divides the feature space into regions where all instances in one region are assigned to only one class label. These regions are known as **decision regions**.\n",
    "* **Decision Boundary:** surface (line, plane, hyperplane) separating different decision regions. \n",
    "* A classification tree produces rectangular decision regions in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5511d6",
   "metadata": {},
   "source": [
    "#### Classification-Tree Learning\n",
    "* **Decision Tree:** data structure consisting of a hierarchy of nodes.\n",
    "* **Node:** question or prediction.\n",
    "* **Root:** *no* parent node, question giving rise to *two* children nodes.\n",
    "* **Internal Node:** *one* parent node, question giving rise to *two* children nodes.\n",
    "* **Leaf:** *one* parent node, *no* children nodes $\\Rightarrow$ *prediction*\n",
    "* **Information Gain:** The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature f and a split-point sp. But how does it know which feature and which split-point to pick? It does so by maximizing Information gain! The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split.\n",
    "\n",
    "* When a classification is trained on a labeled data set, the tree learns patterns from the features in such a way to produce the purest leaves.\n",
    "* When an **unconstrained tree** is trained, the nodes are grown recursively. In other words, a node is grown based on the state of its predecessors \n",
    "* At the non-leaf node the data is split based on feature *f* and and split point *sp* in such a way as to maximize *IG*\n",
    "* If the *IG* obtained by splitting a node is nil (0), the node is declared as a leaf.\n",
    "\n",
    "`dt = DecisionTreeClassifier(criterion= 'gini', random_state=1)`\n",
    "\n",
    "#### Decision Tree for Regression\n",
    "* In regression, the target variable is continuous (a real value)\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "dt= DecisionTreeRegressor(max_depth=4, min_samples_leaf= 0.1, random_state=3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "print(rmse_dt)\n",
    "\n",
    "```\n",
    "\n",
    "* When a regression tree is trained on a dataset, the impurity of a node is measure using the MSE of the targets in that node\n",
    "* This means that the Regression Tree tries to find the splits that produce the leaves where, in each leaf, the target values are, on average, the closest possible to the mean value of the labels in that particular leaf.\n",
    "* As a new instance traverses the tree and reaches a leaf, its target variable, *y*, is computed as the average of the target variables contained in that leaf.\n",
    "* Regression trees are able to capture greater flexibility than Linear Regression models (though are also more prone to overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10685d5",
   "metadata": {},
   "source": [
    "gini-index and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dc5ff",
   "metadata": {},
   "source": [
    "#### Generalization Error\n",
    "* In supervised learning, you make the assumption that there is a mapping, $f$, between features and labels:\n",
    "\\begin{equation}\n",
    "y = f(x)\n",
    "\\end{equation}\n",
    "where *f* is unknown.\n",
    "* In reality, data generation is always accompanied by randomness, or noise.\n",
    "* The goal of supervised learning is to find $\\hat f$ that best approximates $f$.\n",
    "* $\\hat f$ can be Logistic Regression, Decision Tree, Neural Network...\n",
    "* When training $\\hat f$, you want to make sure that noise is discarded as much as possible.\n",
    "* **End goal:** $\\hat f$ should achieve a low predictive error on unseen datasets.\n",
    "#### Difficulties in approximating $f$\n",
    "* Two main difficulties:\n",
    "    * **Overfitting:** $\\hat f(x)$ fits the training set *noise*.\n",
    "    * **Underfitting:** $\\hat f$ is not flexible enough to approximate $f$; here, the training set error will be roughly equivalent to the test set error and both errors are relatively high. \n",
    "    \n",
    "* **Generalization Error:** The generalization error of a model tells you how well it generalizes to unseen data\n",
    "    * Can be decomposed into three terms: bias, variance, and irreducible error\n",
    "    * $\\hat f$ = $bias^2$ + $variance$ + irreducible error\n",
    "    * irreducible error = error contribution of noise\n",
    "    \n",
    "* **Bias:** by how much are $\\hat f$ and $f$ different?\n",
    "    * High bias models lead to underfitting\n",
    "* **Variance:** tells you how much $\\hat f$ is inconsistent over different datasets. \n",
    "    * High variance models lead to overfitting\n",
    "* **Model Complexity:** sets the flexibility of $\\hat f$\n",
    "    * Example: maximum tree depth, minimum samples per leaf\n",
    "    * The goal is to find the model complexity that achieves the lowest generalization error \n",
    "    * Since this (generalization) error is the sum of three terms, with the irreducible error being constant, you need to find a balance between bias and variance (**\"Bias-Variance Tradeoff\"**) because as one increases, the other decreases.\n",
    "\n",
    "#### Diagnosing Bias and Variance Problems\n",
    "* How do you estimate $\\hat f$'s generalization error?\n",
    "* This cannot be done directly because:\n",
    "    * $f$ is unknown\n",
    "    * usually you only have one dataset\n",
    "    * noise is unpredictable\n",
    "* Solution:\n",
    "    * split the data into training and test sets\n",
    "    * train on training set, evaluate error of $\\hat f$ on test set\n",
    "    * **Test should only be used to evaluate $\\hat f$'s *final* performance.**\n",
    "    * To obtain a reliable estimate of $\\hat f$'s performance, use **Cross-Validation (CV).**\n",
    "        * K-Fold CV\n",
    "        * Hold-Out CV\n",
    "* **K-Fold CV:**\n",
    "    * If $\\hat f$'s cross-validation error is greater than $\\hat f$'s training error, then $\\hat f$ suffers from **high variance** (overfitting).\n",
    "        * Try decreasing model complexity\n",
    "        * For example: reduce maximum tree depth or increase maximum samples per leaf (for Decision Trees)\n",
    "        * Also try: gathering more data (if possible)\n",
    "    * If $\\hat f$'s CV error is roughly equal to the training set error, but much greater than desired error, then $\\hat f$ suffers from **high bias** (underfitting).\n",
    "        * Try increasing model complexity \n",
    "        * For example: increase max depth, decrease min samples per leaf (for Decision Trees)\n",
    "        * Also try: gathering *more relevant* features\n",
    "\n",
    "**K-Fold CV:**\n",
    "`n_jobs = -1` to exploit all available CPUs and computation.\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "from sklearn.model_selection import cross_val_score\n",
    "SEED=123\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=SEED)\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv=10, scoring= 'neg_mean_squared_error', n_jobs=-1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict_train = dt.predict(X_train)\n",
    "y_predict_test = dt.predict(X_test)\n",
    "```\n",
    "`print('CV MSE: {:.2f}'.format(MSE_CV.mean()))` \n",
    "\n",
    "`print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))` \n",
    "\n",
    "`print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))` \n",
    "\n",
    "```\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_train = dt.predict(X_train)\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f2d6e",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "* **Advantages of CARTs:**\n",
    "    * Simple to understand\n",
    "    * Simple to interpret\n",
    "    * Easy to use\n",
    "    * Flexibility: ability to describe non-linear dependencies\n",
    "    * Preprocessing: no need to standardize or normalize features, ...\n",
    "* **Limitations of CARTs:**\n",
    "    * Classification: Can only produce orthogonal decision boundaries\n",
    "    * Sensitive to small variations in the training set\n",
    "    * High variance: unconstrained CARTs may overfit the training set.\n",
    "    * **Solution: ensemble learning**\n",
    "    \n",
    "* **Ensemble Learning:** \n",
    "    * Train different models on the same dataset\n",
    "    * Let each model make its predictions\n",
    "    * Meta-model: aggregates predictions of individual models \n",
    "    * Final prediction: more robust and less prone to errors (than each individual model\n",
    "    * Best results: models are skillful in different ways \n",
    "    * One example ensemble learner in practice: Voter Classifier (different classifiers hard vote on final prediction)\n",
    "    \n",
    "**Voting Classifier in sklearn:**\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Logistic Regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemblsion(random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "classifiers = [('Logistic Regression', lr), \n",
    "                ('K Nearest Neighbors', knn)\n",
    "                ('Classification Tree', dt)]\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "```\n",
    "\n",
    "```\n",
    "vc = VotingClassifier(estimators = classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "```\n",
    "\n",
    "```\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "```\n",
    "\n",
    "```\n",
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd2343",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "* **Bootstrap aggregation = Bagging**\n",
    "* Voting Classifier is an ensemble of models that are fit to the training set using different algorithms. The final predictions were obtaining with majority voting.\n",
    "* In **bagging**, the ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire training set. Instead, each model is trained on a different subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d11d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f704375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ceb9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ced2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
