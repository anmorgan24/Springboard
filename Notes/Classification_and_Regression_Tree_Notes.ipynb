{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e0247e",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682809ec",
   "metadata": {},
   "source": [
    "* Decision trees are supervised learning models used for problems involving classification and regression. \n",
    "* Tree models present a high flexibility that comes at a price: on one hand, trees are able to capture complex non-linear relationships; on the other hand, they are prone to memorizing the noise present in a dataset. \n",
    "* By aggregating the predictions of trees that are trained differently, ensemble methods take advantage of the flexibility of trees while reducing their tendency to memorize noise. \n",
    "* Ensemble methods are used across a variety of fields and have a proven track record of winning many machine learning competitions. \n",
    "\n",
    "\n",
    "* **Classification and Regression Trees (CART)** are a set of supervised learning models used for problems involving classification and regression.\n",
    "* Given a labeled dataset, classification tree learns a sequence of if-else questions about individual features in order to infer the labels \n",
    "    * **Objective: infer class labels**\n",
    "* Able to capture non-linear relationships between features and labels\n",
    "* Don't require feature scaling (ex: Standardization, etc..)\n",
    "* When a classification tree is trained, the tree learns a sequence of if-else questions, with each question involving one feature and one split point.\n",
    "* The maximum number of branches separating the top from an extreme end is known as the `maximum depth`. For example, a max_depth of 2 means 2 levels of branches (and 3 levels of if-else statements).\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(X_test, y_pred)\n",
    "```\n",
    "* `stratify= y` means train and test sets to have the same proportion of class labels as the unsplit dataset.\n",
    "\n",
    "\n",
    "* **Decision Regions:** A classification model divides the feature space into regions where all instances in one region are assigned to only one class label. These regions are known as **decision regions**.\n",
    "* **Decision Boundary:** surface (line, plane, hyperplane) separating different decision regions. \n",
    "* A classification tree produces rectangular decision regions in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6decf2",
   "metadata": {},
   "source": [
    "#### Classification-Tree Learning\n",
    "* **Decision Tree:** data structure consisting of a hierarchy of nodes.\n",
    "* **Node:** question or prediction.\n",
    "* **Root:** *no* parent node, question giving rise to *two* children nodes.\n",
    "* **Internal Node:** *one* parent node, question giving rise to *two* children nodes.\n",
    "* **Leaf:** *one* parent node, *no* children nodes $\\Rightarrow$ *prediction*\n",
    "* **Information Gain:** The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asks a question involving one feature f and a split-point sp. But how does it know which feature and which split-point to pick? It does so by maximizing Information gain! The tree considers that every node contains information and aims at maximizing the Information Gain obtained after each split.\n",
    "\n",
    "* When a classification is trained on a labeled data set, the tree learns patterns from the features in such a way to produce the purest leaves.\n",
    "* When an **unconstrained tree** is trained, the nodes are grown recursively. In other words, a node is grown based on the state of its predecessors \n",
    "* At the non-leaf node the data is split based on feature *f* and and split point *sp* in such a way as to maximize *IG*\n",
    "* If the *IG* obtained by splitting a node is nil (0), the node is declared as a leaf.\n",
    "\n",
    "`dt = DecisionTreeClassifier(criterion= 'gini', random_state=1)`\n",
    "\n",
    "#### Decision Tree for Regression\n",
    "* In regression, the target variable is continuous (a real value)\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "dt= DecisionTreeRegressor(max_depth=4, min_samples_leaf= 0.1, random_state=3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "print(rmse_dt)\n",
    "\n",
    "```\n",
    "\n",
    "* When a regression tree is trained on a dataset, the impurity of a node is measure using the MSE of the targets in that node\n",
    "* This means that the Regression Tree tries to find the splits that produce the leaves where, in each leaf, the target values are, on average, the closest possible to the mean value of the labels in that particular leaf.\n",
    "* As a new instance traverses the tree and reaches a leaf, its target variable, *y*, is computed as the average of the target variables contained in that leaf.\n",
    "* Regression trees are able to capture greater flexibility than Linear Regression models (though are also more prone to overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fe7ed",
   "metadata": {},
   "source": [
    "gini-index and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39fedf",
   "metadata": {},
   "source": [
    "#### Generalization Error\n",
    "* In supervised learning, you make the assumption that there is a mapping, $f$, between features and labels:\n",
    "\\begin{equation}\n",
    "y = f(x)\n",
    "\\end{equation}\n",
    "where *f* is unknown.\n",
    "* In reality, data generation is always accompanied by randomness, or noise.\n",
    "* The goal of supervised learning is to find $\\hat f$ that best approximates $f$.\n",
    "* $\\hat f$ can be Logistic Regression, Decision Tree, Neural Network...\n",
    "* When training $\\hat f$, you want to make sure that noise is discarded as much as possible.\n",
    "* **End goal:** $\\hat f$ should achieve a low predictive error on unseen datasets.\n",
    "#### Difficulties in approximating $f$\n",
    "* Two main difficulties:\n",
    "    * **Overfitting:** $\\hat f(x)$ fits the training set *noise*.\n",
    "    * **Underfitting:** $\\hat f$ is not flexible enough to approximate $f$; here, the training set error will be roughly equivalent to the test set error and both errors are relatively high. \n",
    "    \n",
    "* **Generalization Error:** The generalization error of a model tells you how well it generalizes to unseen data\n",
    "    * Can be decomposed into three terms: bias, variance, and irreducible error\n",
    "    * $\\hat f$ = $bias^2$ + $variance$ + irreducible error\n",
    "    * irreducible error = error contribution of noise\n",
    "    \n",
    "* **Bias:** by how much are $\\hat f$ and $f$ different?\n",
    "    * High bias models lead to underfitting\n",
    "* **Variance:** tells you how much $\\hat f$ is inconsistent over different datasets. \n",
    "    * High variance models lead to overfitting\n",
    "* **Model Complexity:** sets the flexibility of $\\hat f$\n",
    "    * Example: maximum tree depth, minimum samples per leaf\n",
    "    * The goal is to find the model complexity that achieves the lowest generalization error \n",
    "    * Since this (generalization) error is the sum of three terms, with the irreducible error being constant, you need to find a balance between bias and variance (**\"Bias-Variance Tradeoff\"**) because as one increases, the other decreases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62947d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb0387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448e164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317ce7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a5798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5307b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d27533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
