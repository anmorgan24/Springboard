{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96ecee3",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e060d97",
   "metadata": {},
   "source": [
    "#### Bayesian Hyperparameter Optimization\n",
    "* Why auto-tuning matters:\n",
    "    * Humans are really bad at it\n",
    "    * Properly set parameters outperform the most complex, state-of-the-art models\n",
    "* Tuning tips:\n",
    "    * Keep an open-mind (explore the full space from the beginning)\n",
    "        * Don't prejudge hyperparameter possibilities\n",
    "    * Don't do grid search as your hyperparameter search method\n",
    "        * In practice you'll have many hyperparameters, some of them will matter, and some of them will end up being mostly irrelevant\n",
    "    * Try and eliminate irrelevant hyperparameters where possible\n",
    "        * The more parameters you have, the harder it is to tune\n",
    "    * To see a clear pattern it can take way longer than you expect\n",
    "        * If you want to have really good tuning, be prepared to spend a lot of time on it\n",
    "        \n",
    "**Bayesian parameter estimation for automatically tuning hyperparameters:**\n",
    "* Neural nets have certain hyperparameters which aren't part of the training procedure\n",
    "* You can evaluate them using a validation set, but there's still the problem of which values to try:\n",
    "    * Brute force search (e.g. grid search, random search) is very expensive and time-consuming\n",
    "* Hyperparameter tuning is a kind of black box optimization: you want to minimize a function, but you only get to query values, not compute gradients\n",
    "* Each evaluation is expensive, so we want to use few evaluations\n",
    "* You want to query a point which:\n",
    "    * you expect to be good\n",
    "    * you are uncertain about\n",
    "* $\\Rightarrow$ **Bayesian regression allows us to predict not just a value, but a distribution.** $\\Leftarrow$\n",
    "\n",
    "**Bayesian Linear Regression**\n",
    "* We're interested in the uncertainty\n",
    "* Bayesian Linear Regression considers various plausible explanations for how the data were generated \n",
    "* It makes predictions using all possible regression weights, weighted by their posterior probability\n",
    "* We can turn this into non-linear regression using basis functions (e.g. Gaussian basis functions)\n",
    "\n",
    "**Bayesian Optimization**\n",
    "* Applying all of this to black-box optimization: let's review the technique called **Bayesian optimization.**\n",
    "* The actual function we're trying to optimize (e.g. validation error as a function of hyperparameters) is really complicated. So let's approximate it with a simple function, called the surrogate function.\n",
    "* After we've queried a certain number of points, we can condition on these to infer the posterior over the **surrogate function** using Bayesian linear regression.\n",
    "* To choose the next point to query, we must define an **acquisition function**, which tells use how promising a candidate it is.\n",
    "    * Desiderata ([see MW](https://www.merriam-webster.com/dictionary/desideratum)):\n",
    "        * high for points we expect to be good\n",
    "        * high for points we're uncertain about\n",
    "        * low for points we've already tried\n",
    "* The problem with **Probability of Improvement (PI)** is that it queries points it is highly confident will have a small improvement; usually, these are right next to ones we've already evaluated.\n",
    "* A better choice: **Expected Improvement (EI).** \n",
    "    * The idea: if the new value is much better, we win by a lot; if it's much worse, we haven't lost anything\n",
    "    * There is an explicit formula for this if the posterior predictive distribution is Gaussian.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14bff4",
   "metadata": {},
   "source": [
    "#### How does Bayesian optimization work?\n",
    "* Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not.\n",
    "* As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB â€” aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored.\n",
    "* These are the main parameters of the Bayesian Optimizer:\n",
    "    * **`n_iter`:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "    * **`init_points`:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e559a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5b514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b27c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d65b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c1e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654942a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef08bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
