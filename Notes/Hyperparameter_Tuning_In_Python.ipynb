{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7b7863",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c346c1",
   "metadata": {},
   "source": [
    "#### Hyperparameters and Parameters\n",
    "* New, complex algorithms have many hyperparameters\n",
    "* It becomes increasingly important to learn how to efficiently find optimal combinations, as this search will likely take up a large portion of your time\n",
    "* Often it is quite easy to simply run Scikit Learn functions on the default settings or perhaps code from a tutorial or book without really digging under the hood\n",
    "* However, what lies underneath is of vital importance to good model building\n",
    "\n",
    "### Parameters\n",
    "* **Parameters:** \n",
    "    * Components of the final model that are learned through the modeling process\n",
    "    * Crucially, **you do not set these manually.** In fact, you can't.\n",
    "    * The algorithm will discover parameters for you (they are learned during the modeling process).\n",
    "    \n",
    "* To know what parameters an algorithm will produce, you need:\n",
    "    * 1. To know a bit about the algorithm itself and how it works.\n",
    "    * 2. Consult the sklearn documentation to see where the parameter is stored in the returned object.\n",
    "            * parameters are found under the 'Attributes' section - *not* the 'parameters' section!\n",
    "\n",
    "* **Parameters in Random Forest\n",
    "* parameters are in the node decisions \n",
    "    * what feature to split\n",
    "    * what value to split on\n",
    "    * **to view individual tree of Random Forest:**\n",
    "    \n",
    "```\n",
    "rf_clf = RandomForestClassifier(max_depth=2)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "chosen_tree = rf_clf.estimators_[7]\n",
    "```\n",
    "\n",
    "#### Extracting Node Decisions\n",
    "* For example, we can pull out details of the left, second-from-top node of the above isolated tree as follows:\n",
    "\n",
    "```\n",
    "# Get the column it split on\n",
    "split_column = chosen_tree.tree_.feature[1]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "\n",
    "# Get the level it split on \n",
    "split_value = chosen_tree.tree_.threshold[1]\n",
    "print(\"This node split on feature {}, at a. value of {}'.format(split_column_name, split_value))\n",
    "```\n",
    "\n",
    "* **Exercise:** Extract the coefficient parameter (found in the coef_ attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable.\n",
    "\n",
    "```\n",
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)\n",
    "```\n",
    "\n",
    "```\n",
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "imgplot = plt.imshow(tree_viz_image)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b90bd3",
   "metadata": {},
   "source": [
    "#### Hyperparameters Overview\n",
    "* Hyperparameters are something that you set before the modeling process begins\n",
    "* The algorithm does not learn the value of hyperparameters during the modeling process (and this is the crucial differentiator between hyperparameters and parameters: whether you set it or whether the algorithmm learns it and informs you)\n",
    "* Some hyperparameters are more important than others\n",
    "* Some hyperparameters will not help model performance and are related to computational decisions or what information to retain for analysis\n",
    "* `n_jobs` : how many cores to use (speed up computational time)\n",
    "* `random_seed`\n",
    "* `verbose` : whether to print out information as the modeling occurs \n",
    "\n",
    "* `rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f755b",
   "metadata": {},
   "source": [
    "#### Hyperparameter Values\n",
    "* Begin automating the work\n",
    "* Some hyperparameters are likely better to start your tuning with than others\n",
    "* *Which* values to try for hyperparameters?\n",
    "    * This will be specific to the algorthm and to the hyperparameter itself \n",
    "    * However, there do exist some best practice guidelines and tips\n",
    "    \n",
    "#### Top tips for deciding ranges of values to try for different hyperparameters:\n",
    "\n",
    "* **What values NOT to set (as they may conflict):** \n",
    "    * some values of the hyperparameter `penalty` conflict with some values of the hyperparameter `solver`\n",
    "    * Some conflicts will not result in an error, but may result in a model construction we had not anticipated: for example `ElasticNet` with the `normalize` hyperparameter\n",
    "    * Close inspection of the sklearn documentation is important\n",
    "    * Be aware of setting \"silly\" values for different algorithms, for example:\n",
    "        * Random forest with low number of trees\n",
    "            * Would you consider it a forest if it had only 2 trees?\n",
    "        * 1 Neighbor in KNN algorithm\n",
    "            * Averaging the vote of 1 person doesn't sound very robust\n",
    "        * Increasing a hyperparameter by a very small amount is unlikely to greatly improve a model \n",
    "            * One more tree in a forest, for example, isn't likely to have a large impact\n",
    "        \n",
    "* **Try a for loop to iterate through options:\n",
    "\n",
    "```\n",
    "neighbors_list = [3, 5, 10, 20, 50, 75]\n",
    "for test_number in neightbors_list:\n",
    "    model = KNeighborsClassifier(N_neighbors = test_number)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "```\n",
    "* Store the results in a DataFrame to view the effect of this hyperparameter on the accuracy of the model:\n",
    "\n",
    "```\n",
    "results_df = pd.DataFrame({'neighbors': neighbors_list, 'accuracy':accuracy_list})\n",
    "print(results_df)\n",
    "```\n",
    "* Printing the DataFrame in this way, shows that (it appears) adding any more neighbors than 20 does not help.\n",
    "* A common tool that is used to assist with analyzing the impact of a singular hyperparameter on an end result is called a **learning curve.**\n",
    "\n",
    "```\n",
    "neighbors_list = list(range(5, 500, 5))\n",
    "accuracy_list = []\n",
    "for test_number in neighbors_list:\n",
    "    model = KNeighborsClassifier(n_neighbors = test_number)\n",
    "    predicions = model.fit(X_train, y_train_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy': accuracy_list})\n",
    "\n",
    "plt.plot(results_df['neighbors'], results_df['accuracy'])\n",
    "# Add the labels and title\n",
    "plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy', title='Accuracy for different n_neighbors')\n",
    "plt.show()\n",
    "```\n",
    "* One thing to be aware of is that Python's `range` function does not work for decimal steps, which is important for hyperparameters that work on that scale. \n",
    "    * A handy trick uses numpys **`np.linspace(start, end, num)`**:\n",
    "        * Create a number of values (`num`) evenly spread within an interval (`start`, `end`) that you specify\n",
    "\n",
    "```\n",
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test,predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ed17b",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "* Automate 2 hyperparameters or more with a nested for loop:\n",
    "* Firstly define model creation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8a4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "    model = GradientBoostingClassifier(\n",
    "            learning_rate=learn_rate,\n",
    "            max_depth= max_depth)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cab2ce",
   "metadata": {},
   "source": [
    "Now we can loop through our lists of hyperparameters and call our function\n",
    "\n",
    "```\n",
    "results_list = []\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate, max_depth))\n",
    "\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'accuracy'])\n",
    "print(results_df)\n",
    "```\n",
    "* We have a nested loop so we can test all values of our first hyperparameter for all values of our second hyperparameter\n",
    "* Importantly, the relationship between models created and hyperparameters or values to test is **not a linear relationship, but an exponential one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0dc459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_grid_search(learn_rate, max_depth, subsample, max_features):\n",
    "    model = GradientBoostingClassifier(\n",
    "            learning_rate = learn_rate,\n",
    "            max_depth= max_depth,\n",
    "            subsample = subsample, \n",
    "            max_features= max_features)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0b8d5",
   "metadata": {},
   "source": [
    "```\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for subsample in subsample_list:\n",
    "            for max_features in max_features_list:\n",
    "                results_list.append(gbm_grid_search(learn_rate, max_depth, subsample, max_features))\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'subsample', 'max_features', 'accuracy'])\n",
    "print(results_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76e92e",
   "metadata": {},
   "source": [
    "* Safe to say, we cannot keep nesting forever, as our code becomes complex and inefficient\n",
    "\n",
    "#### Grid Search\n",
    "* **Grid Search has a number of advantages:**\n",
    "    * It's programmatic\n",
    "    * It saves many lines of code\n",
    "    * It is guaranteed to find the best model within the grid you specify \n",
    "        * But, obviously, if you specify a poor grid with silly or conflicting values, you won't get a good score.\n",
    "    * It is an easy methodology to explain compared to some other more complex ones.\n",
    "* **Grid search also has a number of disadvantages:**\n",
    "    * It is **very computationally expensive!**\n",
    "    * It is **uninformed**. Results of one model don't help creating the next model \n",
    "    * (We will cover informed methods later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22638e00",
   "metadata": {},
   "source": [
    "### GridSearch with sklearn\n",
    "Steps in a Grid Search:\n",
    "* 1. Select an algorithm to tune the hyperparameters (sometimes called an \"estimator\")\n",
    "* 2. Defining which hyperparameters we will tune\n",
    "* 3. Defining a range of values for each hyperparameter\n",
    "* 4. Setting a cross-validation scheme.\n",
    "* 5. Define a score function so we can decide which square on our grid was \"the best.\"\n",
    "* 6. Include extra useful information or functions\n",
    "\n",
    "#### GridSearchCV Object Inputs:\n",
    "* **`estimator`** : our algorithm\n",
    "* **`param_grid`** : sets which hyperparameters and values to test; must be a dictionary. Dictionary keys must be hyperparameter names, and the values a list of values to test.\n",
    "* **`cv`** : choice of how to undertake cross-validation; you could specify different cross-validation types here; but simply providing an integer will create a k-fold\n",
    "* **`scoring`** : which scoring function used to evaluate model's performance; you can use your own custom metric, or one available from sklearn's `metrics` module. See all available metrics using: `sorted(metrics.SCORERS.keys())`\n",
    "* **`refit`** : set to `True`, means the best hyperparameter combinations are used to undertake a fitting to the training data; the GridSearchCV object can be used as an estimator directly; can be handy in some situations where you don't need to save the best hyperparameters and train another model.\n",
    "* **`n_jobs`** : assists with parallel execution; you can effectively 'split up' your work and have many models being created at the same time. This is possible because the results of one model do not affect the next one. Be careful using all your cores for modelling if you want to do other work, however. You can check how many cores you have available, which determines how many models you can run in parallel with:\n",
    "    * `import os`\n",
    "    * `print(ox.cpu_count())`\n",
    "    \n",
    "* **`return_train_score`** : logs statistics about the training runs that were undertaken. This can be useful for plotting and understanding test vs training set performance (and hence bias-variance tradeoff). While informative, this is computationally expensive and will not assist in finding the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a10faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a377b2a",
   "metadata": {},
   "source": [
    "#### Building a GridSearchCV object:\n",
    "\n",
    "```\n",
    "param_grid = {'max_depth': [2, 4, 6, 8], 'min_samples_leaf': [1, 2, 4, 6]}\n",
    "\n",
    "rf_class = RandomForestClassifier(criterion='entropy', max_features = 'auto')\n",
    "\n",
    "grid_rf_class = GridSearchCV(estimator= rf_class,\n",
    "                             param_grid = parameter_grid,\n",
    "                             scoring = 'accuracy',\n",
    "                             n_jobs = 4,\n",
    "                             cv = 10,\n",
    "                             refit = True,\n",
    "                             return_train_score = True)\n",
    "```\n",
    "* With `refit = True`, we can directly use the GridSearchCV on=bject a an estimator. That means swe can fit onto our data and make predictions, just like any other sklearn estimator!\n",
    "\n",
    "```\n",
    "grid_rf_class.fit(X_train, y_train)\n",
    "\n",
    "grid_rf_class.predict(X_test)\n",
    "```\n",
    "\n",
    "```\n",
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)\n",
    "```\n",
    "                            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab34228",
   "metadata": {},
   "source": [
    "#### Understanding a grid search output\n",
    "* The properties of the GridSearchCV Object can be cateogrized into three different groups:\n",
    "* **A results log:**\n",
    "    * `cv_results_`\n",
    "* **The best results:**\n",
    "    * `best_index_`\n",
    "    * `best_params_`\n",
    "    * `best_score_`\n",
    "* **Extra information:**\n",
    "    * `scorer_`\n",
    "    * `n_splits_`\n",
    "    * `refit_time_`\n",
    "\n",
    "* Properties are accessed using the dot notation: `grid_search_object.property`.\n",
    "\n",
    "#### The `.cv_results_` property\n",
    "* A dictionary that we can read into a pandas DataFrame to explore:\n",
    "    * `cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)`\n",
    "    * `time` columns refer to the time it took to fit and score the model.\n",
    "    * the `params` columns contain information on the different parameters taht were used in the model.\n",
    "    * the `params` column contains dictionary of all the parameters from the previous `param` columns\n",
    "        * Remember that each row in this DataFrame is about one model\n",
    "        * We need to use `pd.set_option` to ensure we don't truncate the results we are printing when using the params column: `pd.set_option(\"display.max_colwidth\", -1)`\n",
    "    * `test_score` columns contain the score on our test set for each of our cross-folds as well as some summary statistics      \n",
    "    * `rank_test_score` orders the `mean_test_score` from best to worst\n",
    "    * Extracting the best row:\n",
    "        * `best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]`\n",
    "    * The `test_score` columns are then repeated for the `train_scores` columns\n",
    "        * **Note:** If we had not set return_train_score to True, this would not include the training scores\n",
    "        * There is also no ranking column for the training scores, as we only care about test set performance\n",
    "#### The best grid square\n",
    "* `best_params_`, the dictionary of parameters that gave the best score\n",
    "* `best_score_`, the actual best score\n",
    "* `best_index_`, the row in our `cv_results_.rank_test_score` that was the best.\n",
    "* `best_estimator_` property is an estimator built using the best parameters from the grid search\n",
    "    * Because it is an estimator, we can use it to predict on our test set\n",
    "    \n",
    "* We can also use the GridSearchCV object itself directly a an estimator\n",
    "\n",
    "#### Extra information:\n",
    "* These are not very useful properties, but may be important if you construct your grid search differently\n",
    "* `scorer_` : what scorer function was used on the held out data\n",
    "* `n_splits_` : how many cros-validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e4c7d",
   "metadata": {},
   "source": [
    "## Random Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb210a8",
   "metadata": {},
   "source": [
    "Why does Random Search work?\n",
    "* Bengio & Bergstra (2012): \"This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid.\"\n",
    "* **Two main reasons:**\n",
    "* 1. Not every hyperparameter is important.\n",
    "* 2. A little trick of probability.\n",
    "\n",
    "* With relatively few trials we can get close to a maximum score with a relatively high probability.\n",
    "* A grid search may spend lots of time in a \"bad area\" as it covers everything exhaustively.\n",
    "\n",
    "* Some important notes about Random Search:\n",
    "* **The maximum is still only as good as the grid you set!**\n",
    "* Remember to fairly compare this to grid search, you need to have the same modeling 'budget.'\n",
    "\n",
    "#### Random Search in sklearn\n",
    "* Very similar to sklearn's GridSearchCV process:\n",
    "    * 1. Decide an algorithm/estimator\n",
    "    * 2. Defining which hyperparameters we will tune \n",
    "    * 3. Defining a range of values for each hyperparameter\n",
    "    * 4. Setting a cross-validation scheme\n",
    "    * 5. Define a score function\n",
    "    * 6. Include extra useful information or functions\n",
    "    \n",
    "* There is only one difference when undertaking a random search\n",
    "    * **7. Decide how many samples to take (then sample)**\n",
    "    \n",
    "* Two key differencesz:\n",
    "    * **`n_iter`**: the number of samples for the random search to take from your grid. \n",
    "    * **`param_distributions`**: is slightly different from `param_grid`, allowing optional ability to set a distribution for sampling; if you just give a list, the default is for all combinations to have equal chance to be chosen (/\"sample uniformly\").\n",
    "    \n",
    "```\n",
    "learn_rate_list = np.linspace(0.001, 2, 150)\n",
    "min_samples_leaf_list = list(range(1, 51))\n",
    "\n",
    "parameter_grid = {\n",
    "        'learning_rate' : learn_rate_list,\n",
    "        'min_samples_leaf' : min_samples_leaf_list}\n",
    "\n",
    "number_models = 10\n",
    "\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "                        estimator = GradientBoostingClassifier(),\n",
    "                        param_distributions = parameter_grid,\n",
    "                        n_iter = number_models,\n",
    "                        scoring = 'accuracy',\n",
    "                        n_jobs=4, \n",
    "                        cv=10,\n",
    "                        refit=True,\n",
    "                        return_train_score = True)\n",
    "random_GBM_class.fit(X_train, y_train)\n",
    "```\n",
    "#### Analyze the output\n",
    "\n",
    "```\n",
    "# Make sure we set the liits of Y and X appropriately\n",
    "x_lims = [np.min(learn_rate_list), np.max(learn_rate_list)]\n",
    "y_lims = [np.min(min_samples_leaf_list), np.max(min_samples_leaf_list)]\n",
    "\n",
    "# Plot grid results\n",
    "plt.scatter(rand_y, rand_x, c=['blue']*10)\n",
    "plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf', title='Random Search Hyperparameters')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} \n",
    "\n",
    "# Create a random search object\n",
    "random_rf_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_estimators=80),\n",
    "    param_distributions = param_grid, n_iter = 5,\n",
    "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True )\n",
    "\n",
    "# Fit to the training data\n",
    "random_rf_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_rf_class.cv_results_['param_max_depth'])\n",
    "print(random_rf_class.cv_results_['param_max_features'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811fd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11caf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7f20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d7d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851fb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6c59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
