{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7b7863",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c346c1",
   "metadata": {},
   "source": [
    "#### Hyperparameters and Parameters\n",
    "* New, complex algorithms have many hyperparameters\n",
    "* It becomes increasingly important to learn how to efficiently find optimal combinations, as this search will likely take up a large portion of your time\n",
    "* Often it is quite easy to simply run Scikit Learn functions on the default settings or perhaps code from a tutorial or book without really digging under the hood\n",
    "* However, what lies underneath is of vital importance to good model building\n",
    "\n",
    "### Parameters\n",
    "* **Parameters:** \n",
    "    * Components of the final model that are learned through the modeling process\n",
    "    * Crucially, **you do not set these manually.** In fact, you can't.\n",
    "    * The algorithm will discover parameters for you (they are learned during the modeling process).\n",
    "    \n",
    "* To know what parameters an algorithm will produce, you need:\n",
    "    * 1. To know a bit about the algorithm itself and how it works.\n",
    "    * 2. Consult the sklearn documentation to see where the parameter is stored in the returned object.\n",
    "            * parameters are found under the 'Attributes' section - *not* the 'parameters' section!\n",
    "\n",
    "* **Parameters in Random Forest\n",
    "* parameters are in the node decisions \n",
    "    * what feature to split\n",
    "    * what value to split on\n",
    "    * **to view individual tree of Random Forest:**\n",
    "    \n",
    "```\n",
    "rf_clf = RandomForestClassifier(max_depth=2)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "chosen_tree = rf_clf.estimators_[7]\n",
    "```\n",
    "\n",
    "#### Extracting Node Decisions\n",
    "* For example, we can pull out details of the left, second-from-top node of the above isolated tree as follows:\n",
    "\n",
    "```\n",
    "# Get the column it split on\n",
    "split_column = chosen_tree.tree_.feature[1]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "\n",
    "# Get the level it split on \n",
    "split_value = chosen_tree.tree_.threshold[1]\n",
    "print(\"This node split on feature {}, at a. value of {}'.format(split_column_name, split_value))\n",
    "```\n",
    "\n",
    "* **Exercise:** Extract the coefficient parameter (found in the coef_ attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable.\n",
    "\n",
    "```\n",
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)\n",
    "```\n",
    "\n",
    "```\n",
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "imgplot = plt.imshow(tree_viz_image)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65324eb1",
   "metadata": {},
   "source": [
    "#### Hyperparameters Overview\n",
    "* Hyperparameters are something that you set before the modeling process begins\n",
    "* The algorithm does not learn the value of hyperparameters during the modeling process (and this is the crucial differentiator between hyperparameters and parameters: whether you set it or whether the algorithmm learns it and informs you)\n",
    "* Some hyperparameters are more important than others\n",
    "* Some hyperparameters will not help model performance and are related to computational decisions or what information to retain for analysis\n",
    "* `n_jobs` : how many cores to use (speed up computational time)\n",
    "* `random_seed`\n",
    "* `verbose` : whether to print out information as the modeling occurs \n",
    "\n",
    "* `rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1e4bd",
   "metadata": {},
   "source": [
    "#### Hyperparameter Values\n",
    "* Begin automating the work\n",
    "* Some hyperparameters are likely better to start your tuning with than others\n",
    "* *Which* values to try for hyperparameters?\n",
    "    * This will be specific to the algorthm and to the hyperparameter itself \n",
    "    * However, there do exist some best practice guidelines and tips\n",
    "    \n",
    "#### Top tips for deciding ranges of values to try for different hyperparameters:\n",
    "\n",
    "* **What values NOT to set (as they may conflict):** \n",
    "    * some values of the hyperparameter `penalty` conflict with some values of the hyperparameter `solver`\n",
    "    * Some conflicts will not result in an error, but may result in a model construction we had not anticipated: for example `ElasticNet` with the `normalize` hyperparameter\n",
    "    * Close inspection of the sklearn documentation is important\n",
    "    * Be aware of setting \"silly\" values for different algorithms, for example:\n",
    "        * Random forest with low number of trees\n",
    "            * Would you consider it a forest if it had only 2 trees?\n",
    "        * 1 Neighbor in KNN algorithm\n",
    "            * Averaging the vote of 1 person doesn't sound very robust\n",
    "        * Increasing a hyperparameter by a very small amount is unlikely to greatly improve a model \n",
    "            * One more tree in a forest, for example, isn't likely to have a large impact\n",
    "        \n",
    "* **Try a for loop to iterate through options:\n",
    "\n",
    "```\n",
    "neighbors_list = [3, 5, 10, 20, 50, 75]\n",
    "for test_number in neightbors_list:\n",
    "    model = KNeighborsClassifier(N_neighbors = test_number)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "```\n",
    "* Store the results in a DataFrame to view the effect of this hyperparameter on the accuracy of the model:\n",
    "\n",
    "```\n",
    "results_df = pd.DataFrame({'neighbors': neighbors_list, 'accuracy':accuracy_list})\n",
    "print(results_df)\n",
    "```\n",
    "* Printing the DataFrame in this way, shows that (it appears) adding any more neighbors than 20 does not help.\n",
    "* A common tool that is used to assist with analyzing the impact of a singular hyperparameter on an end result is called a **learning curve.**\n",
    "\n",
    "```\n",
    "neighbors_list = list(range(5, 500, 5))\n",
    "accuracy_list = []\n",
    "for test_number in neighbors_list:\n",
    "    model = KNeighborsClassifier(n_neighbors = test_number)\n",
    "    predicions = model.fit(X_train, y_train_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy': accuracy_list})\n",
    "\n",
    "plt.plot(results_df['neighbors'], results_df['accuracy'])\n",
    "# Add the labels and title\n",
    "plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy', title='Accuracy for different n_neighbors')\n",
    "plt.show()\n",
    "```\n",
    "* One thing to be aware of is that Python's `range` function does not work for decimal steps, which is important for hyperparameters that work on that scale. \n",
    "    * A handy trick uses numpys **`np.linspace(start, end, num)`**:\n",
    "        * Create a number of values (`num`) evenly spread within an interval (`start`, `end`) that you specify\n",
    "\n",
    "```\n",
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test,predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581a0e0",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "* Automate 2 hyperparameters or more with a nested for loop:\n",
    "* Firstly define model creation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8a4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "    model = GradientBoostingClassifier(\n",
    "            learning_rate=learn_rate,\n",
    "            max_depth= max_depth)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de8630",
   "metadata": {},
   "source": [
    "Now we can loop through our lists of hyperparameters and call our function\n",
    "\n",
    "```\n",
    "results_list = []\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate, max_depth))\n",
    "\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'accuracy'])\n",
    "print(results_df)\n",
    "```\n",
    "* We have a nested loop so we can test all values of our first hyperparameter for all values of our second hyperparameter\n",
    "* Importantly, the relationship between models created and hyperparameters or values to test is **not a linear relationship, but an exponential one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0dc459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_grid_search(learn_rate, max_depth, subsample, max_features):\n",
    "    model = GradientBoostingClassifier(\n",
    "            learning_rate = learn_rate,\n",
    "            max_depth= max_depth,\n",
    "            subsample = subsample, \n",
    "            max_features= max_features)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c57e0",
   "metadata": {},
   "source": [
    "```\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for subsample in subsample_list:\n",
    "            for max_features in max_features_list:\n",
    "                results_list.append(gbm_grid_search(learn_rate, max_depth, subsample, max_features))\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'subsample', 'max_features', 'accuracy'])\n",
    "print(results_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28224e",
   "metadata": {},
   "source": [
    "* Safe to say, we cannot keep nesting forever, as our code becomes complex and inefficient\n",
    "\n",
    "#### Grid Search\n",
    "* **Grid Search has a number of advantages:**\n",
    "    * It's programmatic\n",
    "    * It saves many lines of code\n",
    "    * It is guaranteed to find the best model within the grid you specify \n",
    "        * But, obviously, if you specify a poor grid with silly or conflicting values, you won't get a good score.\n",
    "    * It is an easy methodology to explain compared to some other more complex ones.\n",
    "* **Grid search also has a number of disadvantages:**\n",
    "    * It is **very computationally expensive!**\n",
    "    * It is **uninformed**. Results of one model don't help creating the next model \n",
    "    * (We will cover informed methods later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20623a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613ec6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ec017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e678a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6c59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
