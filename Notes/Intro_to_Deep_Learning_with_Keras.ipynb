{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0618f10b",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras\n",
    "Deep learning is here to stay! It's the go-to technique to solve complex problems that arise with unstructured data and an incredible tool for innovation. Keras is one of the frameworks that make it easier to start developing deep learning models, and it's versatile enough to build industry-ready models in no time. In this course, you will learn regression and save the earth by predicting asteroid trajectories, apply binary classification to distinguish between real and fake dollar bills, use multiclass classification to decide who threw which dart at a dart board, learn to use neural networks to reconstruct noisy images and much more. Additionally, you will learn how to better control your models during training and how to tune them to boost their performance.\n",
    "\n",
    "**Instructor:** Miguel Esteban, co-founder of Xtreme AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfee880",
   "metadata": {},
   "source": [
    "## $\\star$ Chapter 1: Introducing Keras\n",
    "In this first chapter, you will get introduced to neural networks, understand what kind of problems they can solve, and when to use them. You will also build several networks and save the earth by training a regression model that approximates the orbit of a meteor that is approaching us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d5de1",
   "metadata": {},
   "source": [
    "* Keras is a high-level deep learning framework.\n",
    "\n",
    "#### Theano vs Keras\n",
    "* Theano is a lower level framework\n",
    "* Building a neural network in Theano can take many lines of code and requires a deep understanding of how they work internally \n",
    "\n",
    "<img src='data/Theano_vs_Keras.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* Building and training this very same network in Keras only takes a few lines of code\n",
    "\n",
    "#### Keras\n",
    "* Open source deep learning Framework\n",
    "* Enables fast experimentation with neural networks\n",
    "* Runs on top of other frameworks like TensorFlow, Theano or CNTK\n",
    "* Created by French AI researcher Fran√ßois Chollet\n",
    "\n",
    "#### Why Keras instead of other low-level libraries like TensorFlow?\n",
    "* Fast industry-ready models\n",
    "* For beginners and experts\n",
    "* Less code\n",
    "* Allows for quickly and easily checking if a neural neetwork will solve your problems\n",
    "* Build any architecture, from:\n",
    "    * simple networks\n",
    "    * more complex networks\n",
    "    * auto-encoders\n",
    "    * convolutional neural networks (CNNs)\n",
    "    * recurrent neural networks (RNNs)\n",
    "    * Deploy models in multiple platforms (Android, iOS, web-apps, etc.)\n",
    "* Keras is now fully integrated into TensorFlow2 and is TensorFlow's high-level framework of choice\n",
    "* Keras is complementary to TensorFlow\n",
    "* **Can also use TensorFlow in same code pipeline if, as you dive into deep learning, you find yourself needing to use low-level features, have a finer control of how your network applies gradients, etc.**\n",
    "\n",
    "#### Feature Engineering\n",
    "* Neural networks are good feature extractors, since they learn the best way to make sense of **unstructured data.**\n",
    "* NNs can learn the best features and their combinations, and can perform feature engineering themselves\n",
    "\n",
    "#### Unstructured data\n",
    "* **Unstructured data** is data that is not easily put into a table\n",
    "* Examples: sound, video, images, etc.\n",
    "* These are also the types of data where performing feature engineering can be more challenging, so leaving this task to NNs is often a good idea.\n",
    "\n",
    "#### When to use neural networks\n",
    "* Dealing with unstructured data\n",
    "* Don't need easily interpretable results\n",
    "* You can benefit from a known architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a83178",
   "metadata": {},
   "source": [
    "### Simple Neural Networks\n",
    "* A **neural network** is a machine learning algorithm with the training data being the input to the input layer and the predicted value being the value at the output layer\n",
    "* Each connection from one neuron to another has an associated weight, $w$.\n",
    "\n",
    "<img src='data/bias_weight.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a882c",
   "metadata": {},
   "source": [
    "* Each neuron, except the input layer which just holds the input value, also has an extra weight we call the bias weight, $b$.\n",
    "* During **forward propagation** (feed-forward), our input gets transformed by weight multiplications and additions at each layer, the output of each neuron can also get transformed by the application of what's called an **activation function**.\n",
    "\n",
    "<img src='data/grad_descent.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b301db",
   "metadata": {},
   "source": [
    "* Learning in neural networks consists of tuning the weights or parameters to give the desired output\n",
    "* One way of achieving this is by using gradient descent and applying weight updates incrementally via back-propagation\n",
    "\n",
    "### The sequential API\n",
    "* Keras allows you to build models in 2 different ways; using either the Functional API or the Sequential API\n",
    "* The sequential API is a simple, yet very powerful way of building neural networks that will get you covered for most use cases\n",
    "* With the sequential API, you're essentially building a model as a stack of layers\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a new sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input AND dense layer\n",
    "model.add(Dense(2, input_shape=(3,)))\n",
    "```\n",
    "* **In this last line of code, *we add 2 layers*: a 2-neuron Dense fully-connected layer, and an input later consisting of 3 neurons.**\n",
    "\n",
    "```\n",
    "# Add a final 1-neuron layer\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "#### model.summary()\n",
    "* **Parameters** are the weights, including the bias weight of each neuron in a given layer (or the model as a whole)\n",
    "* **When the input layer is defined via the `input_shape` parameter, as we did before, it is not shown as a layer in the summary, but is included in the layer where it was defined** (in the case above, within the first Dense layer).\n",
    "\n",
    "<img src='data/visualize_parameters.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfcd75",
   "metadata": {},
   "source": [
    "* The image above helps illustrate why the layer **`dense_3`** has **8 parameters**:\n",
    "    * **6 parameters** (or weights) come from the connections of the 3 input neurons to the 2 neurons in the layer (green lines)\n",
    "    * **2 parameters** come from the bias weights, `b0`, and `b1`, 1 per each neuron in the hidden layer\n",
    "    \n",
    "#### Exercises: Hello nets!\n",
    "You're going to build a simple neural network to get a feeling of how quickly it is to accomplish this in Keras.\n",
    "\n",
    "You will build a network that **takes two numbers as an input**, passes them through **a hidden layer of 10 neurons**, and finally **outputs a single non-constrained number**.\n",
    "\n",
    "A **non-constrained output can be obtained by avoiding setting an activation function in the output layer**. This is useful for problems like regression, when we want our output to be able to take any non-constrained value.\n",
    "\n",
    "<img src='data/ex1_vis.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "# Import the Sequential model and Dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b80060",
   "metadata": {},
   "source": [
    "#### Exercises: Counting Parameters\n",
    "You've just created a neural network. But you're going to create a new one now, taking some time to think about the weights of each layer. The Keras `Dense` layer and the `Sequential` model are already loaded for you to use.\n",
    "\n",
    "This is the network you will be creating:\n",
    "\n",
    "<img src='data/ex2_vis.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3,), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()\n",
    "```\n",
    "**There are 20 parameters, 15 from the connections of our inputs to our hidden layer and 5 from the bias weight of each neuron in the hidden layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5c43b",
   "metadata": {},
   "source": [
    "#### Exercises: Build as shown!\n",
    "You will take on a final challenge before moving on to the next lesson. Build the network shown in the picture below. Prove your mastered Keras basics in no time!\n",
    "\n",
    "<img src='data/ex3_vis.png' width=\"200\" height=\"100\" align=\"center\"/>\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Build the input and hidden layer\n",
    "model.add(Dense(3, input_shape=(2,)))\n",
    "\n",
    "# Add the ouput layer\n",
    "model.add(Dense(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bbe2e8",
   "metadata": {},
   "source": [
    "### Surviving a meteor\n",
    "* **`loss_function` is the function we are trying minimize during training.**\n",
    "* **Compiling a model produces no output.**\n",
    "    * But, our model is now ready to train.\n",
    "* Creating a model is useless if we don't train it.\n",
    "\n",
    "#### Training\n",
    "\n",
    "```\n",
    "# Train your model\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "#### Predicting\n",
    "* We can store predictions in a variable for later use.\n",
    "* The predictions are stored as numbers in a numpy array \n",
    "\n",
    "```\n",
    "# Predict on new data\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Look at the predicitons\n",
    "print(preds)\n",
    "```\n",
    "\n",
    "#### Evaluating\n",
    "* Feed-forward consists in computing a model's output from a given set of inputs\n",
    "* It then computes the error comparing the results to the true values stored in `y_test`\n",
    "\n",
    "```\n",
    "# Evaluate your results\n",
    "model.evaluate(X_test, y_test)\n",
    "```\n",
    "\n",
    "#### Exercises: Specifying a model \n",
    "You will build a simple regression model to predict the orbit of the meteor!\n",
    "\n",
    "Your training data consist of measurements taken at time steps from **-10 minutes before the impact region to +10 minutes after**. Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor orbit at that time step.\n",
    "\n",
    "*Note that you can view this problem as approximating a quadratic function via the use of neural networks.*\n",
    "\n",
    "<img src='data/impact_reg.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "This data is stored in two numpy arrays: one called `time_steps` , what we call *features*, and another called `y_positions`, with the *labels*. Go on and build your model! It should be able to predict the y positions for the meteor orbit at future time steps.\n",
    "\n",
    "Keras `Sequential` model and `Dense` layers are available for you to use.\n",
    "\n",
    "```\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "#### Training\n",
    "You're going to train your first model in this course, and for a good cause!\n",
    "\n",
    "Remember that **before training your Keras models you need to compile them**. This can be done with the `.compile()` method. The `.compile()` method takes arguments such as the `optimizer`, used for weight updating, and the `loss` function, which is what we want to minimize. Training your model is as easy as calling the `.fit()` method, passing on the features, labels and a number of epochs to train for.\n",
    "\n",
    "The regression `model` you built in the previous exercise is loaded for you to use, along with the `time_steps` and `y_positions` data. Train it and evaluate it on this very same data, let's see if your model can learn the meteor's trajectory.\n",
    "\n",
    "```\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))\n",
    "```\n",
    "\n",
    "#### Exercises: Predicting the orbit!\n",
    "You've already trained a `model` that approximates the orbit of the meteor approaching Earth and it's loaded for you to use.\n",
    "\n",
    "Since you trained your model for values between -10 and 10 minutes, your model hasn't yet seen any other values for different time steps. You will now visualize how your model behaves on unseen data.\n",
    "\n",
    "If you want to check the source code of `plot_orbit`, paste `show_code(plot_orbit)` into the console.\n",
    "\n",
    "Hurry up, the Earth is running out of time!\n",
    "\n",
    "*Remember `np.arange(x,y)` produces a range of values from **x** to **y-1**. That is the `[x, y)` interval.*\n",
    "\n",
    "```\n",
    "# Predict the twenty minutes orbit\n",
    "twenty_min_orbit = model.predict(np.arange(-10, 11))\n",
    "\n",
    "# Plot the twenty minute orbit \n",
    "plot_orbit(twenty_min_orbit)\n",
    "\n",
    "# Predict the eighty minute orbit\n",
    "eighty_min_orbit = model.predict(np.arange(-40, 41))\n",
    "\n",
    "# Plot the eighty minute orbit \n",
    "plot_orbit(eighty_min_orbit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35940c",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "* We use binary classification when we want to solve problems where you predict whether an observation belongs to one of two possible classes\n",
    "\n",
    "<img src='data/bin_class_plot.png' width=\"500\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd3081",
   "metadata": {},
   "source": [
    "* The coordinates are pairs of values corresponding to the X and Y coordinates of each circle in the graph\n",
    "* The labels are `1` for red circles and `0` for blue circles \n",
    "\n",
    "#### Pairplot\n",
    "* We can make use of seaborn's `pairplot` function to explore a small dataset and identify whether our classification problem will be easily separable\n",
    "* We can get an intuition for this is we see that the classes separate well-enough along several variables\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot a pairplot\n",
    "sns.pairplot(circles, hue='target')\n",
    "```\n",
    "\n",
    "<img src='data/pplots.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90e734",
   "metadata": {},
   "source": [
    "* In this case, for the circles dataset, there is a very clear boundary. The red circle concentrate at the center while the blue are outside. It should be easy for our network to find a way to separate them just based on x and y coordinates\n",
    "\n",
    "#### The sigmoid function\n",
    "* You can consider the output of the sigmoid function as the probability of a pair of coordinates being in one class or another\n",
    "* So we can set a threshold and say everything below 0.5 will be a blue circle and everything above 0.5, a red circle.\n",
    "\n",
    "#### Binary crossentropy\n",
    "* **Binary cross-entropy** is the function we use when our output neuron is using sigmoid as its activation function $\\star$\n",
    "* For binary classification; sigmoid function will be used to predict logistic regression/binary classification problems.\n",
    "\n",
    "```\n",
    "# Compile model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.train(coordinates, labels, epochs=20)\n",
    "\n",
    "# Predict with trained model\n",
    "preds = model.predict(coordinates)\n",
    "```\n",
    "\n",
    "* Note that we obtain the predicted labels by calling `predict` on `coordinates`\n",
    "\n",
    "<img src='data/circ_class.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12041f66",
   "metadata": {},
   "source": [
    "```\n",
    "# Import the sequential model and dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()\n",
    "\n",
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs = 20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a4d47",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "* If we have more than two classes to classify, then we have a multi-class classification problem.\n",
    "* Outcomes of multi-class classification problems must be **mutually exclusive**\n",
    "* The activation function of choice for the *final* layer of a mulit-class classification neural network is `softmax`.\n",
    "* Output values will be provided as probabilities, and the class with the highest probability is chosen as the model's prediction.\n",
    "\n",
    "```\n",
    "# Instantiate a sequential model \n",
    "# ...\n",
    "# Add an input and hidden layer\n",
    "# ...\n",
    "# Add more hidden layers\n",
    "# ...\n",
    "# Add your output layer\n",
    "model.add(Dense(4, activation='softmax')\n",
    "```\n",
    "\n",
    "#### Categorical cross-entropy\n",
    "* When compiling your model, instead of binary cross-entropy as is used for binary classification problems, we use categorical cross-entropy (aka **log loss**).\n",
    "* **Categorical cross-entropy** measures the difference between the predicted probabilities and the true label of the class we should have predicted\n",
    "\n",
    "<img src='data/log_loss1.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* So, if we should have predicted `1` for a given class, by taking a look at the graph above, we see we would get high loss values for predicting close to `0` (since we'd be \"very\" wrong) and low loss values for predicting closer to 1 (the true label).\n",
    "\n",
    "* Since our outputs are vectors containing the probabilities of each class, our neural network must also be trained with vectors representing this concept. To achieve that, we make use of the `keras.utils.to_categorical` function\n",
    "* We first turn our response variable into a categorical variable with pandas `Categorical`; This allows us to redefine the column using the categorical codes (**cat codes**) of the different categories \n",
    "* Once our categories are each represented by a unique integer, we can use the `to_categorical` function to turn them into one-hot-encoded vectors, where each component is 0 except for the one corresponding to the labeled categories\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Turn response variable into labeled codes\n",
    "df.response = pd.Categorical(df.response)\n",
    "df.response.cat.codes\n",
    "\n",
    "# Turn response variable into one-hot response vector\n",
    "y = to_categorical(df.response)\n",
    "```\n",
    "\n",
    "#### Label encoding vs one-hot encoding\n",
    "\n",
    "<img src='data/label_vs_ohe.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* Keras `to_categorical` essentially performs the process described in the picture above (of transformed label-encoded variables into one-hot-encoded variables).\n",
    "\n",
    "#### Exercises: A multi-class model\n",
    "You're going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart's x and y coordinates on the board.)\n",
    "\n",
    "This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes/labels are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the `softmax` activation function to achieve a total sum of probabilities of 1 over all competitors.\n",
    "\n",
    "Keras `Sequential` model and `Dense` layer are already loaded for you to use.\n",
    "\n",
    "```\n",
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```          \n",
    "\n",
    "#### Exercises: Prepare your dataset\n",
    "In the console you can check that your labels, `darts.competitor` are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the `to_categorical()` function from `keras.utils` to turn these numbers into their one-hot encoded representation.\n",
    "\n",
    "This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated.\n",
    "\n",
    "The dart's dataset is loaded as `darts`. Pandas is imported as `pd`. Let's prepare this dataset!\n",
    "\n",
    "```\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes\n",
    "\n",
    "# Print the label encoded competitors\n",
    "print('Label encoded competitors: \\n',darts.competitor.head())\n",
    "\n",
    "# Drop the original competitor column with competitors' names\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "\n",
    "# Use to_categorical on your labels\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)\n",
    "```\n",
    "\n",
    "#### Exercises: Training on dart throwers\n",
    "Your model is now ready, just as your dataset. It's time to train!\n",
    "\n",
    "The `coordinates` features and `competitors` labels you just transformed have been partitioned into `coord_train`, `coord_test` and `competitors_train`, `competitors_test`.\n",
    "\n",
    "Your `model` is also loaded. Feel free to visualize your training data or `model.summary()` in the console.\n",
    "\n",
    "Let's find out who threw which dart just by looking at the board!\n",
    "\n",
    "```\n",
    "# Fit your model to the training data for 200 epochs\n",
    "model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bea96",
   "metadata": {},
   "source": [
    "#### Exercises: Softmax predictions\n",
    "Your recently trained `model` is loaded for you. This model is generalizing well!, that's why you got a high accuracy on the test set.\n",
    "\n",
    "Since you used the `softmax` activation function, for every input of 2 coordinates provided to your model there's an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors.\n",
    "\n",
    "When computing accuracy with the model's `.evaluate()` method, your model takes the class with the highest probability as the prediction. `np.argmax()` can help you do this since it returns the index with the highest value in an array.\n",
    "\n",
    "Use the collection of test throws stored in `coords_small_test` and `np.argmax()`to check this out!\n",
    "\n",
    "```\n",
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699999fc",
   "metadata": {},
   "source": [
    "### Multi-label classification\n",
    "* Now that we know how multi-class classification works, we can take a look at **multi-label classification.**\n",
    "* Both deal with predicting classes, but in multi-label classification, a single input can be assigned to more than one class\n",
    "* Real world example: Movie genre classification- could be multi-label, for example: Drama, Suspense, Action\n",
    "\n",
    "<img src='data/multiclass_vs_multilabel.png' width=\"500\" height=\"250\" align=\"center\"/>\n",
    "\n",
    "* Imagine we have three clases: `sun`, `moon`, and `clouds`\n",
    "* In **multi-class problems**, if we took a sample of our observations, each individual in the sample will belong to a unique class\n",
    "* However, in **multi-label problems**, each individual in the sample can have **all, none, or some subset of the available classes.**\n",
    "* As you can see in the image, multilabel vectors are also **one-hot encoded** (there's a 1 or a 0 representing the presence or absence of each class).\n",
    "\n",
    "### Multi-label architecture\n",
    "* Making a multi-label model is not very different from building a multi-class model\n",
    "* For the sake of this example, we will assume that to differentiate between these three different classes, we need just one input and 2 hidden neurons\n",
    "* The **biggest changes** (between this model and the multiclass model) happen in the **output layer** and in its **activation function**.\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input and hidden layers\n",
    "model.add(Dense(2, input_shape=(1,)))\n",
    "\n",
    "# Add an output layer for the 3 classes and sigmoid activation\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "* **In the output layer, we use as many neurons as possible classes** and we also use **sigmoid activation**\n",
    "* **We use sigmoid outputs because we no longer care about the sum of probabilities.** We are not deciding **between** or **among** possible outcomes, but rather **selecting any and all possible outcomes with a probabilty greater than 0.5.**\n",
    "\n",
    "<img src='data/multilabel_outcome_probabilities.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5d293",
   "metadata": {},
   "source": [
    "* We want each output neuron to be able to individually take a value between 0 and 1\n",
    "* This can be achieved with the sigmoid activation because it constrains our neuron output in range 0-1. (This is what we did in binary classification, though we only had one output neuron there).\n",
    "* **Binary cross-entropy** is now used as the loss function when compiling the model \n",
    "* You can look at is **as if we were performing several binary classification problems; for each output we are deciding whether or not its corresponding label is present.**\n",
    "\n",
    "```\n",
    "# Compile the model with binary crossentropy\n",
    "model.compile(optimizer='adam', loss = binary_crossentropy')\n",
    "\n",
    "# Train your model, recall validation_split\n",
    "model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
    "```\n",
    "\n",
    "* By using `validation_split`, a percentage of training data is left out for testing at each epoch.\n",
    "* Using neural networks for multi-label classification can be performed by minor tweaks in our model architecture\n",
    "\n",
    "<img src='data/one_vs_rest.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* If we were to use a classical machine learning approach to solve multi-label problems, we would need more complex methods.\n",
    "    * One way to do so would be to train several classifiers to distinguish each particular class from the rest\n",
    "    * This is called **one-vs-rest classification** and is illustrated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c05ca",
   "metadata": {},
   "source": [
    "### Keras Callbacks\n",
    "* Now that we've trained quite a few models, it's time to learn more about how to better control and supervise model training by using **callbacks**.\n",
    "* A **callback** is a function that is executed after some other function, event, or task has finished.\n",
    "* A Keras callback is a block of code that gets executed after each epoch during training or after the training is finished\n",
    "    * `EarlyStopping`\n",
    "    * `ModelCheckpoint`\n",
    "    * `History`\n",
    "* They are useful to store metrics as the model trains and to make decisions as the training goes by\n",
    "* Every time you call the fit method on a keras model, there's a callback object that gets returned after the model finishes training\n",
    "    * This is the **`history`** attribute, which is a python dictionary.\n",
    "    * Within the `history` attribute, we can check the **saved metrics of the model at each epoch during training as an array of numbers.**\n",
    "    \n",
    "```\n",
    "# Training a model and saving its history\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 100,\n",
    "                    metrics = ['accuracy'])\n",
    "print(history.history['loss'])                    \n",
    "```\n",
    "* To get the most out of the history object, we should use the the `validation_data` parameter in our fit method, passing `X_test` and `y_test` as a tuple, as shown below:\n",
    "\n",
    "```\n",
    "# Training a model and saving its history\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs = 100,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    metrics=['accuracy'])\n",
    "print(history.history['val_loss']              \n",
    "```\n",
    "* The `validation_split` parameter can be used instead too, specifying a percentage of the training data that will be left out for testing purposes\n",
    "* That way, we not only have the training metrics, but also the validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ebe84",
   "metadata": {},
   "source": [
    "## History plots\n",
    "* You can compare training and validation metrics with a few matplotlib commands\n",
    "* We just need to define a figure\n",
    "* Plot the values of the history attribute for the training accuracy (`acc`) and the validation accuracy (`val_acc`)\n",
    "\n",
    "```\n",
    "# Plot train vs test accuracy per epoch\n",
    "plt.figure()\n",
    "\n",
    "# Use history metrics\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "# Make it pretty\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src='data/history_plots.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* **We can see our model accuracy increases for both training and test sets until it reaches epoch 25**\n",
    "* Then accuracy flattens for the test set whilst the training keeps improving\n",
    "* At that point, overfitting is occurring, since we see the training set keeps improving, as the test set plateaus and then even decreases in accuracy).\n",
    "\n",
    "### Early stopping\n",
    "* Early stopping a model can solve the overfitting problem, since it stops training when it no longer improves\n",
    "* This is extremely useful since deep neural models can take a long time to train and we don't know beforehand how many epochs will be needed\n",
    "* The early stopping callback can monitor several metrics, like validation accuracy, validation loss, etc. specified with the **`monitor`** parameter.\n",
    "* It is also important to define a **`patience`** argument, or, the number of epochs to wait for the model to improve before stopping its training\n",
    "    * There aren't any rules to decide which patience number works best at any time, and this depends mostly on the implementation\n",
    "\n",
    "```\n",
    "# Import early stopping from keras callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Instantiate an early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train your model with the callback\n",
    "model.fit(X_train, y_train, epochs=100, \n",
    "                            validation_data = (X_test, y_test),\n",
    "                            callbacks = [early_stopping])\n",
    "```\n",
    "* The callback is passed as a list to the `callbacks` parameter in the model `fit` method\n",
    "\n",
    "### Model checkpoint\n",
    "* The model checkpoint callback allows us to save our model as it trains \n",
    "* We specify the model filename with a name and the `.hdf5` extension\n",
    "* You can also decide what to monitor to determine which model is best with the **`monitor`** parameter; **by default validation loss is monitored**\n",
    "* Setting the `save_best_only` parameter to `True` guarantees that the latest best model according to the quantity monitored wil not be overwritten\n",
    "\n",
    "```\n",
    "# Import model checkpoint from keras callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Instantiate a model checkpoint callback\n",
    "model_save = ModelCheckpoint('best_model.hdf5', save_best_only=True)\n",
    "\n",
    "# Train your model with the callback\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),\n",
    "                                        callbacks = [model_save])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907cfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2f1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd435d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320ea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0267ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c089b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38956ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce14eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f4933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa41e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8d477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00148cff",
   "metadata": {},
   "source": [
    "<img src='data/visualize_parameters.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
