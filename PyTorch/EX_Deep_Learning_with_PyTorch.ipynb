{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914ea322",
   "metadata": {},
   "source": [
    "Note: This notebook was completed as part of DataCamp's course by the same name.\n",
    "# Introduction to Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e9012a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5513d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269a88d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046c310",
   "metadata": {},
   "source": [
    "### Neural Networks \n",
    "\n",
    "Let us see the differences between neural networks which apply `ReLU` and those which do not apply `ReLU`. We will prove that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n",
    "\n",
    "<img src='data/net1.png' width=\"700\" height=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4069070",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = torch.tensor([[ 0.0401, -0.9005,  0.0397, -0.0876]])\n",
    "\n",
    "weight_1 = torch.tensor([[-0.1094, -0.8285,  0.0416, -1.1222],\n",
    "                        [ 0.3327, -0.0461,  1.4473, -0.8070],\n",
    "                        [ 0.0681, -0.7058, -1.8017,  0.5857],\n",
    "                        [ 0.8764,  0.9618, -0.4505,  0.2888]])\n",
    "\n",
    "weight_2 = torch.tensor([[ 0.6856, -1.7650,  1.6375, -1.5759],\n",
    "                        [-0.1092, -0.1620,  0.1951, -0.1169],\n",
    "                        [-0.5120,  1.1997,  0.8483, -0.2476],\n",
    "                        [-0.3369,  0.5617, -0.6658,  0.2221]])\n",
    "\n",
    "weight_3 = torch.tensor([[ 0.8824,  0.1268,  1.1951,  1.3061],\n",
    "                        [-0.8753, -0.3277, -0.1454, -0.0167],\n",
    "                        [ 0.3582,  0.3254, -1.8509, -1.4205],\n",
    "                        [ 0.3786,  0.5999, -0.5665, -0.3975]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0033ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n",
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7a196",
   "metadata": {},
   "source": [
    "### ReLU activation\n",
    "Now we are going to build a neural network which has non-linearity and by doing so, we are going to demonstrate that networks with multiple layers and non-linearity functions cannot be expressed as a neural network with one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9498fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2770, -0.0345, -0.1410, -0.0664]])\n",
      "tensor([[-0.2117, -0.4782,  4.0438,  3.0417]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate non-linearity\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Apply non-linearity on the hidden layers\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febbc50",
   "metadata": {},
   "source": [
    "Awesome! As expected, the results are different from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc53a43",
   "metadata": {},
   "source": [
    "### ReLU activation again\n",
    "Neural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the `ReLU` activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have 4 features, but then the first hidden layer will have 6 units and the output layer will have 2 units.\n",
    "\n",
    "<img src='data/net2.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7764860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "weight_1 = torch.rand(4, 6)\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8dffc",
   "metadata": {},
   "source": [
    "Great! You can now build neural networks with different number of neurons on each layer.\n",
    "\n",
    "During the course, we are going to use 2 datasets: MNIST and CIFAR-10. MNIST is arguably the most famous dataset in computer vision. Both data sets are part of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eee5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a783a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac6987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ecdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fbebf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89567c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e56c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2bef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15aedb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb01677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bae40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12697de2",
   "metadata": {},
   "source": [
    "<img src='data/net.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
