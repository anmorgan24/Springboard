{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914ea322",
   "metadata": {},
   "source": [
    "Note: This notebook was completed as part of DataCamp's course by the same name.\n",
    "# Introduction to Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "269a88d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6493db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0.post2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb51614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0a0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046c310",
   "metadata": {},
   "source": [
    "### Neural Networks \n",
    "\n",
    "Let us see the differences between neural networks which apply `ReLU` and those which do not apply `ReLU`. We will prove that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n",
    "\n",
    "<img src='data/net1.png' width=\"700\" height=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4069070",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = torch.tensor([[ 0.0401, -0.9005,  0.0397, -0.0876]])\n",
    "\n",
    "weight_1 = torch.tensor([[-0.1094, -0.8285,  0.0416, -1.1222],\n",
    "                        [ 0.3327, -0.0461,  1.4473, -0.8070],\n",
    "                        [ 0.0681, -0.7058, -1.8017,  0.5857],\n",
    "                        [ 0.8764,  0.9618, -0.4505,  0.2888]])\n",
    "\n",
    "weight_2 = torch.tensor([[ 0.6856, -1.7650,  1.6375, -1.5759],\n",
    "                        [-0.1092, -0.1620,  0.1951, -0.1169],\n",
    "                        [-0.5120,  1.1997,  0.8483, -0.2476],\n",
    "                        [-0.3369,  0.5617, -0.6658,  0.2221]])\n",
    "\n",
    "weight_3 = torch.tensor([[ 0.8824,  0.1268,  1.1951,  1.3061],\n",
    "                        [-0.8753, -0.3277, -0.1454, -0.0167],\n",
    "                        [ 0.3582,  0.3254, -1.8509, -1.4205],\n",
    "                        [ 0.3786,  0.5999, -0.5665, -0.3975]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0033ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n",
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3d20c",
   "metadata": {},
   "source": [
    "### ReLU activation\n",
    "Now we are going to build a neural network which has non-linearity and by doing so, we are going to demonstrate that networks with multiple layers and non-linearity functions cannot be expressed as a neural network with one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9498fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2770, -0.0345, -0.1410, -0.0664]])\n",
      "tensor([[-0.2117, -0.4782,  4.0438,  3.0417]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate non-linearity\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Apply non-linearity on the hidden layers\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c24af9",
   "metadata": {},
   "source": [
    "Awesome! As expected, the results are different from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499bd66",
   "metadata": {},
   "source": [
    "### ReLU activation again\n",
    "Neural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the `ReLU` activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have 4 features, but then the first hidden layer will have 6 units and the output layer will have 2 units.\n",
    "\n",
    "<img src='data/net2.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7764860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "weight_1 = torch.rand(4, 6)\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270641b8",
   "metadata": {},
   "source": [
    "Great! You can now build neural networks with different number of neurons on each layer.\n",
    "\n",
    "During the course, we are going to use 2 datasets: MNIST and CIFAR-10. MNIST is arguably the most famous dataset in computer vision. Both data sets are part of PyTorch.\n",
    "\n",
    "# Building a Neural Network with PyTorch: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f060be5",
   "metadata": {},
   "source": [
    "### Preparing the MNIST dataset\n",
    "You are going to prepare dataloaders for MNIST training and testing set. MNIST images are grayscale, and therefore one-channel based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0160ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to torch tensors and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307), (0.3081))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2038d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-_4mbk50o/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Preparing training set and test set\n",
    "trainset = torchvision.datasets.MNIST('mnist', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('mnist', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50dd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training loader and test loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3db87d",
   "metadata": {},
   "source": [
    "#### Inspecting the dataloaders\n",
    "Now you are going to explore a bit the dataloaders you created. In particular, you will compute the shape of the dataset in addition to the minibatch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b509557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "# Compute the shape of the training set and testing set\n",
    "trainset_shape = train_loader.dataset.train_data.shape\n",
    "testset_shape = test_loader.dataset.test_data.shape\n",
    "\n",
    "# Print the computed shapes\n",
    "print(trainset_shape, testset_shape)\n",
    "\n",
    "# Compute the size of the minibatch for training set and testing set\n",
    "trainset_batchsize = train_loader.batch_size\n",
    "testset_batchsize = test_loader.batch_size\n",
    "\n",
    "# Print sizes of the minibatch\n",
    "print(trainset_batchsize, testset_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97dce68",
   "metadata": {},
   "source": [
    "#### Building the Neural Network\n",
    "Build a class for a neural network which will be used to train on the MNIST dataset. The dataset contains images of shape (28, 28, 1), so you should deduct the size of the input layer. For hidden layer use 200 units, while for output layer use 10 units (1 for each class). For activation function, use `relu` in a functional way.\n",
    "\n",
    "For context, the same net will be trained and used to make predictions in the next two exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c22dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class Net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):    \n",
    "    \t# Define all the parameters of the net\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):   \n",
    "    \t# Do the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc14d97",
   "metadata": {},
   "source": [
    "#### Training a Neural Network\n",
    "Given the fully connected neural network (called `model`) which you built in the previous exercise and a train loader called `train_loader` containing the MNIST dataset (which we created for you), you're to train the net in order to predict the classes of digits. You will use the Adam optimizer to optimize the network, and considering that this is a classification problem you are going to use cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990fc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
    "model = Net()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "for batch_idx, data_target in enumerate(train_loader):\n",
    "    data = data_target[0]\n",
    "    target = data_target[1]\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Complete a forward pass\n",
    "    output = model(data)\n",
    "\n",
    "    # Compute the loss, gradients and change the weights\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc4bc5",
   "metadata": {},
   "source": [
    "#### Using the network to make predictions\n",
    "Now that you have trained the network, use it to make predictions for the data in the testing set. The network is called `model` (same as in the previous exercise), and the loader is called `test_loader`. First initialize the variables `total` and `correct` to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af397835",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1188f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set accuracy of the network is: 95 %\n"
     ]
    }
   ],
   "source": [
    "# Set the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # Put each image into a vector\n",
    "    inputs = inputs.view(-1, 28*28)\n",
    "    \n",
    "    # Do the forward pass and get the predictions\n",
    "    outputs = model(inputs)\n",
    "    _, outputs = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645741e8",
   "metadata": {},
   "source": [
    "## Your first CNN - __init__ method\n",
    "You are going to build your first convolutional neural network. You're going to use the MNIST dataset as the dataset, which is made of handwritten digits from 0 to 9. The convolutional neural network is going to have 2 convolutional layers, each followed by a ReLU nonlinearity, and a fully connected layer. Remember that each pooling layer halves both the height and the width of the image, so by using 2 pooling layers, the height and width are 1/4 of the original sizes. MNIST images have shape (1, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60aa0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size =2, stride=2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(49*10, 10)\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 10)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "261cfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to torch tensors and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307), (0.3081))\n",
    "])\n",
    "\n",
    "# Preparing the training and test set\n",
    "trainset = torchvision.datasets.MNIST('mnist', train=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('mnist', train=False, transform=transform)\n",
    "\n",
    "# Prepare loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6f998",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3c6e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6719d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /tmp/pip-req-build-_4mbk50o/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the forward pass\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # Compute the loss function\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce584e60",
   "metadata": {},
   "source": [
    "#### Making predictions\n",
    "This is the entire reason why the field of deep learning has bloomed in the last few years, as neural networks predictions are extremely accurate. On this exercise, we are going to use the convolutional neural network you already trained in order to make predictions on the MNIST dataset.\n",
    "\n",
    "Remember that `torch.max()` takes two arguments: -`output.data` - the tensor which contains the data.\n",
    "\n",
    "Either `1` to do argmax or `0` to do `max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92e35a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yipes, your net made the right prediction tensor([7])\n",
      "Yipes, your net made the right prediction tensor([2])\n",
      "Yipes, your net made the right prediction tensor([1])\n",
      "Yipes, your net made the right prediction tensor([0])\n",
      "Yipes, your net made the right prediction tensor([4])\n",
      "Yipes, your net made the right prediction tensor([1])\n",
      "Yipes, your net made the right prediction tensor([4])\n",
      "Yipes, your net made the right prediction tensor([9])\n",
      "Yipes, your net made the right prediction tensor([5])\n",
      "Yipes, your net made the right prediction tensor([9])\n",
      "Yipes, your net made the right prediction tensor([0])\n",
      "Yipes, your net made the right prediction tensor([6])\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "# Iterate over the data in the test_loader\n",
    "for i, data in enumerate(test_loader):\n",
    "    # Get the image and label from data\n",
    "    image, label = data\n",
    "    \n",
    "    # Make a forward pass in the net with your image\n",
    "    output = net(image)\n",
    "    \n",
    "    # Argmax the results of the net\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    if predicted == label:\n",
    "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
    "    else:\n",
    "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + str(label))\n",
    "        \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737389e0",
   "metadata": {},
   "source": [
    "## The sequential model\n",
    "Having learned about the sequential module, now is the time to see how you can convert a neural network that doesn't use sequential modules to one that uses them. We are giving the code to build the network in the usual way, and you are going to write the code for the same network using sequential modules.\n",
    "\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 40, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10) \n",
    "\n",
    "    def forward():\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.pool(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.pool(self.conv4(x)))\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "We want the pooling layer to be used after the second and fourth convolutional layers, while the relu nonlinearity needs to be used after each layer except the last (fully-connected) layer. For the number of filters (kernels), stride, passing, number of channels and number of units, use the same numbers as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "834da97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024), nn.ReLU(inplace=True),\n",
    "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(2048, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
