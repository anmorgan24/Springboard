{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f06886",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0195230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, RepeatedKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error as mse, mean_absolute_error as mae, SCORERS\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b9ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe OHE\n",
    "#parse datetime column\n",
    "df_ohe = pd.read_csv('ohe_data.csv', parse_dates=['date'])\n",
    "df_ohe.drop(columns='Unnamed: 0', inplace=True)\n",
    "df_ohe.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cc3981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg_Temp(F)</th>\n",
       "      <th>Conf_Cases</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>new_case_percent_pop*</th>\n",
       "      <th>state_id_CT</th>\n",
       "      <th>state_id_MA</th>\n",
       "      <th>state_id_ME</th>\n",
       "      <th>state_id_VT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>26.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>36.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>55.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.014225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>46.94</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028450</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>42.98</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Avg_Temp(F)  Conf_Cases  day_of_week  day_of_year  Year  Month  \\\n",
       "date                                                                         \n",
       "2020-03-01        26.42         1.0            6           61  2020      3   \n",
       "2020-03-02        36.50         1.0            0           62  2020      3   \n",
       "2020-03-03        55.94         1.0            1           63  2020      3   \n",
       "2020-03-04        46.94         2.0            2           64  2020      3   \n",
       "2020-03-05        42.98         8.0            3           65  2020      3   \n",
       "\n",
       "            Day  new_case_percent_pop*  state_id_CT  state_id_MA  state_id_ME  \\\n",
       "date                                                                            \n",
       "2020-03-01    1               0.014225            0            1            0   \n",
       "2020-03-02    2               0.014225            0            1            0   \n",
       "2020-03-03    3               0.014225            0            1            0   \n",
       "2020-03-04    4               0.028450            0            1            0   \n",
       "2020-03-05    5               0.113799            0            1            0   \n",
       "\n",
       "            state_id_VT  \n",
       "date                     \n",
       "2020-03-01            0  \n",
       "2020-03-02            0  \n",
       "2020-03-03            0  \n",
       "2020-03-04            0  \n",
       "2020-03-05            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4584343",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e7fdc",
   "metadata": {},
   "source": [
    "Before training any models on the data, let's do a train-test split to keep training and testing data consistent and separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1b041e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2224"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e4f26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 2224, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bf3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "X, y = df_ohe.drop(columns=['Conf_Cases', 'new_case_percent_pop*']), df_ohe['new_case_percent_pop*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c990903",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binned = np.digitize(y, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1741c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y_binned, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2bb775",
   "metadata": {},
   "source": [
    "## Using the mean as a baseline prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedbf61",
   "metadata": {},
   "source": [
    "Previously, we determined the R2 score of using the mean to predict COVID19 cases for each individual state. Let's do the same thing now that we have all the states in one DataFrame, so that we will have a baseline \"dummy\" model to compare our future optimized models to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d6a3b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score of using the mean to predict COVID19 cases in our states is: -0.0008611149015183006\n"
     ]
    }
   ],
   "source": [
    "dummy_mean = DummyRegressor()\n",
    "# \"Train\" dummy regressor\n",
    "dummy_mean.fit(X_train, y_train)\n",
    "# Get R2 score\n",
    "score_dummy = dummy_mean.score(X_test, y_test)\n",
    "print(\"The R2 score of using the mean to predict COVID19 cases in our states is:\", score_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69401f7b",
   "metadata": {},
   "source": [
    "Let's store the evaluation metric values for the dummy regressor so we can compare them with our future models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc2a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred = dummy_mean.predict(X_test)\n",
    "dummy_r2 = r2_score(y_test, dummy_pred)\n",
    "dummy_mse = mse(y_test, dummy_pred)\n",
    "dummy_rmse = np.sqrt(mse(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ca28d",
   "metadata": {},
   "source": [
    "## Tuning the top performing models for ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfcff3",
   "metadata": {},
   "source": [
    "In the pre-processing step, we determined (with the help of Pycaret) that our top performing models were **CatBoost Regressor**, **Random Forest Regressor**, and **Extra Trees Regressor**. Let's now fine tune the hyperparameters of each of these models, using Coarse to Fine hyperparameter tuning, as well as Bayesian Optimization (where applicable), in preparation for feeding them into the pipeline of the Voting Regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c46073",
   "metadata": {},
   "source": [
    "### 1. Random Forest Regressor\n",
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself. I'll explore a couple different ways to determine optimal hyperparameters and will choose those that produce the best R2 and MSE for the final ensemble model (a VotingRegressor).\n",
    "\n",
    "First, let's run a basic hyperparameter grid search and check the resulting R2, MSE, and RMSE. Then we'll put these values in a DataFrame with the those obtained from the dummy regressor (above) and compare the results of each set of hyperparameters determined throughout the course of the Coarse to Fine hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd07755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to search for GridSearchCV\n",
    "basic_param_grid = {'n_estimators': [100, 300, 500, 900, 1200],\n",
    "              'max_depth': [3, 5, 20, 50, 100],\n",
    "              }\n",
    "# Instantiate RandomForestRegressor\n",
    "basic_rf = RandomForestRegressor(random_state=42)\n",
    "cv_rf = GridSearchCV(basic_rf, basic_param_grid, cv = 5)\n",
    "cv_rf_fit = cv_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f36ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal max_depth for the RandomForestRegressor is: 50\n",
      "The optimal n_estimators for the RandomForestRegressor is: 300\n"
     ]
    }
   ],
   "source": [
    "print('The optimal max_depth for the RandomForestRegressor is: {}'.format(cv_rf_fit.best_params_['max_depth']))\n",
    "print('The optimal n_estimators for the RandomForestRegressor is: {}'.format(cv_rf_fit.best_params_['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8da5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "basic_rf = RandomForestRegressor(max_depth = 50, n_estimators = 300, random_state=42)\n",
    "basic_rf.fit(X_train, y_train)\n",
    "basic_rf_pred = basic_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e557250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics on basic_rf\n",
    "basic_rf_r2 = r2_score(y_test, basic_rf_pred)\n",
    "basic_rf_mse = mse(y_test, basic_rf_pred)\n",
    "basic_rf_rmse = np.sqrt(basic_rf_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "954aa316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create RF results dataframe\n",
    "rf_results = pd.DataFrame({'Model':['dummy_reg', 'basic_rf'], 'R2': [dummy_r2, basic_rf_r2], 'MSE':[dummy_mse, basic_rf_mse], 'RMSE':[dummy_rmse, basic_rf_rmse]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600825df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dummy_reg</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>284.483911</td>\n",
       "      <td>16.866651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_rf</td>\n",
       "      <td>0.849384</td>\n",
       "      <td>42.810904</td>\n",
       "      <td>6.543004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model        R2         MSE       RMSE\n",
       "0  dummy_reg -0.000861  284.483911  16.866651\n",
       "1   basic_rf  0.849384   42.810904   6.543004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744795d6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1d56d",
   "metadata": {},
   "source": [
    "#### Using RandomSearchCV to find optimal hyperparameter values\n",
    "Now let's perform RandomizedSearchCV across a broader range of hyperparameters (and values) to get an idea of where to focus a grid search of a more comprehensive range of hyperparamters than those evaluated in the basic grid search above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "329c271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 120, num = 11)]\n",
    "max_depth.append(None)\n",
    "#max_depth.append(9)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95bb4747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [20, 30, 40, 50, 60, 70,\n",
       "                                                      80, 90, 100, 110, 120,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4, 10],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, n_jobs=-1, random_state=42)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4591b1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 110,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bee4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "rf2 = RandomForestRegressor(n_estimators= 900,\n",
    "                             min_samples_split= 2,\n",
    "                             min_samples_leaf= 2,\n",
    "                             max_features= 'auto',\n",
    "                             max_depth= 90,\n",
    "                             bootstrap= True, \n",
    "                             random_state=42)\n",
    "rf2.fit(X_train, y_train)\n",
    "rf_pred2 = rf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9e0d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine R2, MSE, RMSE \n",
    "rf_random_r2 = r2_score(y_test, rf_pred2)\n",
    "rf_random_mse = mse(y_test, rf_pred2)\n",
    "rf_random_rmse = np.sqrt(mse(y_test, rf_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c54fadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to rf_results df\n",
    "rf_results.loc[len(rf_results)]= ['rf_randomCV', \n",
    "                                 r2_score(y_test, rf_pred2),\n",
    "                                 mse(y_test, rf_pred2),\n",
    "                                 np.sqrt(mse(y_test, rf_pred2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "890b6484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dummy_reg</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>284.483911</td>\n",
       "      <td>16.866651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_rf</td>\n",
       "      <td>0.849384</td>\n",
       "      <td>42.810904</td>\n",
       "      <td>6.543004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_randomCV</td>\n",
       "      <td>0.849289</td>\n",
       "      <td>42.838057</td>\n",
       "      <td>6.545079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model        R2         MSE       RMSE\n",
       "0    dummy_reg -0.000861  284.483911  16.866651\n",
       "1     basic_rf  0.849384   42.810904   6.543004\n",
       "2  rf_randomCV  0.849289   42.838057   6.545079"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf22d8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de6aea",
   "metadata": {},
   "source": [
    "### Using GridSearchCV to find the optimal hyperparameter values\n",
    "We can now use the insights we gained from the above random search to perform a grid search across a broader range of hyperparameters (with more focused values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09454aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "GS_param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [70, 80, 90, 100, 110],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'n_estimators': [700, 800, 900, 1000, 1100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e616be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = GS_param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94759d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb24a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "rf3 = RandomForestRegressor(n_estimators= 1100,\n",
    "                             min_samples_split= 2,\n",
    "                             min_samples_leaf= 2,\n",
    "                             max_features= 'auto',\n",
    "                             max_depth= 90,\n",
    "                             bootstrap= True)\n",
    "rf3.fit(X_train, y_train)\n",
    "rf_pred3 = rf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71edeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the RandomForestRegressor with GSCV tuning is {}'.format(r2_score(y_test, rf_pred3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MSE of the RandomForestRegressor with GSCV tuning is {}'.format(mse(y_test, rf_pred3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to rf_results\n",
    "rf_results.loc[len(rf_results)]= ['rf_gridCV', \n",
    "                                 r2_score(y_test, rf_pred3),\n",
    "                                 mse(y_test, rf_pred3),\n",
    "                                 np.sqrt(mse(y_test, rf_pred3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec586b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f9472",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with hyperopt\n",
    "Lastly, we'll try a different method of determining the best hyperparameter values: Bayesian Optimization. We'll use the hyperopt package across a narrower range of hyperparameters for the sake of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb797d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cdd3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c8cc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = np.arange(50, 1510, 10)\n",
    "max_depth = np.arange(5, 205, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1330ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  50,   60,   70,   80,   90,  100,  110,  120,  130,  140,  150,\n",
       "        160,  170,  180,  190,  200,  210,  220,  230,  240,  250,  260,\n",
       "        270,  280,  290,  300,  310,  320,  330,  340,  350,  360,  370,\n",
       "        380,  390,  400,  410,  420,  430,  440,  450,  460,  470,  480,\n",
       "        490,  500,  510,  520,  530,  540,  550,  560,  570,  580,  590,\n",
       "        600,  610,  620,  630,  640,  650,  660,  670,  680,  690,  700,\n",
       "        710,  720,  730,  740,  750,  760,  770,  780,  790,  800,  810,\n",
       "        820,  830,  840,  850,  860,  870,  880,  890,  900,  910,  920,\n",
       "        930,  940,  950,  960,  970,  980,  990, 1000, 1010, 1020, 1030,\n",
       "       1040, 1050, 1060, 1070, 1080, 1090, 1100, 1110, 1120, 1130, 1140,\n",
       "       1150, 1160, 1170, 1180, 1190, 1200, 1210, 1220, 1230, 1240, 1250,\n",
       "       1260, 1270, 1280, 1290, 1300, 1310, 1320, 1330, 1340, 1350, 1360,\n",
       "       1370, 1380, 1390, 1400, 1410, 1420, 1430, 1440, 1450, 1460, 1470,\n",
       "       1480, 1490, 1500])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37f025a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n",
       "        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n",
       "       135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195,\n",
       "       200])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70dac5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", n_estimators),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", max_depth),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50248948",
   "metadata": {},
   "source": [
    "n_estimators-> start from 50 until 1500, 10\n",
    "max_depth-> start from 5 until 150 or 200 append None\n",
    "If better hp, experiment with hp for rf and et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08e52b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objective function\n",
    "\n",
    "def hyperparameter_tuning(params):\n",
    "    rf_reg = RandomForestRegressor(**params,n_jobs=-1)\n",
    "    r2 = cross_val_score(rf_reg, X_train, y_train ,scoring=\"r2\").mean()\n",
    "    return -r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea24410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:18:34<00:00,  4.71s/trial, best loss: -0.8305086670912729]\n",
      "Best: {'max_depth': 18, 'n_estimators': 6}\n"
     ]
    }
   ],
   "source": [
    "best = fmin(\n",
    "    fn=hyperparameter_tuning,\n",
    "    space = space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=1000, \n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85d742",
   "metadata": {},
   "source": [
    "'max_depth': 20, 'n_estimators': 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a77847d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "rf4 = RandomForestRegressor(n_estimators= 105,\n",
    "                             max_depth= 95)\n",
    "                             \n",
    "rf4.fit(X_train, y_train)\n",
    "rf_pred4 = rf4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03689d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to rf_results\n",
    "rf_results.loc[len(rf_results)]= ['rf_bayes_hyperopt', \n",
    "                                 r2_score(y_test, rf_pred4),\n",
    "                                 mse(y_test, rf_pred4),\n",
    "                                 np.sqrt(mse(y_test, rf_pred4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e267860",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = rf_results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61455606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dummy_reg</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>284.483911</td>\n",
       "      <td>16.866651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_rf</td>\n",
       "      <td>0.849384</td>\n",
       "      <td>42.810904</td>\n",
       "      <td>6.543004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_randomCV</td>\n",
       "      <td>0.849289</td>\n",
       "      <td>42.838057</td>\n",
       "      <td>6.545079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model        R2         MSE       RMSE\n",
       "0    dummy_reg -0.000861  284.483911  16.866651\n",
       "1     basic_rf  0.849384   42.810904   6.543004\n",
       "2  rf_randomCV  0.849289   42.838057   6.545079"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f546a68",
   "metadata": {},
   "source": [
    "$\\star$ All of our Random Forest Regressors performed remarkably similarly. However, it looks like the optimal hyperparameters chosen by our original basic grid search and our random search performed the best, both in terms of the R2 and mean squared error. Because we tested a wider range of hyperparameters in the random search, we'll use those hyperparameters (as defined in `rf_randomCV`) in our final Voting Regressor ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef09a0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a68369",
   "metadata": {},
   "source": [
    "## 2. Extra Trees Regressor\n",
    "Now let's determine some optimal hyperparameters for the second estimator in our Voting Regressor: an Extra Trees Regressor. We'll repeat the same process we used for our Random Forest Regressor above, using Coarse to Fine hyperparameter tuning, as well as experimenting with Bayesian Optimization. We'll put the results of each of these searches in a DataFrame for easy comparison, and select the best set of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf9cf67",
   "metadata": {},
   "source": [
    "### Using basic hyperparameter tuning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to search for GridSearchCV\n",
    "param_grid = {'n_estimators': [100, 300, 500, 900, 1200],\n",
    "              'max_depth': [3, 5, 20, 50, 100],\n",
    "              }\n",
    "# Instantiate ExtraTreesRegressor\n",
    "et = ExtraTreesRegressor(random_state=42)\n",
    "cv_et = GridSearchCV(et, param_grid, cv = 5)\n",
    "cv_et_fit = cv_et.fit(X_train, y_train)\n",
    "\n",
    "#Print the best parameter\n",
    "print(cv_et_fit.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf33b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0edcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The optimal max_depth for the ExtraTreesRegressor is: {}'.format(cv_et_fit.best_params_['max_depth']))\n",
    "print('The optimal n_estimators for the ExtraTreesRegressor is: {}'.format(cv_et_fit.best_params_['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ETR with optimal hyperparameters\n",
    "et_basic = RandomForestRegressor(max_depth = 50, n_estimators = 1200)\n",
    "et_basic.fit(X_train, y_train)\n",
    "et_basic_pred = et_basic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the basic Extra Trees Regressor is {}'.format(r2_score(y_test, et_basic_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb346517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MSE of the basic ExtraTreesRegressor is {}'.format(mse(y_test, et_basic_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate results\n",
    "basic_et_r2 = r2_score(y_test, et_basic_pred)\n",
    "basic_et_mse = mse(y_test, et_basic_pred)\n",
    "basic_et_rmse = np.sqrt(mse(y_test, et_basic_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ET DataFrame with dummy regressor results\n",
    "et_results = pd.DataFrame({'Model':['dummy_reg', 'basic_et'], 'R2': [dummy_r2, basic_et_r2], 'MSE':[dummy_mse, basic_et_mse], 'RMSE':[dummy_rmse, basic_et_rmse]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a610b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721cf75",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b2e6c2",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV to find optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(30, 140, num = 12)]\n",
    "max_depth.append(None)\n",
    "#max_depth.append(9)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_et = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa09662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "et = ExtraTreesRegressor(random_state=42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "et2 = RandomizedSearchCV(estimator = et, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "et2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "et2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "et_random = ExtraTreesRegressor(n_estimators= 300,\n",
    "                             min_samples_split= 2,\n",
    "                             min_samples_leaf= 1,\n",
    "                             max_features= 'auto',\n",
    "                             max_depth= 120,\n",
    "                             bootstrap= True)\n",
    "et_random.fit(X_train, y_train)\n",
    "et_random_pred = et_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53656ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the ExtraTreesRegressor with RSCV tuning is {}'.format(r2_score(y_test, et_random_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4796b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MSE of the ExtraTreesRegressor with RSCV tuning is {}'.format(mse(y_test, et_random_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e315521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results\n",
    "et_results.loc[len(et_results)]= ['et_randomCV', \n",
    "                                 r2_score(y_test, et_random_pred),\n",
    "                                 mse(y_test, et_random_pred),\n",
    "                                 np.sqrt(mse(y_test, et_random_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfa218",
   "metadata": {},
   "source": [
    "#### Using GridSearchCV to find the optimal hyperparamter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92dbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "GS_param_grid_et = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': np.linspace(40, 150, 12),\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [2, 3],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'n_estimators': [300, 500, 800, 1000, 1200, 1600]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2299472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model\n",
    "et = ExtraTreesRegressor()\n",
    "# Instantiate the grid search model\n",
    "et3 = GridSearchCV(estimator = et, param_grid = GS_param_grid_et, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "et3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "et3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "et_grid = ExtraTreesRegressor(n_estimators= 800,\n",
    "                             min_samples_split= 4,\n",
    "                             min_samples_leaf= 2,\n",
    "                             max_features= 'auto',\n",
    "                             max_depth= 90,\n",
    "                             bootstrap= True)\n",
    "et_grid.fit(X_train, y_train)\n",
    "et_grid_pred = et_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the Extra Trees Regressor with GSCV is {}'.format(r2_score(y_test, et_grid_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The MSE of the ExtraTreesRegressor with GSCV is {}'.format(mse(y_test, et_grid_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results\n",
    "et_results.loc[len(et_results)]= ['et_gridCV', \n",
    "                                 r2_score(y_test, et_grid_pred),\n",
    "                                 mse(y_test, et_grid_pred),\n",
    "                                 np.sqrt(mse(y_test, et_grid_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594486f",
   "metadata": {},
   "source": [
    "### Bayestian Optimization with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef71e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_et = Trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f81df",
   "metadata": {},
   "source": [
    "n_estimators-> 50 until 1500, 10\n",
    "max_depth-> 5, 150 or 200, 10\n",
    "if hp improves greatly, run similar hp grids on rf and et gridsearch and random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_et = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", [400, 500, 600, 700, 800, 900, 1000, 1100, 1200]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 30, 120, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9fb9b",
   "metadata": {},
   "source": [
    "n_estimators-> start from 50 until 1500, 10\n",
    "max_depth-> start from 5 until 150 or 200 append None\n",
    "If better hp, experiment with hp for rf and et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_et = fmin(\n",
    "    fn=hyperparameter_tuning,\n",
    "    space = space_et, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=100, \n",
    "    trials=trials_et\n",
    ")\n",
    "\n",
    "print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf99c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "et4 = ExtraTreesRegressor(n_estimators= 1000,\n",
    "                             max_depth= 30)\n",
    "                             \n",
    "et4.fit(X_train, y_train)\n",
    "et_pred4 = et4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results\n",
    "et_results.loc[len(rf_results)]= ['et_bayes_hyperopt', \n",
    "                                 r2_score(y_test, et_pred4),\n",
    "                                 mse(y_test, et_pred4),\n",
    "                                 np.sqrt(mse(y_test, et_pred4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f25e2",
   "metadata": {},
   "source": [
    "For the Extra Trees Regressor, it seems that the hyperparameters suggested by RandomizedSearchCV (stored in `et_randomCV`) produced the best R2, MSE, and RMSE, so we'll use those hyperparameters for our final Voting Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c491c4",
   "metadata": {},
   "source": [
    "## 3. CatBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985ef0f",
   "metadata": {},
   "source": [
    "Now for the third, and final, estimator in our ensemble method, a CatBoost Regressor. First we'll run the regressor without using the Pool function, and then we'll run it with the Pool function (each time testing a range of hyperparameters) to determine whether the model performs better with or without the Pool function. Then, we'll perform Coarse to Fine hyperparameter tuning, using random search and then grid search, just as with the above models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa86ea0",
   "metadata": {},
   "source": [
    "### Without Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_names = ['Month', 'day_of_week', 'state_id_MA', 'state_id_CT', 'state_id_VT', 'state_id_ME'] # here we specify names of categorical features\n",
    "cat_features = [X.columns.get_loc(col) for col in cat_features_names]\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218756cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {'loss_function':'RMSE',\n",
    "          'eval_metric':'R2',\n",
    "          'cat_features': cat_features,\n",
    "          'verbose': 200,\n",
    "          'early_stopping_rounds': 200,\n",
    "          'random_seed': 42\n",
    "         }\n",
    "cbc_1 = CatBoostRegressor(**params)\n",
    "cbc_1.fit(X_train, y_train,\n",
    "          eval_set=(X_test, y_test),\n",
    "          use_best_model=True,\n",
    "          plot=True\n",
    "         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display.Image(\"cat_boost_wo_pool.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee207df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cbc1= cbc_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc64c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_cbc1 = r2_score(y_test, pred_cbc1)\n",
    "rmse_cbc1 = np.sqrt(mse(y_test, pred_cbc1))\n",
    "mse_cbc1 = mse(y_test, pred_cbc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing performance')\n",
    "print('RMSE: {:.2f}'.format(rmse_cbc1))\n",
    "print('MSE: {:.2f}'.format(mse_cbc1))\n",
    "print('R2: {:.2f}'.format(r2_cbc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e976a94",
   "metadata": {},
   "source": [
    "### With Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9de143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Pool(X_train, y_train) \n",
    "test_dataset = Pool(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbc2 = CatBoostRegressor(loss_function='RMSE', random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35addcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbc2.grid_search(grid, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cbc2 = model_cbc2.predict(X_test)\n",
    "rmse_cbc2 = (np.sqrt(mse(y_test, pred_cbc2)))\n",
    "r2_cbc2 = r2_score(y_test, pred_cbc2)\n",
    "print('Testing performance')\n",
    "print('RMSE: {:.2f}'.format(rmse_cbc2))\n",
    "print('R2: {:.2f}'.format(r2_cbc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03883f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbc2_params = {'depth': 8,\n",
    "  'iterations': 150,\n",
    "  'learning_rate': 0.1,\n",
    "  'l2_leaf_reg': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1b385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_cbc2_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df105d33",
   "metadata": {},
   "source": [
    "It looks like the model performed slightly better when using the Pool function on the training and testing sets, so we'll use the Pool function when conducting Coarse to Fine hyperparameter tuning. We'll use five-fold cross-validation as we did with the previous two estimators.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436807b",
   "metadata": {},
   "source": [
    "### RandomizedSearch with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_random = CatBoostRegressor(loss_function='RMSE', random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_random = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c135296",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_random.randomized_search(grid_random, train_dataset, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cbc_random = cbc_random.predict(X_test)\n",
    "rmse_cbc_random = (np.sqrt(mse(y_test, pred_cbc_random)))\n",
    "r2_cbc_random = r2_score(y_test, pred_cbc_random)\n",
    "print('Testing performance')\n",
    "print('RMSE: {:.2f}'.format(rmse_cbc_random))\n",
    "print('R2: {:.2f}'.format(r2_cbc_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61320d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_cbc_params = {'depth': 8,\n",
    "  'iterations': 100,\n",
    "  'learning_rate': 0.1,\n",
    "  'l2_leaf_reg': 0.5},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4636fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randomized_cbc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32d606",
   "metadata": {},
   "source": [
    "### GridSearch with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_grid = CatBoostRegressor(loss_function='RMSE', random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv = {'iterations': [100, 150, 200],\n",
    "        'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f58265",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_grid.grid_search(grid_cv, train_dataset, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cbc_gridcv = cbc_grid.predict(X_test)\n",
    "rmse_cbc_gridcv = (np.sqrt(mse(y_test, pred_cbc_gridcv)))\n",
    "r2_cbc_gridcv = r2_score(y_test, pred_cbc_gridcv)\n",
    "print('Testing performance')\n",
    "print('RMSE: {:.2f}'.format(rmse_cbc_gridcv))\n",
    "print('R2: {:.2f}'.format(r2_cbc_gridcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca64102",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridcv_cbc_params = {'depth': 8,\n",
    "  'iterations': 150,\n",
    "  'learning_rate': 0.1,\n",
    "  'l2_leaf_reg': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe17fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridcv_cbc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598659b",
   "metadata": {},
   "source": [
    "It looks like RandomizedSearch with cv and GridSearch with cv produced equivalent R2 and RMSE, but used *slightly* different hyperparameters. Because GridSearch with cross-validation is more exhaustive than RandomizedSearch, let's go with GridSearch's optimal hyperparameters for the final ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00480e3",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879dfe1",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db28680",
   "metadata": {},
   "source": [
    "Now that we've determined the optimal hyperparameters for each of our three estimators, it's time to pass unfitted versions of these estimators to our final Voting Regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base estimators\n",
    "# RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(bootstrap = True,\n",
    "                             max_depth = 50,\n",
    "                             max_features = 'auto',\n",
    "                             #min_samples_leaf = 2,\n",
    "                             #min_samples_split = 2,\n",
    "                             n_estimators = 20,\n",
    "                             random_state=42\n",
    ")\n",
    "\n",
    "# ExtraTreesRegressor\n",
    "et_reg = ExtraTreesRegressor(n_estimators = 300,\n",
    "                             min_samples_split = 2,\n",
    "                             min_samples_leaf = 1,\n",
    "                             max_features = 'auto',\n",
    "                             max_depth = 120,\n",
    "                             bootstrap = True,\n",
    "                             random_state = 42)\n",
    "\n",
    "# CatBoostRegressor\n",
    "cb_reg = CatBoostRegressor(iterations=150,\n",
    "                           depth = 8,\n",
    "                           learning_rate = 0.1,\n",
    "                           l2_leaf_reg = 0.5,\n",
    "                           random_seed = 42,\n",
    "                           )\n",
    "#cb_reg.set_params(**cbc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74328cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Voting Classifier \n",
    "\n",
    "clf_voting = VotingRegressor(estimators = [('rf_est', rf_reg), ('et_est', et_reg),\n",
    "                                           ('cb_est', cb_reg)])\n",
    "#Fit and predict\n",
    "\n",
    "clf_voting.fit(X_train, y_train)\n",
    "pred_voting = clf_voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a307c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unweighted VotingReg R2: ', r2_score(y_test, pred_voting))\n",
    "print('Unweighted VotingReg MSE: ', mse(y_test, pred_voting))\n",
    "print('Unweighted VotingReg RMSE: ', np.sqrt(mse(y_test, pred_voting)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ca1f1",
   "metadata": {},
   "source": [
    "To determine the optimal weights to use for each of the estimators, we'll use nested for loops to determine the weight combinations that lead to the best evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52762b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists that will store the different weights\n",
    "\n",
    "weights1 = []\n",
    "weights2 = []\n",
    "weights3 = []\n",
    "scores = []\n",
    "\n",
    "# Create a for loop to evaluate different combinations of weights\n",
    "\n",
    "for i in np.arange(0.1,1, 0.1):\n",
    "    for j in np.arange(0.1,1, 0.1):\n",
    "        for k in np.arange(0.1,1, 0.1):\n",
    "            reg_voting = VotingRegressor(estimators = [('rf_est', rf_reg), ('et_est', et_reg),\n",
    "                                           ('cb_est', cb_reg)], weights = [i, j, k])\n",
    "            reg_voting.fit(X_train, y_train)\n",
    "            pred = reg_voting.predict(X_test)\n",
    "            score = r2_score(y_test, pred)\n",
    "            scores.append(score)\n",
    "            weights1.append(i)\n",
    "            weights2.append(j)\n",
    "            weights3.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in a data frame\n",
    "\n",
    "test_scores = pd.DataFrame()\n",
    "test_scores['Weight1'] = weights1\n",
    "test_scores['Weight2'] = weights2\n",
    "test_scores['Weight3'] = weights3\n",
    "test_scores['Test Score'] = scores\n",
    "\n",
    "# Create an additional column to save the sum of all the weights\n",
    "\n",
    "test_scores['sum_weights'] = test_scores['Weight1'].add(test_scores['Weight2']).add(test_scores['Weight3'])\n",
    "\n",
    "#We are only getting the rows that the sum of all weights were equal to one\n",
    "\n",
    "condition = test_scores['sum_weights'] == 1\n",
    "\n",
    "test_scores = test_scores.loc[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ea5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the values to see the different test scores depending on the weights\n",
    "test_scores.sort_values(by = 'Test Score', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e158d",
   "metadata": {},
   "source": [
    "It seems that the Voting Regressor performs best with a weight of 0.1 on estimator 1 (Random Forest), a weight of 0.6 on estimator 2 (Extra Trees), and a weight of 0.3 on estimator 3 (CatBoost). Let's apply the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Voting Classifier with the most equally weighted because all models performed similarly\n",
    "\n",
    "reg_voting = VotingRegressor(estimators = [('rf_est', rf_reg), ('et_est', et_reg),\n",
    "                                           ('cb_est', cb_reg)], weights = [0.1, 0.6, 0.3])\n",
    "\n",
    "#Fit and predict\n",
    "\n",
    "reg_voting.fit(X_train, y_train)\n",
    "pred_voting = reg_voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3790d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_final = r2_score(y_test, pred_voting)\n",
    "MSE_final = mse(y_test, pred_voting)\n",
    "RMSE_final = np.sqrt(mse(y_test, pred_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final VotingRegressor R2: ', R2_final)\n",
    "print('Final VotingRegressor MSE: ', MSE_final)\n",
    "print('Final VotingRegressor RMSE: ', RMSE_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be58077",
   "metadata": {},
   "source": [
    "Our model's final R2 is about .86!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8291d3b",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b05079",
   "metadata": {},
   "source": [
    "Let's briefly investigate feature importance to see if we find anything unusual. There is currently no way of directly determining feature importance of a Voting Regressor, so instead, we'll define a function, `compute_feature_importance`, to determine the feature importances of each of the three individual estimators fed to the final Voting Regressor, and then apply the same weights to the feature importances as we did to the estimators themselves above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae80d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(voting_clf, weights):\n",
    "    \"\"\" Function to compute feature importance of Voting Classifier \"\"\"\n",
    "    \n",
    "    feature_importance = {}\n",
    "    for est in reg_voting.estimators_:\n",
    "        feature_importance[str(est)] = est.feature_importances_\n",
    "    \n",
    "    fe_scores = [0]*len(list(feature_importance.values())[0])\n",
    "    for idx, imp_score in enumerate(feature_importance.values()):\n",
    "        imp_score_with_weight = imp_score*weights[idx]\n",
    "        fe_scores = list(np.add(fe_scores, list(imp_score_with_weight)))\n",
    "    return fe_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame()\n",
    "feat_df['Feature'] = X_train.columns\n",
    "feat_df['Feature Importance'] = compute_feature_importance(reg_voting, [0.1, 0.6, 0.3])\n",
    "feat_df=(feat_df.sort_values('Feature Importance', ascending = False)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = feat_df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots()\n",
    "plt.barh(feat_df['Feature'],\n",
    "         feat_df['Feature Importance'])\n",
    "plt.xlabel('Feature Importance', fontsize=16)\n",
    "plt.ylabel('Feature', fontsize=16)\n",
    "#ax.set_ylabel('Feature')\n",
    "plt.title('Feature Importances in Weighted Voting Regressor', fontsize=18)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
