{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e155626",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8888365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44217440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe REGULAR\n",
    "#parse datetime column\n",
    "#df=pd.read_csv('COVID19_modeling.csv', parse_dates=[0])\n",
    "#df.set_index('date', inplace= True)\n",
    "#df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ce3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe OHE\n",
    "#parse datetime column\n",
    "df_ohe = pd.read_csv('ohe_data.csv', parse_dates=['date'])\n",
    "df_ohe.drop(columns='Unnamed: 0', inplace=True)\n",
    "df_ohe.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30989e8f",
   "metadata": {},
   "source": [
    "## Using the mean as a baseline prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd771ce",
   "metadata": {},
   "source": [
    "Previously, we determined the R2 score of using the mean to predict COVID19 cases for each individual state. Let's do the same thing now that we have all the states in one DataFrame, so that we will have a baseline \"dummy\" model to compare our future optimized model to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d76c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score of using the mean to predict COVID19 cases in our states is: -0.0019588491283020204\n"
     ]
    }
   ],
   "source": [
    "# Create features\n",
    "X, y = df_ohe.drop(columns=['Conf_Cases', 'new_case_percent_pop*']), df_ohe['new_case_percent_pop*']\n",
    "# Make test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "dummy_mean = DummyRegressor()\n",
    "# \"Train\" dummy regressor\n",
    "dummy_mean.fit(X_train, y_train)\n",
    "# Get R2 score\n",
    "score_dummy = dummy_mean.score(X_test, y_test)\n",
    "print(\"The R2 score of using the mean to predict COVID19 cases in our states is:\", score_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18106e10",
   "metadata": {},
   "source": [
    "## Tuning the top performing models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d447b1",
   "metadata": {},
   "source": [
    "In the pre-processing step, we determined (with the help of Pycaret) that our top performing models were **CatBoost Regressor**, **Random Forest Regressor**, and **Gradient Boosting Regressor**. Let's now fine tune the hyperparameters of each of these models, in preparation for feeding them into the pipeline of the Voting Regressor. \n",
    "\n",
    "**NOTE TO SELF: Determine hyperparameters for each model, but pass unfitted (tuned) models to the VotingRegressor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a6ddc",
   "metadata": {},
   "source": [
    "#### CatBoost Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffeb1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22071089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "165a0210",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor\n",
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75d3d7",
   "metadata": {},
   "source": [
    "#### Using basic hyperparameter tuning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda31fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = df_ohe.drop(columns=['Conf_Cases', 'new_case_percent_pop*']), df_ohe['new_case_percent_pop*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f6903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69edccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# Define parameters to search for GridSearchCV\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500],\n",
    "              'max_depth': [3, 5, 7, 9],\n",
    "              }\n",
    "# Instantiate RandomForestClassifier\n",
    "rf = RandomForestRegressor()\n",
    "cv_rf = GridSearchCV(rf, param_grid, cv = 10)\n",
    "cv_rf_fit = cv_rf.fit(X_train, y_train)\n",
    "\n",
    "#Print the best parameter\n",
    "print(cv_rf_fit.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df6f1be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal max_depth for the RandomForestRegressor is: 9\n",
      "The optimal n_estimators for the RandomForestRegressor is: 400\n"
     ]
    }
   ],
   "source": [
    "print('The optimal max_depth for the RandomForestRegressor is: {}'.format(cv_rf_fit.best_params_['max_depth']))\n",
    "print('The optimal n_estimators for the RandomForestRegressor is: {}'.format(cv_rf_fit.best_params_['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37276dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "rf = RandomForestRegressor(max_depth = 9, n_estimators = 400)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a75cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for the RandomForestRegressor is 0.8327308664092816\n"
     ]
    }
   ],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the RandomForestRegressor is {}'.format(r2_score(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba3647",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad4f9f",
   "metadata": {},
   "source": [
    "#### Using RandomSearchCV to find optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a26909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3646f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   16.2s\n",
      "/Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3626e85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 100,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a301c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "rf2 = RandomForestRegressor(n_estimators= 800,\n",
    "                             min_samples_split= 2,\n",
    "                             min_samples_leaf= 1,\n",
    "                             max_features= 'auto',\n",
    "                             max_depth= 100,\n",
    "                             bootstrap= True)\n",
    "rf2.fit(X_train, y_train)\n",
    "rf_pred2 = rf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1241536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for the RandomForestRegressor with RSCV tuning is 0.8396750629461008\n"
     ]
    }
   ],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the RandomForestRegressor with RSCV tuning is {}'.format(r2_score(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64944a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the RandomForestRegressor with RSCV tuning is 55.73368057130931\n"
     ]
    }
   ],
   "source": [
    "print('The MSE of the RandomForestRegressor with RSCV tuning is {}'.format(mean_squared_error(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67b3e3",
   "metadata": {},
   "source": [
    "#### Using GridSearchCV to find the optimal hyperparamter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f88fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "GS_param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110, 120],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'n_estimators': [600, 700, 800, 9000, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = GS_param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06578428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   29.8s\n",
      "/Users/abigailmorgan/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 38.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 39.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True],\n",
       "                         'max_depth': [80, 90, 100, 110, 120],\n",
       "                         'max_features': ['auto'],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5],\n",
       "                         'n_estimators': [600, 700, 800, 9000, 1000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "713795ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 110,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 800}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18b7030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RFR with optimal hyperparameters\n",
    "rf3 = RandomForestRegressor(n_estimators= 800,\n",
    "                             min_samples_split= 3,\n",
    "                             min_samples_leaf= 2,\n",
    "                             max_features= 'auto',\n",
    "                             max_depth= 110,\n",
    "                             bootstrap= True)\n",
    "rf3.fit(X_train, y_train)\n",
    "rf_pred3 = rf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eb8530f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for the RandomForestRegressor with tuning is 0.8311230661025624\n"
     ]
    }
   ],
   "source": [
    "# Determine R2 score\n",
    "print('The R2 score for the RandomForestRegressor with GSCV tuning is {}'.format(r2_score(y_test, rf_pred3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "705f4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the RandomForestRegressor with GSCV tuning is 58.70660711089296\n"
     ]
    }
   ],
   "source": [
    "print('The MSE of the RandomForestRegressor with GSCV tuning is {}'.format(mean_squared_error(y_test, rf_pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f8e4d2",
   "metadata": {},
   "source": [
    "It looks like the optimal hyperparameters chosen by RandomSearchCV performed the best, both in terms of the R2 and mean squared error, so we'll use those hyperparameters (as defined in `rf2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54883ccd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea81a5",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae90bf6",
   "metadata": {},
   "source": [
    "#### Voting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8e566",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "reg_voting = VotingRegressor(\n",
    "        estimators = [\n",
    "            ('label1', reg_1),\n",
    "            ('label2', reg_2),\n",
    "            ...\n",
    "            ('labelN', reg_N)],\n",
    "        voting = 'soft',\n",
    "        weights = [w_1, w_2, ..., w_N]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78f7d2",
   "metadata": {},
   "source": [
    "#### Determining optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists that will storage the different weights\n",
    "\n",
    "weights1 = []\n",
    "weights2 = []\n",
    "weights3 = []\n",
    "scores = []\n",
    "\n",
    "# Create a for loop to evaluate different combinations of weights\n",
    "\n",
    "for i in np.arange(0.1,1, 0.1):\n",
    "    for j in np.arange(0.1,1, 0.1):\n",
    "        for k in np.arange(0.1,1, 0.1):\n",
    "            clf_voting = VotingClassifier(estimators = [('est1', clf1), ('est2', clf2),\n",
    "                                           ('est3', clf3)], voting = 'soft', weights = [i, j, k])\n",
    "            clf_voting.fit(X_train, y_train)\n",
    "            pred = clf_voting.predict(X_test)\n",
    "            score = accuracy_score(y_test, pred)\n",
    "            scores.append(score)\n",
    "            weights1.append(i)\n",
    "            weights2.append(j)\n",
    "            weights3.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722015d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in a data frame\n",
    "\n",
    "test_scores = pd.DataFrame()\n",
    "test_scores['Weight1'] = weights1\n",
    "test_scores['Weight2'] = weights2\n",
    "test_scores['Weight3'] = weights3\n",
    "test_scores['Test Score'] = scores\n",
    "\n",
    "# Create an additional column to save the sum of all the weights\n",
    "\n",
    "test_scores['sum_weights'] = test_scores['Weight1'].add(test_scores['Weight2']).add(test_scores['Weight3'])\n",
    "\n",
    "#We are only getting the rows that the sum of all weights were equal to one\n",
    "\n",
    "condition = test_scores['sum_weights'] == 1\n",
    "\n",
    "test_scores = test_scores.loc[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the values to see the different test scores depending on the weights\n",
    "test_scores.sort_values(by = 'Test Score', ascending = False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d17c36",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc6a27",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a563a",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_voting = VotingRegressor(\n",
    "                estimators = [\n",
    "                    ('rf_reg', rf),\n",
    "                    ('gb_reg', gb),\n",
    "                    ('cb_reg', cb)],\n",
    "                voting = 'soft',\n",
    "                weights = [w_1, w_2, w_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0bec19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
